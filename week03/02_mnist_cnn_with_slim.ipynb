{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural networks with slim\n",
    "\n",
    "* MNIST data를 가지고 softmax classifier를 만들어보자.\n",
    "* [`tf.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shatapy/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A very simple MNIST classifier.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "np.random.seed(219)\n",
    "tf.set_random_seed(219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfiAowg8ERfiBoAg/EBThB4L6D2naezQfIi/9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = 500\n",
    "print(\"label = {}\".format(train_labels[index]))\n",
    "plt.imshow(train_data[index].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dataset with `tf.data`\n",
    "\n",
    "#### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "test_dataset = test_dataset.shuffle(buffer_size = 10000)\n",
    "test_dataset = test_dataset.batch(batch_size = len(test_data))\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.data.Iterator.from_string_handle의 output_shapes는 default = None이지만 꼭 값을 넣는 게 좋음\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle,\n",
    "                                               train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "x, y = iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.contrib.slim`\n",
    "\n",
    "```python\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# pylint: disable=unused-import,line-too-long,g-importing-member,wildcard-import\n",
    "# TODO(jart): Delete non-slim imports\n",
    "from tensorflow.contrib import losses\n",
    "from tensorflow.contrib import metrics\n",
    "from tensorflow.contrib.framework.python.ops.arg_scope import *\n",
    "from tensorflow.contrib.framework.python.ops.variables import *\n",
    "from tensorflow.contrib.layers.python.layers import *\n",
    "from tensorflow.contrib.layers.python.layers.initializers import *\n",
    "from tensorflow.contrib.layers.python.layers.regularizers import *\n",
    "from tensorflow.contrib.slim.python.slim import evaluation\n",
    "from tensorflow.contrib.slim.python.slim import learning\n",
    "from tensorflow.contrib.slim.python.slim import model_analyzer\n",
    "from tensorflow.contrib.slim.python.slim import queues\n",
    "from tensorflow.contrib.slim.python.slim import summaries\n",
    "from tensorflow.contrib.slim.python.slim.data import data_decoder\n",
    "from tensorflow.contrib.slim.python.slim.data import data_provider\n",
    "from tensorflow.contrib.slim.python.slim.data import dataset\n",
    "from tensorflow.contrib.slim.python.slim.data import dataset_data_provider\n",
    "from tensorflow.contrib.slim.python.slim.data import parallel_reader\n",
    "from tensorflow.contrib.slim.python.slim.data import prefetch_queue\n",
    "from tensorflow.contrib.slim.python.slim.data import tfexample_decoder\n",
    "from tensorflow.python.util.all_util import make_all\n",
    "# pylint: enable=unused-import,line-too-long,g-importing-member,wildcard-import\n",
    "\n",
    "__all__ = make_all(__name__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers) and [`tf.contrib.layers`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers)\n",
    "\n",
    "#### [`tf.layers.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)\n",
    "```python\n",
    "tf.layers.conv2d(\n",
    "    inputs,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=(1, 1),\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=None,\n",
    "    bias_initializer=tf.zeros_initializer(),\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    trainable=True,\n",
    "    name=None,\n",
    "    reuse=None\n",
    ")\n",
    "```\n",
    "\n",
    "#### [`slim.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d)\n",
    "```python\n",
    "tf.contrib.layers.conv2d(\n",
    "    inputs,\n",
    "    num_outputs,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding='SAME',\n",
    "    data_format=None,\n",
    "    rate=1,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=initializers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(x):\n",
    "  \"\"\"Model function for CNN.\n",
    "  Args:\n",
    "    x: input images\n",
    "    mode: boolean whether trainig mode or test mode\n",
    "    \n",
    "  Returns:\n",
    "    logits: unnormalized score funtion\n",
    "  \"\"\"\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "  with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "  # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  conv1 = slim.conv2d(x_image, 32, [5, 5])\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  pool1 = slim.max_pool2d(conv1, [2, 2])\n",
    "  \n",
    "  # Convolutional Layer #2\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  conv2 = slim.conv2d(pool1, 64, [5, 5])\n",
    "\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  pool2 = slim.max_pool2d(conv2, [2, 2])\n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  pool2_flat = slim.flatten(pool2)\n",
    "  \n",
    "  # Fully connected Layer\n",
    "  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  fc1 = slim.fully_connected(pool2_flat, 1024, activation_fn=tf.nn.relu)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  is_training = tf.placeholder(tf.bool)\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  fc1_drop = slim.dropout(fc1, 0.6, is_training=is_training)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 10]\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  logits = slim.fully_connected(fc1_drop, 10, activation_fn=tf.nn.relu)\n",
    "  \n",
    "  return logits, is_training, x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, is_training, x_image = cnn_model_fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "y_one_hot = tf.one_hot(y, 10)\n",
    "cross_entropy = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=y_one_hot))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign `tf.summary.FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to: graphs/02_mnist_cnn_with_slim\n"
     ]
    }
   ],
   "source": [
    "graph_location = 'graphs/02_mnist_cnn_with_slim'\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "  tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
    "  tf.summary.image('images', x_image)\n",
    "  for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "  # merge all summaries\n",
    "  summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session()` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.31413\n",
      "step: 10, loss: 2.21872\n",
      "step: 20, loss: 2.11076\n",
      "step: 30, loss: 2.06672\n",
      "step: 40, loss: 1.81771\n",
      "step: 50, loss: 1.66868\n",
      "step: 60, loss: 1.47269\n",
      "step: 70, loss: 1.27892\n",
      "step: 80, loss: 1.17966\n",
      "step: 90, loss: 0.994523\n",
      "step: 100, loss: 0.620166\n",
      "step: 110, loss: 0.822477\n",
      "step: 120, loss: 0.45813\n",
      "step: 130, loss: 0.633566\n",
      "step: 140, loss: 0.390372\n",
      "step: 150, loss: 0.417386\n",
      "step: 160, loss: 0.299729\n",
      "step: 170, loss: 0.376684\n",
      "step: 180, loss: 0.316323\n",
      "step: 190, loss: 0.281795\n",
      "step: 200, loss: 0.416732\n",
      "step: 210, loss: 0.834931\n",
      "step: 220, loss: 0.507515\n",
      "step: 230, loss: 0.169468\n",
      "step: 240, loss: 0.353798\n",
      "step: 250, loss: 0.140062\n",
      "step: 260, loss: 0.278425\n",
      "step: 270, loss: 0.101519\n",
      "step: 280, loss: 0.42218\n",
      "step: 290, loss: 0.260041\n",
      "step: 300, loss: 0.422316\n",
      "step: 310, loss: 0.245676\n",
      "step: 320, loss: 0.258643\n",
      "step: 330, loss: 0.409888\n",
      "step: 340, loss: 0.40515\n",
      "step: 350, loss: 0.150999\n",
      "step: 360, loss: 0.31755\n",
      "step: 370, loss: 0.125788\n",
      "step: 380, loss: 0.199848\n",
      "step: 390, loss: 0.397922\n",
      "step: 400, loss: 0.246928\n",
      "step: 410, loss: 0.267351\n",
      "step: 420, loss: 0.222703\n",
      "step: 430, loss: 0.396011\n",
      "step: 440, loss: 0.254893\n",
      "step: 450, loss: 0.314108\n",
      "step: 460, loss: 0.125927\n",
      "step: 470, loss: 0.402241\n",
      "step: 480, loss: 0.397471\n",
      "step: 490, loss: 0.210791\n",
      "step: 500, loss: 0.132786\n",
      "step: 510, loss: 0.279918\n",
      "step: 520, loss: 0.133614\n",
      "step: 530, loss: 0.137703\n",
      "step: 540, loss: 0.0870287\n",
      "step: 550, loss: 0.229969\n",
      "step: 560, loss: 0.201935\n",
      "step: 570, loss: 0.240273\n",
      "step: 580, loss: 0.322794\n",
      "step: 590, loss: 0.250688\n",
      "step: 600, loss: 0.109895\n",
      "step: 610, loss: 0.124867\n",
      "step: 620, loss: 0.163486\n",
      "step: 630, loss: 0.149443\n",
      "step: 640, loss: 0.0903801\n",
      "step: 650, loss: 0.306435\n",
      "step: 660, loss: 0.293463\n",
      "step: 670, loss: 0.126605\n",
      "step: 680, loss: 0.125048\n",
      "step: 690, loss: 0.19083\n",
      "step: 700, loss: 0.276696\n",
      "step: 710, loss: 0.101562\n",
      "step: 720, loss: 0.48991\n",
      "step: 730, loss: 0.297372\n",
      "step: 740, loss: 0.124948\n",
      "step: 750, loss: 0.435398\n",
      "step: 760, loss: 0.123321\n",
      "step: 770, loss: 0.172491\n",
      "step: 780, loss: 0.0754761\n",
      "step: 790, loss: 0.0417487\n",
      "step: 800, loss: 0.321059\n",
      "step: 810, loss: 0.214037\n",
      "step: 820, loss: 0.0627355\n",
      "step: 830, loss: 0.179838\n",
      "step: 840, loss: 0.0413689\n",
      "step: 850, loss: 0.169269\n",
      "step: 860, loss: 0.22381\n",
      "step: 870, loss: 0.222355\n",
      "step: 880, loss: 0.0617517\n",
      "step: 890, loss: 0.154222\n",
      "step: 900, loss: 0.110538\n",
      "step: 910, loss: 0.0714635\n",
      "step: 920, loss: 0.217813\n",
      "step: 930, loss: 0.0627968\n",
      "step: 940, loss: 0.0103643\n",
      "step: 950, loss: 0.352652\n",
      "step: 960, loss: 0.0636851\n",
      "step: 970, loss: 0.302749\n",
      "step: 980, loss: 0.232639\n",
      "step: 990, loss: 0.081708\n",
      "step: 1000, loss: 0.282767\n",
      "step: 1010, loss: 0.142451\n",
      "step: 1020, loss: 0.173217\n",
      "step: 1030, loss: 0.0495202\n",
      "step: 1040, loss: 0.084307\n",
      "step: 1050, loss: 0.0947756\n",
      "step: 1060, loss: 0.0635275\n",
      "step: 1070, loss: 0.227538\n",
      "step: 1080, loss: 0.159656\n",
      "step: 1090, loss: 0.04367\n",
      "step: 1100, loss: 0.220114\n",
      "step: 1110, loss: 0.0748065\n",
      "step: 1120, loss: 0.168498\n",
      "step: 1130, loss: 0.137831\n",
      "step: 1140, loss: 0.246929\n",
      "step: 1150, loss: 0.0661469\n",
      "step: 1160, loss: 0.0531177\n",
      "step: 1170, loss: 0.0167032\n",
      "step: 1180, loss: 0.0875841\n",
      "step: 1190, loss: 0.0802328\n",
      "step: 1200, loss: 0.0762739\n",
      "step: 1210, loss: 0.261749\n",
      "step: 1220, loss: 0.152368\n",
      "step: 1230, loss: 0.285623\n",
      "step: 1240, loss: 0.1144\n",
      "step: 1250, loss: 0.392128\n",
      "step: 1260, loss: 0.18078\n",
      "step: 1270, loss: 0.0391463\n",
      "step: 1280, loss: 0.0988882\n",
      "step: 1290, loss: 0.2847\n",
      "step: 1300, loss: 0.0820678\n",
      "step: 1310, loss: 0.152884\n",
      "step: 1320, loss: 0.148916\n",
      "step: 1330, loss: 0.153592\n",
      "step: 1340, loss: 0.090376\n",
      "step: 1350, loss: 0.224252\n",
      "step: 1360, loss: 0.161384\n",
      "step: 1370, loss: 0.0429739\n",
      "step: 1380, loss: 0.0622635\n",
      "step: 1390, loss: 0.0394801\n",
      "step: 1400, loss: 0.0267854\n",
      "step: 1410, loss: 0.0357354\n",
      "step: 1420, loss: 0.0336727\n",
      "step: 1430, loss: 0.0952778\n",
      "step: 1440, loss: 0.640858\n",
      "step: 1450, loss: 0.012535\n",
      "step: 1460, loss: 0.0358797\n",
      "step: 1470, loss: 0.0548014\n",
      "step: 1480, loss: 0.0311974\n",
      "step: 1490, loss: 0.0487958\n",
      "step: 1500, loss: 0.0384459\n",
      "step: 1510, loss: 0.130049\n",
      "step: 1520, loss: 0.246237\n",
      "step: 1530, loss: 0.173429\n",
      "step: 1540, loss: 0.0872806\n",
      "step: 1550, loss: 0.0609948\n",
      "step: 1560, loss: 0.0608367\n",
      "step: 1570, loss: 0.0595195\n",
      "step: 1580, loss: 0.0494522\n",
      "step: 1590, loss: 0.0441013\n",
      "step: 1600, loss: 0.0868498\n",
      "step: 1610, loss: 0.0191856\n",
      "step: 1620, loss: 0.0936209\n",
      "step: 1630, loss: 0.0462881\n",
      "step: 1640, loss: 0.294601\n",
      "step: 1650, loss: 0.167157\n",
      "step: 1660, loss: 0.0212292\n",
      "step: 1670, loss: 0.0919764\n",
      "step: 1680, loss: 0.146046\n",
      "step: 1690, loss: 0.0112341\n",
      "step: 1700, loss: 0.0340683\n",
      "End of dataset\n",
      "Epochs: 0 Elapsed time: 148.34672284126282\n",
      "step: 1710, loss: 0.216221\n",
      "step: 1720, loss: 0.169325\n",
      "step: 1730, loss: 0.0194147\n",
      "step: 1740, loss: 0.162655\n",
      "step: 1750, loss: 0.144134\n",
      "step: 1760, loss: 0.061256\n",
      "step: 1770, loss: 0.0227251\n",
      "step: 1780, loss: 0.391361\n",
      "step: 1790, loss: 0.0585832\n",
      "step: 1800, loss: 0.086359\n",
      "step: 1810, loss: 0.0596672\n",
      "step: 1820, loss: 0.216747\n",
      "step: 1830, loss: 0.0891972\n",
      "step: 1840, loss: 0.0359566\n",
      "step: 1850, loss: 0.117615\n",
      "step: 1860, loss: 0.0468166\n",
      "step: 1870, loss: 0.117854\n",
      "step: 1880, loss: 0.00718847\n",
      "step: 1890, loss: 0.0595809\n",
      "step: 1900, loss: 0.0116386\n",
      "step: 1910, loss: 0.0856554\n",
      "step: 1920, loss: 0.103078\n",
      "step: 1930, loss: 0.039282\n",
      "step: 1940, loss: 0.00843313\n",
      "step: 1950, loss: 0.103087\n",
      "step: 1960, loss: 0.0700076\n",
      "step: 1970, loss: 0.131382\n",
      "step: 1980, loss: 0.0500257\n",
      "step: 1990, loss: 0.133269\n",
      "step: 2000, loss: 0.103998\n",
      "step: 2010, loss: 0.0693712\n",
      "step: 2020, loss: 0.111628\n",
      "step: 2030, loss: 0.299482\n",
      "step: 2040, loss: 0.115666\n",
      "step: 2050, loss: 0.0179682\n",
      "step: 2060, loss: 0.0464969\n",
      "step: 2070, loss: 0.276546\n",
      "step: 2080, loss: 0.0217511\n",
      "step: 2090, loss: 0.0415878\n",
      "step: 2100, loss: 0.0101229\n",
      "step: 2110, loss: 0.0723025\n",
      "step: 2120, loss: 0.158514\n",
      "step: 2130, loss: 0.019471\n",
      "step: 2140, loss: 0.0725522\n",
      "step: 2150, loss: 0.0307171\n",
      "step: 2160, loss: 0.175897\n",
      "step: 2170, loss: 0.0718394\n",
      "step: 2180, loss: 0.382047\n",
      "step: 2190, loss: 0.0528616\n",
      "step: 2200, loss: 0.0877281\n",
      "step: 2210, loss: 0.0747017\n",
      "step: 2220, loss: 0.0309306\n",
      "step: 2230, loss: 0.0588709\n",
      "step: 2240, loss: 0.0430537\n",
      "step: 2250, loss: 0.0700447\n",
      "step: 2260, loss: 0.0613123\n",
      "step: 2270, loss: 0.0344146\n",
      "step: 2280, loss: 0.0266684\n",
      "step: 2290, loss: 0.305072\n",
      "step: 2300, loss: 0.101842\n",
      "step: 2310, loss: 0.0183575\n",
      "step: 2320, loss: 0.0119359\n",
      "step: 2330, loss: 0.0616722\n",
      "step: 2340, loss: 0.0359352\n",
      "step: 2350, loss: 0.0281655\n",
      "step: 2360, loss: 0.196382\n",
      "step: 2370, loss: 0.0182911\n",
      "step: 2380, loss: 0.152259\n",
      "step: 2390, loss: 0.0328032\n",
      "step: 2400, loss: 0.0234326\n",
      "step: 2410, loss: 0.0446087\n",
      "step: 2420, loss: 0.029817\n",
      "step: 2430, loss: 0.0688287\n",
      "step: 2440, loss: 0.108602\n",
      "step: 2450, loss: 0.0162615\n",
      "step: 2460, loss: 0.0905697\n",
      "step: 2470, loss: 0.0489682\n",
      "step: 2480, loss: 0.00948076\n",
      "step: 2490, loss: 0.0273516\n",
      "step: 2500, loss: 0.232758\n",
      "step: 2510, loss: 0.0233558\n",
      "step: 2520, loss: 0.0315593\n",
      "step: 2530, loss: 0.103119\n",
      "step: 2540, loss: 0.045112\n",
      "step: 2550, loss: 0.059357\n",
      "step: 2560, loss: 0.0387529\n",
      "step: 2570, loss: 0.067683\n",
      "step: 2580, loss: 0.170306\n",
      "step: 2590, loss: 0.075023\n",
      "step: 2600, loss: 0.0103746\n",
      "step: 2610, loss: 0.0152216\n",
      "step: 2620, loss: 0.0387875\n",
      "step: 2630, loss: 0.0183764\n",
      "step: 2640, loss: 0.0387338\n",
      "step: 2650, loss: 0.0310318\n",
      "step: 2660, loss: 0.183556\n",
      "step: 2670, loss: 0.0524052\n",
      "step: 2680, loss: 0.0469435\n",
      "step: 2690, loss: 0.0074157\n",
      "step: 2700, loss: 0.206339\n",
      "step: 2710, loss: 0.0099457\n",
      "step: 2720, loss: 0.211067\n",
      "step: 2730, loss: 0.0955994\n",
      "step: 2740, loss: 0.0470839\n",
      "step: 2750, loss: 0.0356091\n",
      "step: 2760, loss: 0.0206043\n",
      "step: 2770, loss: 0.171679\n",
      "step: 2780, loss: 0.0621361\n",
      "step: 2790, loss: 0.0157822\n",
      "step: 2800, loss: 0.134975\n",
      "step: 2810, loss: 0.189863\n",
      "step: 2820, loss: 0.0260555\n",
      "step: 2830, loss: 0.0562887\n",
      "step: 2840, loss: 0.0158134\n",
      "step: 2850, loss: 0.16846\n",
      "step: 2860, loss: 0.0586179\n",
      "step: 2870, loss: 0.0681737\n",
      "step: 2880, loss: 0.0464448\n",
      "step: 2890, loss: 0.064963\n",
      "step: 2900, loss: 0.0314336\n",
      "step: 2910, loss: 0.0406892\n",
      "step: 2920, loss: 0.044721\n",
      "step: 2930, loss: 0.0467877\n",
      "step: 2940, loss: 0.0231565\n",
      "step: 2950, loss: 0.0662911\n",
      "step: 2960, loss: 0.0436003\n",
      "step: 2970, loss: 0.0747949\n",
      "step: 2980, loss: 0.0265448\n",
      "step: 2990, loss: 0.166098\n",
      "step: 3000, loss: 0.0997423\n",
      "step: 3010, loss: 0.00898321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3020, loss: 0.0102455\n",
      "step: 3030, loss: 0.0199806\n",
      "step: 3040, loss: 0.0307569\n",
      "step: 3050, loss: 0.0855085\n",
      "step: 3060, loss: 0.122695\n",
      "step: 3070, loss: 0.0364364\n",
      "step: 3080, loss: 0.0115856\n",
      "step: 3090, loss: 0.0185213\n",
      "step: 3100, loss: 0.0179669\n",
      "step: 3110, loss: 0.0207366\n",
      "step: 3120, loss: 0.115116\n",
      "step: 3130, loss: 0.0890324\n",
      "step: 3140, loss: 0.0376131\n",
      "step: 3150, loss: 0.0409208\n",
      "step: 3160, loss: 0.123736\n",
      "step: 3170, loss: 0.117231\n",
      "step: 3180, loss: 0.00607836\n",
      "step: 3190, loss: 0.0660428\n",
      "step: 3200, loss: 0.488534\n",
      "step: 3210, loss: 0.0812929\n",
      "step: 3220, loss: 0.128541\n",
      "step: 3230, loss: 0.0274213\n",
      "step: 3240, loss: 0.100071\n",
      "step: 3250, loss: 0.0171381\n",
      "step: 3260, loss: 0.00829276\n",
      "step: 3270, loss: 0.0277011\n",
      "step: 3280, loss: 0.0400591\n",
      "step: 3290, loss: 0.0238682\n",
      "step: 3300, loss: 0.137709\n",
      "step: 3310, loss: 0.0112257\n",
      "step: 3320, loss: 0.0345276\n",
      "step: 3330, loss: 0.0256569\n",
      "step: 3340, loss: 0.0499375\n",
      "step: 3350, loss: 0.104281\n",
      "step: 3360, loss: 0.0536238\n",
      "step: 3370, loss: 0.00247134\n",
      "step: 3380, loss: 0.0796401\n",
      "step: 3390, loss: 0.15304\n",
      "step: 3400, loss: 0.00453919\n",
      "End of dataset\n",
      "Epochs: 1 Elapsed time: 146.44975590705872\n",
      "training done!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train_iterator\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "train_handle = sess.run(train_iterator.string_handle())\n",
    "\n",
    "# Train\n",
    "max_epochs = 2\n",
    "step = 0\n",
    "for epochs in range(max_epochs):\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  sess.run(train_iterator.initializer)\n",
    "\n",
    "  start_time = time.time()\n",
    "  while True:\n",
    "    try:\n",
    "      # 여기를 직접 채워 넣으시면 됩니다.\n",
    "      _, loss = sess.run([train_step, cross_entropy], \n",
    "                         feed_dict={handle: train_handle, is_training:True})\n",
    "      if step % 10 == 0:\n",
    "        print(\"step: %d, loss: %g\" % (step, loss))\n",
    "        \n",
    "        # summary\n",
    "        summary_str = sess.run(summary_op,\n",
    "                         feed_dict={handle: train_handle, is_training:True})\n",
    "        train_writer.add_summary(summary_str, global_step=step)\n",
    "        \n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break\n",
    "    \n",
    "  print(\"Epochs: {} Elapsed time: {}\".format(epochs, time.time() - start_time))\n",
    "\n",
    "train_writer.close()\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slim: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_iterator\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "test_handle = sess.run(test_iterator.string_handle())\n",
    "sess.run(test_iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "accuracy, acc_op = tf.metrics.accuracy(labels=y, predictions=tf.argmax(logits, 1), name='accuracy')\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "sess.run(acc_op, feed_dict={handle: test_handle, is_training: False})\n",
    "print(\"test accuracy:\", sess.run(accuracy, feed_dict={handle: test_handle, is_training: False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAEcCAYAAADdpwmrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF/JKXjF1ZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "batch_xs = test_data[batch_index]\n",
    "y_pred = sess.run(logits, feed_dict={x: batch_xs, is_training: False})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  p.set_title(\"y_pred: {}\".format(np.argmax(py)))\n",
    "  p.imshow(px.reshape(28, 28), cmap='gray')\n",
    "  p.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 실습\n",
    "\n",
    "* 여러가지 hyper-parameter들을 바꿔가면서 accuracy를 높혀보자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
