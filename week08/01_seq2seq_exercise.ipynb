{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "Simple example for Seq2Seq (Machine Translation) by Encoder RNN and Decoder RNN.\n",
    "\n",
    "### Seq2Seq by Encoder RNN and Decoder RNN\n",
    "- Creating the **data pipeline** with `tf.data`\n",
    "- Preprocessing word sequences (variable input sequence length) using `padding technique` by `user function (pad_seq)`\n",
    "- Using `tf.nn.embedding_lookup` for getting vector of tokens (eg. word, character)\n",
    "- Training **many to many classification** with `tf.contrib.seq2seq.sequence_loss`\n",
    "- Masking unvalid token with `tf.sequence_mask`\n",
    "- Using `tf.contrib.seq2seq.dynamic_decode`\n",
    "- Training with `tf.contrib.seq2seq.TrainingHelper`\n",
    "- Translating with `tf.contriv.seq2seq.GreedyEmbeddingHelper`\n",
    "- Creating the model as **Class**\n",
    "- Reference\n",
    "    - https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/03%20-%20Seq2Seq.py\n",
    "    - https://github.com/HiJiGOO/tf_nmt_tutorial\n",
    "    - https://github.com/hccho2/RNN-Tutorial\n",
    "    - https://github.com/j-min/tf_tutorial_plus/tree/master/RNN_seq2seq/contrib_seq2seq\n",
    "    - https://gist.github.com/ilblackdragon/c92066d9d38b236a21d5a7b729a10f12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare example data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'framework': 9, '<pad>': 0, 'for': 8, 'changing': 3, 'learning': 12, 'feel': 7, 'I': 1, 'fast': 6, 'is': 11, 'deep': 4, 'very': 14, 'difficult': 5, 'a': 2, 'hungry': 10, 'tensorflow': 13}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# word dic for sentences\n",
    "source_words = []\n",
    "# source에 있는 각 단어들을 source_words 리스트에 하나씩 더한다.\n",
    "for elm in sources:\n",
    "    source_words += elm\n",
    "# set을 하여서 중복을 제거한 뒤 list를 다시 만든다.\n",
    "source_words = list(set(source_words))\n",
    "# sorting을 한다.\n",
    "source_words.sort()\n",
    "# pad special symbol을 추가한다.\n",
    "source_words = ['<pad>'] + source_words\n",
    "\n",
    "#단어-id 쌍의 dict를 만든다.\n",
    "source_dic = {word : idx for idx, word in enumerate(source_words)}\n",
    "print(source_dic)\n",
    "print(len(source_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'I',\n",
       " 2: 'a',\n",
       " 3: 'changing',\n",
       " 4: 'deep',\n",
       " 5: 'difficult',\n",
       " 6: 'fast',\n",
       " 7: 'feel',\n",
       " 8: 'for',\n",
       " 9: 'framework',\n",
       " 10: 'hungry',\n",
       " 11: 'is',\n",
       " 12: 'learning',\n",
       " 13: 'tensorflow',\n",
       " 14: 'very'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id-단어 쌍의 dict를 만든다.\n",
    "source_idx_dic = {elm[1] : elm[0] for elm in source_dic.items()}\n",
    "source_idx_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'배가': 7, '<pad>': 0, '매우': 6, '고프다': 3, '나는': 4, '어렵다': 10, '변화한다': 8, '위한': 11, '프레임워크이다': 13, '텐서플로우는': 12, '딥러닝을': 5, '빠르게': 9, '<start>': 1, '<end>': 2}\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# word dic for translations\n",
    "target_words = []\n",
    "# target sentence에서 단어를 하나씩 읽어서 target_words에 더한다.\n",
    "for elm in targets:\n",
    "    target_words += elm\n",
    "target_words = list(set(target_words))\n",
    "target_words.sort()\n",
    "target_words =  ['<pad>']+ ['<start>'] + ['<end>'] + \\\n",
    "                    target_words # 번역문의 시작과 끝을 알리는 'start', 'end' token 추가\n",
    "\n",
    "target_dic = {word : idx for idx, word in enumerate(target_words)}\n",
    "print(target_dic)\n",
    "print(len(target_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: '고프다',\n",
       " 4: '나는',\n",
       " 5: '딥러닝을',\n",
       " 6: '매우',\n",
       " 7: '배가',\n",
       " 8: '변화한다',\n",
       " 9: '빠르게',\n",
       " 10: '어렵다',\n",
       " 11: '위한',\n",
       " 12: '텐서플로우는',\n",
       " 13: '프레임워크이다'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_idx_dic = {elm[1] : elm[0] for elm in target_dic.items()}\n",
    "target_idx_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pad_seq function for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더에 들어가는 문장에 padding 주는 함수\n",
    "def pad_seq_enc(sequences, max_len, dic):\n",
    "    seq_len = []\n",
    "    seq_indices = []\n",
    "    # 문장 하나 하나를 읽는다\n",
    "    for seq in sequences:\n",
    "        # 한 문장의 길이\n",
    "        seq_len.append(len(seq))\n",
    "        # 한 문장의 단어의 id로 바꾸어 놓은 list를 만든다\n",
    "        seq_idx = [dic.get(word) for word in seq]\n",
    "        # 최대 길이를 채우지 못한 남은 부분은 <pad>에 해당하는 index로 채운다.\n",
    "        seq_idx += (max_len - len(seq_idx)) * [dic.get('<pad>')] \n",
    "        # 이렇게 만든 한문장의 idx를 리스트에 append한다.\n",
    "        seq_indices.append(seq_idx)        \n",
    "    return seq_len, seq_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더에 들어가는 문장에 padding\n",
    "def pad_seq_dec(sequences, max_len, dic):\n",
    "    seq_input_len = []\n",
    "    seq_input_indices = []\n",
    "    seq_target_indices = []\n",
    "    \n",
    "    # for decoder input\n",
    "    for seq in sequences:\n",
    "        # 각 문장 하나마다 단어 token에 해당하는 idx리스트를 구하고 \n",
    "        # 이 앞에 <start> symbol에 대한 idx 리스트를 더한다.\n",
    "        seq_input_idx = [dic.get('<start>')] + [dic.get(token) for token in seq]\n",
    "        # 이렇게 구한 idx list의 길이를 구한다.\n",
    "        seq_input_len.append(len(seq_input_idx))\n",
    "        # 최대 길이를 채우지 못한 남은 부분은 <pad>에 해당하는 idx로 채운다.\n",
    "        seq_input_idx += (max_len - len(seq_input_idx)) * [dic.get('<pad>')] \n",
    "        seq_input_indices.append(seq_input_idx)\n",
    "        \n",
    "    # for decoder output\n",
    "    for seq in sequences:\n",
    "        # 문장마다 단어의 idx로 바꾼 list에 <end>에 해당하는 idx를 추가한다.\n",
    "        seq_target_idx = [dic.get(token) for token in seq] + [dic.get('<end>')]\n",
    "        # 최대길이를 채우지 못한 경우 <pad>에 대한 idx를 더한다.\n",
    "        seq_target_idx += (max_len - len(seq_target_idx)) * [dic.get('<pad>')]\n",
    "        seq_target_indices.append(seq_target_idx)\n",
    "        \n",
    "    return seq_input_len, seq_input_indices, seq_target_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] (4, 10)\n",
      "[[1, 7, 10, 0, 0, 0, 0, 0, 0, 0],\n",
      " [13, 11, 14, 5, 0, 0, 0, 0, 0, 0],\n",
      " [13, 11, 2, 9, 8, 4, 12, 0, 0, 0],\n",
      " [13, 11, 14, 6, 3, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# for encoder\n",
    "source_max_len = 10\n",
    "X_length, X_indices = pad_seq_enc(sequences = sources, max_len = source_max_len, dic = source_dic)\n",
    "print(X_length, np.shape(X_indices))\n",
    "pprint(X_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 5, 5]\n",
      "[[1, 4, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 6, 10, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 5, 11, 13, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 6, 9, 8, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[4, 7, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 6, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 5, 11, 13, 2, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 6, 9, 8, 2, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# for decoder\n",
    "target_max_len = 12\n",
    "y_length, y_input_indices, y_target_indices = pad_seq_dec(sequences = targets, max_len = target_max_len,\n",
    "                                                             dic = target_dic)\n",
    "pprint(y_length)\n",
    "pprint(y_input_indices)\n",
    "pprint(y_target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dic['<end>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SimpleNMT\n",
    "Encoder RNN, Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNMT:\n",
    "    # source_dic, target_dic, target_max_len는 global variable\n",
    "    # target_max_len = 12\n",
    "    def __init__(self, s_len, s_indices, t_len, t_input_indices, t_output_indices,\n",
    "                 t_max_len = target_max_len, s_dic = source_dic, t_dic = target_dic,\n",
    "                 n_of_classes = len(target_dic), hidden_dim = 16):\n",
    "        \n",
    "        with tf.variable_scope('input_layer'):\n",
    "            # s : source, t : target\n",
    "            self._s_len = s_len\n",
    "            self._s_indices = s_indices\n",
    "            self._t_len = t_len\n",
    "            self._t_input_indices = t_input_indices\n",
    "            self._t_output_indices = t_output_indices\n",
    "            self._s_dic = s_dic\n",
    "            self._t_dic = t_dic\n",
    "            self._t_max_len = target_max_len\n",
    "            # tf.eye를 이용하세요\n",
    "            s_embeddings = tf.eye(len(self._s_dic), dtype=tf.float32) \n",
    "            # tf.get_variable을 이용하세요\n",
    "            s_embeddings = tf.get_variable(name='s_embedding', initializer=s_embeddings,\n",
    "                                          trainable=False)\n",
    "            # tf.nn.embedding_lookup을 이용하세요.\n",
    "            s_batch = tf.nn.embedding_lookup(params=s_embeddings, ids=self._s_indices) \n",
    "            \n",
    "        with tf.variable_scope('encoder'):\n",
    "            # tf.contrib.rnn.BasicRNNCell을 이용하세요\n",
    "            # 원래라면 num_units은 s_embeidding의 dimension으로 주어야 한다.\n",
    "            enc_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "            # tf.nn.dynamic_rnn을 이용하세요\n",
    "            # enc_state = [batch_size, n_step, hidden_size]이 아니라\n",
    "            # [batch_size, hidden_size]\n",
    "            _, self.enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=s_batch, \n",
    "                                             sequence_length=self._s_len,\n",
    "                                             dtype=tf.float32)\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('pipe'):\n",
    "            # tf.eye를 이용하세요\n",
    "            t_embeddings = tf.eye(len(self._t_dic), dtype=tf.float32)\n",
    "            # tf.get_variable을 이용하세요\n",
    "            t_embeddings = tf.get_variable(name='t_embedding', initializer=t_embeddings,\n",
    "                                          trainable=False)\n",
    "            # tf.nn.embedding_lookup을 이용하세요\n",
    "            t_batch = tf.nn.embedding_lookup(params=t_embeddings, ids=self._t_input_indices)\n",
    "            # _s_len 길이만큼 1이 들어간 텐서를 만든다.\n",
    "            # 1은 start token을 나타냄.\n",
    "            tokens = tf.ones_like(tensor = self._s_len, dtype = tf.int32) # idx 1 start_token\n",
    "            # 12만큼 곱한다. target_sequence의 최대길이 vector를 만듬.\n",
    "            tr_tokens = tf.map_fn(lambda elm : tf.multiply(elm, self._t_max_len), tokens)\n",
    "            trans_tokens = tokens\n",
    "        \n",
    "        with tf.variable_scope('decoder'):\n",
    "            # tf.contrib.rnn.BasicRNNCell를 이용하세요\n",
    "            dec_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "            # tf.contrib.rnn.OutputProjectionWrapper를 이용하세요\n",
    "            score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell=dec_cell, \n",
    "                                                                output_size=n_of_classes)\n",
    "            \n",
    "            with tf.variable_scope('training'):\n",
    "                # tf.contrib.seq2seq.TrainingHelper를 이용하세요\n",
    "                # sequence_length: An int32 vector tensor.\n",
    "                # max_len으로 맞춰서 output이 나오게 하려면 sequence_length를 써야한다.\n",
    "                # TrainingHelper는 매 step마다 ground truth값을 넣는다. \n",
    "                tr_helper = tf.contrib.seq2seq.TrainingHelper(inputs=t_batch,\n",
    "                                                              sequence_length=tr_tokens)\n",
    "                # tf.contrib.seq2seq.BasicDecoder를 이용하세요\n",
    "                tr_decoder = tf.contrib.seq2seq.BasicDecoder(cell=score_cell, \n",
    "                                                             helper=tr_helper, \n",
    "                                                             initial_state=self.enc_state)\n",
    "                # tf.contrib.seq2seq.dynamic_decode를 이용하세요\n",
    "                self._tr_outputs,self.final_state,_ = tf.contrib.seq2seq.dynamic_decode(decoder=tr_decoder)\n",
    "                \n",
    "            with tf.variable_scope('translation'):\n",
    "                \n",
    "                # tf.contrib.seq2seq.GreedyEmbeddingHelper를 이용하세요\n",
    "                # start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n",
    "                # end_token: `int32` scalar, the token that marks end of decoding.\n",
    "                # inference에 사용하는 helper로 \n",
    "                # 전 단계의 output의 argmax에 해당하는 결과를 다음 단계의 input으로 전달한다.\n",
    "                trans_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=t_embeddings, \n",
    "                                                                        start_tokens=trans_tokens, \n",
    "                                                                        end_token= target_dic['<end>'])\n",
    "                # tf.contrib.seq2seq.BasicDecoder를 이용하세요\n",
    "                trans_decoder = tf.contrib.seq2seq.BasicDecoder(cell=score_cell,\n",
    "                                                                helper=trans_helper,\n",
    "                                                                initial_state=self.enc_state)\n",
    "                # tf.contrib.seq2seq.dynamic_decode를 이용하세요\n",
    "                self._trans_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=trans_decoder)\n",
    "                \n",
    "        with tf.variable_scope('seq2seq_loss'):\n",
    "            # tf.sequence_mask\n",
    "            masking = tf.sequence_mask(lengths=self._t_len, maxlen=t_max_len, dtype=tf.float32)\n",
    "            # tf.contrib.seq2seq.sequence_loss를 이용하세요\n",
    "            self.seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits=self._tr_outputs.rnn_output,\n",
    "                                                                 targets=self._t_output_indices,\n",
    "                                                                 weights=masking)\n",
    "            \n",
    "    def translate(self, sess, s_len, s_indices):\n",
    "        feed_translation = {self._s_len : s_len, self._s_indices : s_indices}\n",
    "        return sess.run(self._trans_outputs.sample_id, feed_dict = feed_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model of SimpleNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter#\n",
    "lr = .003\n",
    "epochs = 200\n",
    "batch_size = 2\n",
    "total_step = int(np.shape(X_indices)[0] / batch_size)\n",
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?,), (?, 10), (?,), (?, 12), (?, 12)), types: (tf.int32, tf.int32, tf.int32, tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "## create data pipeline with tf.data\n",
    "# tf.data.Dataset.from_tensor_slices를 이용하세요\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((X_length, X_indices, y_length, \n",
    "                                                 y_input_indices, y_target_indices))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size = 20)\n",
    "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
    "tr_iterator = tr_dataset.make_initializable_iterator()\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_length_mb, X_indices_mb, y_length_mb, y_input_indices_mb, y_target_indices_mb = tr_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_nmt = SimpleNMT(s_len = X_length_mb, s_indices  = X_indices_mb,\n",
    "                    t_len = y_length_mb, t_input_indices = y_input_indices_mb,\n",
    "                    t_output_indices = y_target_indices_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:4' shape=(?, 12) dtype=int32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_nmt._t_output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder/translation/decoder/transpose:0' shape=(?, ?, 14) dtype=float32>, sample_id=<tf.Tensor 'decoder/translation/decoder/transpose_1:0' shape=(?, ?) dtype=int32>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_nmt._trans_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sim_nmt._trans_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder/translation/decoder/transpose:0' shape=(?, ?, 14) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch_size, target_max_len=12, n_of_classes(target_dict)=14]\n",
    "sim_nmt._trans_outputs.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder/rnn/while/Exit_3:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch_size, hidden_size = 16]\n",
    "sim_nmt.enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder/translation/decoder/transpose_1:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_nmt._trans_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder/training/decoder/while/Exit_3:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch_size, hidden_size]\n",
    "sim_nmt.final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat training op and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create training op\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "training_op = opt.minimize(loss = sim_nmt.seq2seq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  10, tr_loss : 2.112\n",
      "epoch :  20, tr_loss : 1.558\n",
      "epoch :  30, tr_loss : 1.089\n",
      "epoch :  40, tr_loss : 0.763\n",
      "epoch :  50, tr_loss : 0.546\n",
      "epoch :  60, tr_loss : 0.397\n",
      "epoch :  70, tr_loss : 0.294\n",
      "epoch :  80, tr_loss : 0.223\n",
      "epoch :  90, tr_loss : 0.173\n",
      "epoch : 100, tr_loss : 0.138\n",
      "epoch : 110, tr_loss : 0.111\n",
      "epoch : 120, tr_loss : 0.093\n",
      "epoch : 130, tr_loss : 0.078\n",
      "epoch : 140, tr_loss : 0.066\n",
      "epoch : 150, tr_loss : 0.058\n",
      "epoch : 160, tr_loss : 0.051\n",
      "epoch : 170, tr_loss : 0.045\n",
      "epoch : 180, tr_loss : 0.040\n",
      "epoch : 190, tr_loss : 0.036\n",
      "epoch : 200, tr_loss : 0.033\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    sess.run(tr_iterator.initializer)\n",
    "    try:\n",
    "        while True:\n",
    "            # 직접 작성해주세요\n",
    "            _, loss = sess.run([training_op, sim_nmt.seq2seq_loss])\n",
    "            avg_tr_loss += loss\n",
    "            tr_step += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    \n",
    "    avg_tr_loss /= tr_step\n",
    "    tr_loss_hist.append(avg_tr_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  7,  3,  2,  2],\n",
       "       [12,  6, 10,  2,  2],\n",
       "       [12,  5, 11, 13,  2],\n",
       "       [12,  6,  9,  8,  2]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = sim_nmt.translate(sess = sess, s_len = X_length, s_indices = X_indices)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '배가', '고프다', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['텐서플로우는', '매우', '어렵다', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['텐서플로우는', '딥러닝을', '위한', '프레임워크이다', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['텐서플로우는', '매우', '빠르게', '변화한다', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# 원래 문장\n",
    "# y_target_indices의 한 elem에 있는 idx값에 대해 idx-word dict를 적용\n",
    "originals = list(map(lambda elm : [target_idx_dic.get(idx) for idx in elm], y_target_indices))\n",
    "for original in originals:\n",
    "    print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '배가', '고프다', '<end>', '<end>']\n",
      "['텐서플로우는', '매우', '어렵다', '<end>', '<end>']\n",
      "['텐서플로우는', '딥러닝을', '위한', '프레임워크이다', '<end>']\n",
      "['텐서플로우는', '매우', '빠르게', '변화한다', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# 한글 넣은 번역문장\n",
    "translations = list(map(lambda elm : [target_idx_dic.get(idx) for idx in elm], yhat))\n",
    "for translation in translations:\n",
    "    print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
