{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "DATA_FILE = './birth_life_2010.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : read in data from the .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, n_samples = utils.read_birth_life_data(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : create placeholders for X(birth rate) and Y(life expectancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32, shape=None, name='x')\n",
    "Y = tf.placeholder(tf.float32, shape=None, name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 : create weight and bias, initialized to 0.,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.get_variable(name='w', dtype=None, initializer=tf.constant(0.0))\n",
    "b = tf.get_variable(name='b', dtype=None, initializer=tf.constant(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 : build model to predict Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = w*X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 : use the square error as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.square(Y - Y_predicted, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 : using gradient descent with learning rate of 0.001 to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Operation"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1661.8637834631543\n",
      "Epoch 1: 956.3224148609137\n",
      "Epoch 2: 844.6737023980994\n",
      "Epoch 3: 750.7312486011339\n",
      "Epoch 4: 667.6598341012079\n",
      "Epoch 5: 594.1417715627896\n",
      "Epoch 6: 529.07878103068\n",
      "Epoch 7: 471.5004191489204\n",
      "Epoch 8: 420.5458626462441\n",
      "Epoch 9: 375.45530721966765\n",
      "Epoch 10: 335.5543025185697\n",
      "Epoch 11: 300.24629857978107\n",
      "Epoch 12: 269.00376475843336\n",
      "Epoch 13: 241.35957466852116\n",
      "Epoch 14: 216.90039135300015\n",
      "Epoch 15: 195.25972298129324\n",
      "Epoch 16: 176.1137693605349\n",
      "Epoch 17: 159.17551693441837\n",
      "Epoch 18: 144.1907111125557\n",
      "Epoch 19: 130.93503488078713\n",
      "Epoch 20: 119.20935661137888\n",
      "Epoch 21: 108.8379309807855\n",
      "Epoch 22: 99.66466760624593\n",
      "Epoch 23: 91.55177013029001\n",
      "Epoch 24: 84.37664046781751\n",
      "Epoch 25: 78.03217824997724\n",
      "Epoch 26: 72.42182927812989\n",
      "Epoch 27: 67.46136239485718\n",
      "Epoch 28: 63.07566952367442\n",
      "Epoch 29: 59.19874146522856\n",
      "Epoch 30: 55.77168446383194\n",
      "Epoch 31: 52.74269822355127\n",
      "Epoch 32: 50.065632780875376\n",
      "Epoch 33: 47.70006421631674\n",
      "Epoch 34: 45.61017902122909\n",
      "Epoch 35: 43.76379750625255\n",
      "Epoch 36: 42.13259221098116\n",
      "Epoch 37: 40.69221939330516\n",
      "Epoch 38: 39.420219863367905\n",
      "Epoch 39: 38.297008645340895\n",
      "Epoch 40: 37.305591759538146\n",
      "Epoch 41: 36.43066341609841\n",
      "Epoch 42: 35.658453942681234\n",
      "Epoch 43: 34.97724816803575\n",
      "Epoch 44: 34.37655378567349\n",
      "Epoch 45: 33.84671358035044\n",
      "Epoch 46: 33.379665882282545\n",
      "Epoch 47: 32.96800991297258\n",
      "Epoch 48: 32.60548541990942\n",
      "Epoch 49: 32.28618434173986\n",
      "Epoch 50: 32.004961317298495\n",
      "Epoch 51: 31.757531331044525\n",
      "Epoch 52: 31.53978877073019\n",
      "Epoch 53: 31.348356819100445\n",
      "Epoch 54: 31.180119247269193\n",
      "Epoch 55: 31.03225782010038\n",
      "Epoch 56: 30.902462910201574\n",
      "Epoch 57: 30.78859985760776\n",
      "Epoch 58: 30.688725355066556\n",
      "Epoch 59: 30.60122861903357\n",
      "Epoch 60: 30.524590178362192\n",
      "Epoch 61: 30.457532704476954\n",
      "Epoch 62: 30.398967422668726\n",
      "Epoch 63: 30.34777825418737\n",
      "Epoch 64: 30.303121465726413\n",
      "Epoch 65: 30.26424930739051\n",
      "Epoch 66: 30.230392129550456\n",
      "Epoch 67: 30.200964921590334\n",
      "Epoch 68: 30.175501555469697\n",
      "Epoch 69: 30.153343991707324\n",
      "Epoch 70: 30.134226098457216\n",
      "Epoch 71: 30.117758308603477\n",
      "Epoch 72: 30.103543774372174\n",
      "Epoch 73: 30.09139442229674\n",
      "Epoch 74: 30.0809388476427\n",
      "Epoch 75: 30.07208499982095\n",
      "Epoch 76: 30.06452690966084\n",
      "Epoch 77: 30.058150938555205\n",
      "Epoch 78: 30.05278219980139\n",
      "Epoch 79: 30.04828310612785\n",
      "Epoch 80: 30.04458791257593\n",
      "Epoch 81: 30.041550708114855\n",
      "Epoch 82: 30.039046437352113\n",
      "Epoch 83: 30.03704103724602\n",
      "Epoch 84: 30.03545715799831\n",
      "Epoch 85: 30.034288759106282\n",
      "Epoch 86: 30.03338805212261\n",
      "Epoch 87: 30.032769865304076\n",
      "Epoch 88: 30.032386838833535\n",
      "Epoch 89: 30.032150670733166\n",
      "Epoch 90: 30.032092865493677\n",
      "Epoch 91: 30.032186730024037\n",
      "Epoch 92: 30.03240725137661\n",
      "Epoch 93: 30.032643962397827\n",
      "Epoch 94: 30.033039376884087\n",
      "Epoch 95: 30.033435566514413\n",
      "Epoch 96: 30.033922631802085\n",
      "Epoch 97: 30.03442924663878\n",
      "Epoch 98: 30.0349335548615\n",
      "Epoch 99: 30.03552558278714\n",
      "Took: 12.441733 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Create a filewrite to write the model's graph to Tensorboard\n",
    "writer = tf.summary.FileWriter('./board/linear', tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 7 :  Initialize the necessary variables\n",
    "    # sess.run(tf.variables_initializer([w, b]))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Step 8 : train the model for 100 epochs\n",
    "    for i in range(100):\n",
    "        total_loss = 0\n",
    "        for x, y in data:\n",
    "            # Execute train_op and get the value of loss.\n",
    "            _, loss_ = sess.run([optimizer, loss], feed_dict={X: x, Y: y})\n",
    "            total_loss += loss_\n",
    "            \n",
    "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "        \n",
    "    writer.close()\n",
    "    \n",
    "    # Step 9 :output the values of w and b\n",
    "    w_out, b_out = sess.run([w, b])\n",
    "    \n",
    "print('Took: %f seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.0702143"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25952291358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data[:,0], data[:,1], 'bo', label='Real data')\n",
    "plt.plot(data[:,0], data[:,0]*w_out+b_out, 'r', label='Predicted data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber loss\n",
    "\n",
    "Robust to outliers\n",
    "\n",
    "If the differences between the predicted value and the real value is small, square it\n",
    "\n",
    "If it's large, take its absolute value\n",
    "\n",
    "**tf.cond(pred, fn1, fn2, name=None)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(lables, predictions, delta=14.0):\n",
    "    residual = tf.abs(labels - predictions)\n",
    "    def f1(): return 0.5*tf.square(residual)\n",
    "    def f2(): return delta*residual - 0.5 * tf.square(delta)\n",
    "    return tf.cond(residual < delta, f1, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Control Flow\n",
    "\n",
    "Since TF builds graph before computation, we have to specify all possible subgraphs beforehand.\n",
    "\n",
    "- `Control Flow Ops: tf.cond, tf.group`\n",
    "\n",
    "- `Comparison Ops : tf.equal, tf.not_equal, tf.greater`\n",
    "\n",
    "- `Logical Ops : tf.logical_and, tf.logical_not, tf.logical_or, tf.logical_xor`\n",
    "\n",
    "- `Debugging Ops : tf.is_infiite, tf.is_inf, tf.is_nan,..`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.tf.data\n",
    "\n",
    "Instead of doing inference with placeholders and feeding in data later, do inference directly with data.\n",
    "\n",
    "**tf.data.Dataset**\n",
    "\n",
    "- `tf.data.Dataset.from_tensor_slices((features, labels))`\n",
    "\n",
    "- `tf.data.Dataset.from_generator(gen, output_types, output_shape)`\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "\n",
    "**tf.data.Iterator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder \n",
    "\n",
    "Pros : put the data processing outside Tensorflow, making it easy to do in Python\n",
    "\n",
    "Cons : users often end up processing their data in a single thread and creating data bottleneck that slows exceution down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape([]), TensorShape([]))\n"
     ]
    }
   ],
   "source": [
    "print(dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can also create Dataset from files\n",
    "\n",
    "**tf.data.TextLineDataset(filenames)**\n",
    "\n",
    "**tf.data.FixedLengthRecordDataset(filenames)**\n",
    "\n",
    "**tf.data.TFRecordDataset(filenames)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data.Iterator\n",
    "\n",
    "Craete an iterator to iterate through samples in Dataset\n",
    "\n",
    "- `iterator = dataset.make_one_shot_iterator()`\n",
    "\n",
    "Iterator through the dataset exactly once. No need to initialization\n",
    "\n",
    "정확히 한번만 Iteration을 할 수 있는데 초기화가 필요없다.\n",
    "\n",
    "- `iterator = dataset.make_initializable_iterator()`\n",
    "\n",
    "Iterates through the dataset as many as we want. Need to initialize with each epoch\n",
    "\n",
    "원하는 만큼 계속 Iteration을 할 수 있는데 초기화가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "print(type(X))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([X, Y]))\n",
    "    print(sess.run([X, Y]))\n",
    "    print(sess.run([X, Y]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.822, 3.869, 3.911], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74.82825, 70.81949, 72.15066], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:3,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style1\n",
    "\n",
    "```Python\n",
    "while True:\n",
    "    sess.run(iterator.initializer())\n",
    "    try:\n",
    "        sess.run([result])\n",
    "    except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "Epoch 0: 712.5349964342619\n",
      "Epoch 1: 480.17684252550333\n",
      "Epoch 2: 411.2783675131045\n",
      "Epoch 3: 381.1438026114514\n",
      "Epoch 4: 363.965206889867\n",
      "Epoch 5: 352.0028216342001\n",
      "Epoch 6: 342.65910039112754\n",
      "Epoch 7: 334.84100784196664\n",
      "Epoch 8: 327.9518736341282\n",
      "Epoch 9: 321.63482000235666\n",
      "Epoch 10: 315.6941960449968\n",
      "Epoch 11: 310.0039571509844\n",
      "Epoch 12: 304.4644903231608\n",
      "Epoch 13: 299.02843485184405\n",
      "Epoch 14: 293.6694610852944\n",
      "Epoch 15: 288.3639351369519\n",
      "Epoch 16: 283.09333086735325\n",
      "Epoch 17: 277.85642405654255\n",
      "Epoch 18: 272.6439196676016\n",
      "Epoch 19: 267.4467619214795\n",
      "Epoch 20: 262.2604258420485\n",
      "Epoch 21: 257.0869437708936\n",
      "Epoch 22: 251.9340898333877\n",
      "Epoch 23: 246.7978208001213\n",
      "Epoch 24: 241.68042055982608\n",
      "Epoch 25: 236.58248748748417\n",
      "Epoch 26: 231.5038126934712\n",
      "Epoch 27: 226.446897909256\n",
      "Epoch 28: 221.41354965849007\n",
      "Epoch 29: 216.40121694964014\n",
      "Epoch 30: 211.41856330033195\n",
      "Epoch 31: 206.4636707354533\n",
      "Epoch 32: 201.52567135660271\n",
      "Epoch 33: 196.60663291278638\n",
      "Epoch 34: 191.69951618658868\n",
      "Epoch 35: 186.81956688087237\n",
      "Epoch 36: 181.97026153144083\n",
      "Epoch 37: 177.15823659771368\n",
      "Epoch 38: 172.36514927776236\n",
      "Epoch 39: 167.59127469086334\n",
      "Epoch 40: 162.846115277237\n",
      "Epoch 41: 158.130295219939\n",
      "Epoch 42: 153.44470056146383\n",
      "Epoch 43: 148.78938361339664\n",
      "Epoch 44: 144.17467782383687\n",
      "Epoch 45: 139.59350521709572\n",
      "Epoch 46: 135.0799269277359\n",
      "Epoch 47: 130.61376950433967\n",
      "Epoch 48: 126.20577719486073\n",
      "Epoch 49: 121.83948346442224\n",
      "Epoch 50: 117.53008210936356\n",
      "Epoch 51: 113.27451068002203\n",
      "Epoch 52: 109.09293648436372\n",
      "Epoch 53: 104.98878282097454\n",
      "Epoch 54: 100.96831140439761\n",
      "Epoch 55: 97.04090395072033\n",
      "Epoch 56: 93.20598700570653\n",
      "Epoch 57: 89.4480488677068\n",
      "Epoch 58: 85.79511271075002\n",
      "Epoch 59: 82.24485430505715\n",
      "Epoch 60: 78.81007276318086\n",
      "Epoch 61: 75.46468113892662\n",
      "Epoch 62: 72.22824622276974\n",
      "Epoch 63: 69.09702040678576\n",
      "Epoch 64: 66.07316091323369\n",
      "Epoch 65: 63.17249699153364\n",
      "Epoch 66: 60.39144995697823\n",
      "Epoch 67: 57.73244161395429\n",
      "Epoch 68: 55.1888651488888\n",
      "Epoch 69: 52.765521257783064\n",
      "Epoch 70: 50.44132328157951\n",
      "Epoch 71: 48.23246335056856\n",
      "Epoch 72: 46.147970477846414\n",
      "Epoch 73: 44.15656507492653\n",
      "Epoch 74: 42.26910537076428\n",
      "Epoch 75: 40.49456375182459\n",
      "Epoch 76: 38.82137250063549\n",
      "Epoch 77: 37.26528264113555\n",
      "Epoch 78: 35.80450206644049\n",
      "Epoch 79: 34.42862613410214\n",
      "Epoch 80: 33.14077403210119\n",
      "Epoch 81: 31.9352735351477\n",
      "Epoch 82: 30.80680125493397\n",
      "Epoch 83: 29.74883579689593\n",
      "Epoch 84: 28.75753156554542\n",
      "Epoch 85: 27.82884260135899\n",
      "Epoch 86: 26.958930806568087\n",
      "Epoch 87: 26.143821237912697\n",
      "Epoch 88: 25.38612711686935\n",
      "Epoch 89: 24.68093857832351\n",
      "Epoch 90: 24.019699340203537\n",
      "Epoch 91: 23.41134356741769\n",
      "Epoch 92: 22.841170396880205\n",
      "Epoch 93: 22.308239167192216\n",
      "Epoch 94: 21.810647152839717\n",
      "Epoch 95: 21.345948222237883\n",
      "Epoch 96: 20.91201356974749\n",
      "Epoch 97: 20.5068725735208\n",
      "Epoch 98: 20.12850519813058\n",
      "Epoch 99: 19.776658200199325\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "data, n_samples = utils.read_birth_life_data(DATA_FILE)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# X= tf.placeholder(tf.float32, shape=None, name='x')\n",
    "# Y = tf.placeholder(tf.float32, shape=None, name='y')\n",
    "\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "w = tf.get_variable(name='w', dtype=None, initializer=tf.constant(0.0))\n",
    "b = tf.get_variable(name='b', dtype=None, initializer=tf.constant(0.0))\n",
    "\n",
    "Y_predicted = w*X + b\n",
    "\n",
    "\n",
    "def huber_loss(labels, predictions, delta=14.0):\n",
    "    residual = tf.abs(labels - predictions)\n",
    "    def f1(): return 0.5*tf.square(residual)\n",
    "    def f2(): return delta*residual - 0.5 * tf.square(delta)\n",
    "    return tf.cond(residual < delta, f1, f2)\n",
    "\n",
    "loss = huber_loss(Y, Y_predicted)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "\n",
    "print(iterator.output_types)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config =config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        sess.run(iterator.initializer)\n",
    "        total_loss = 0\n",
    "        while True:\n",
    "            try:\n",
    "                _, loss_ = sess.run([optimizer, loss])\n",
    "                total_loss += loss_\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "        # Step 9 :output the values of w and b\n",
    "        w_out, b_out = sess.run([w, b])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style2\n",
    "\n",
    "```Python\n",
    "sess.run(iterator.intializer)\n",
    "try:\n",
    "    while True:\n",
    "        sess.run([result])\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    pass\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "Epoch 0: 712.5349964342619\n",
      "Epoch 1: 480.17684252550333\n",
      "Epoch 2: 411.2783675131045\n",
      "Epoch 3: 381.1438026114514\n",
      "Epoch 4: 363.965206889867\n",
      "Epoch 5: 352.0028216342001\n",
      "Epoch 6: 342.65910039112754\n",
      "Epoch 7: 334.84100784196664\n",
      "Epoch 8: 327.9518736341282\n",
      "Epoch 9: 321.63482000235666\n",
      "Epoch 10: 315.6941960449968\n",
      "Epoch 11: 310.0039571509844\n",
      "Epoch 12: 304.4644903231608\n",
      "Epoch 13: 299.02843485184405\n",
      "Epoch 14: 293.6694610852944\n",
      "Epoch 15: 288.3639351369519\n",
      "Epoch 16: 283.09333086735325\n",
      "Epoch 17: 277.85642405654255\n",
      "Epoch 18: 272.6439196676016\n",
      "Epoch 19: 267.4467619214795\n",
      "Epoch 20: 262.2604258420485\n",
      "Epoch 21: 257.0869437708936\n",
      "Epoch 22: 251.9340898333877\n",
      "Epoch 23: 246.7978208001213\n",
      "Epoch 24: 241.68042055982608\n",
      "Epoch 25: 236.58248748748417\n",
      "Epoch 26: 231.5038126934712\n",
      "Epoch 27: 226.446897909256\n",
      "Epoch 28: 221.41354965849007\n",
      "Epoch 29: 216.40121694964014\n",
      "Epoch 30: 211.41856330033195\n",
      "Epoch 31: 206.4636707354533\n",
      "Epoch 32: 201.52567135660271\n",
      "Epoch 33: 196.60663291278638\n",
      "Epoch 34: 191.69951618658868\n",
      "Epoch 35: 186.81956688087237\n",
      "Epoch 36: 181.97026153144083\n",
      "Epoch 37: 177.15823659771368\n",
      "Epoch 38: 172.36514927776236\n",
      "Epoch 39: 167.59127469086334\n",
      "Epoch 40: 162.846115277237\n",
      "Epoch 41: 158.130295219939\n",
      "Epoch 42: 153.44470056146383\n",
      "Epoch 43: 148.78938361339664\n",
      "Epoch 44: 144.17467782383687\n",
      "Epoch 45: 139.59350521709572\n",
      "Epoch 46: 135.0799269277359\n",
      "Epoch 47: 130.61376950433967\n",
      "Epoch 48: 126.20577719486073\n",
      "Epoch 49: 121.83948346442224\n",
      "Epoch 50: 117.53008210936356\n",
      "Epoch 51: 113.27451068002203\n",
      "Epoch 52: 109.09293648436372\n",
      "Epoch 53: 104.98878282097454\n",
      "Epoch 54: 100.96831140439761\n",
      "Epoch 55: 97.04090395072033\n",
      "Epoch 56: 93.20598700570653\n",
      "Epoch 57: 89.4480488677068\n",
      "Epoch 58: 85.79511271075002\n",
      "Epoch 59: 82.24485430505715\n",
      "Epoch 60: 78.81007276318086\n",
      "Epoch 61: 75.46468113892662\n",
      "Epoch 62: 72.22824622276974\n",
      "Epoch 63: 69.09702040678576\n",
      "Epoch 64: 66.07316091323369\n",
      "Epoch 65: 63.17249699153364\n",
      "Epoch 66: 60.39144995697823\n",
      "Epoch 67: 57.73244161395429\n",
      "Epoch 68: 55.1888651488888\n",
      "Epoch 69: 52.765521257783064\n",
      "Epoch 70: 50.44132328157951\n",
      "Epoch 71: 48.23246335056856\n",
      "Epoch 72: 46.147970477846414\n",
      "Epoch 73: 44.15656507492653\n",
      "Epoch 74: 42.26910537076428\n",
      "Epoch 75: 40.49456375182459\n",
      "Epoch 76: 38.82137250063549\n",
      "Epoch 77: 37.26528264113555\n",
      "Epoch 78: 35.80450206644049\n",
      "Epoch 79: 34.42862613410214\n",
      "Epoch 80: 33.14077403210119\n",
      "Epoch 81: 31.9352735351477\n",
      "Epoch 82: 30.80680125493397\n",
      "Epoch 83: 29.74883579689593\n",
      "Epoch 84: 28.75753156554542\n",
      "Epoch 85: 27.82884260135899\n",
      "Epoch 86: 26.958930806568087\n",
      "Epoch 87: 26.143821237912697\n",
      "Epoch 88: 25.38612711686935\n",
      "Epoch 89: 24.68093857832351\n",
      "Epoch 90: 24.019699340203537\n",
      "Epoch 91: 23.41134356741769\n",
      "Epoch 92: 22.841170396880205\n",
      "Epoch 93: 22.308239167192216\n",
      "Epoch 94: 21.810647152839717\n",
      "Epoch 95: 21.345948222237883\n",
      "Epoch 96: 20.91201356974749\n",
      "Epoch 97: 20.5068725735208\n",
      "Epoch 98: 20.12850519813058\n",
      "Epoch 99: 19.776658200199325\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "data, n_samples = utils.read_birth_life_data(DATA_FILE)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# X= tf.placeholder(tf.float32, shape=None, name='x')\n",
    "# Y = tf.placeholder(tf.float32, shape=None, name='y')\n",
    "\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "w = tf.get_variable(name='w', dtype=None, initializer=tf.constant(0.0))\n",
    "b = tf.get_variable(name='b', dtype=None, initializer=tf.constant(0.0))\n",
    "\n",
    "Y_predicted = w*X + b\n",
    "\n",
    "\n",
    "def huber_loss(labels, predictions, delta=14.0):\n",
    "    residual = tf.abs(labels - predictions)\n",
    "    def f1(): return 0.5*tf.square(residual)\n",
    "    def f2(): return delta*residual - 0.5 * tf.square(delta)\n",
    "    return tf.cond(residual < delta, f1, f2)\n",
    "\n",
    "loss = huber_loss(Y, Y_predicted)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "\n",
    "print(iterator.output_types)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config =config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        sess.run(iterator.initializer)\n",
    "        total_loss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, loss_ = sess.run([optimizer, loss])\n",
    "                total_loss += loss_\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "        # Step 9 :output the values of w and b\n",
    "        w_out, b_out = sess.run([w, b])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling data in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.822   , 74.82825 ],\n",
       "       [ 3.869   , 70.81949 ],\n",
       "       [ 3.911   , 72.15066 ],\n",
       "       [ 5.578   , 61.999855],\n",
       "       [ 1.579   , 73.92766 ],\n",
       "       [ 4.229   , 67.465195],\n",
       "       [ 1.15    , 81.641464],\n",
       "       [ 3.86    , 72.30639 ],\n",
       "       [ 3.142   , 68.484314],\n",
       "       [ 3.951   , 62.44061 ],\n",
       "       [ 2.16    , 80.70244 ],\n",
       "       [ 2.141   , 76.30168 ],\n",
       "       [ 2.002   , 64.662094],\n",
       "       [ 2.504   , 68.19498 ],\n",
       "       [ 3.451   , 68.76483 ],\n",
       "       [ 2.635   , 74.02456 ],\n",
       "       [ 1.092   , 80.775314],\n",
       "       [ 2.747   , 67.064   ],\n",
       "       [ 1.22    , 80.76195 ],\n",
       "       [ 1.39    , 82.932686],\n",
       "       [ 2.117   , 68.889656],\n",
       "       [ 1.108   , 82.87805 ],\n",
       "       [ 2.47    , 75.99427 ],\n",
       "       [ 2.09    , 75.07688 ],\n",
       "       [ 2.668   , 69.22583 ],\n",
       "       [ 2.022   , 68.53214 ],\n",
       "       [ 1.598   , 73.273094],\n",
       "       [ 2.581   , 62.53622 ],\n",
       "       [ 2.042   , 77.93202 ],\n",
       "       [ 1.92    , 81.69512 ],\n",
       "       [ 2.499   , 68.001   ],\n",
       "       [ 1.94    , 80.402435],\n",
       "       [ 1.445   , 70.27561 ],\n",
       "       [ 2.399   , 64.86351 ],\n",
       "       [ 2.088   , 73.696655],\n",
       "       [ 3.297   , 67.2599  ],\n",
       "       [ 1.5     , 82.24634 ],\n",
       "       [ 1.98    , 81.45122 ],\n",
       "       [ 1.39    , 81.62683 ],\n",
       "       [ 1.57    , 79.42195 ],\n",
       "       [ 1.4     , 75.1122  ],\n",
       "       [ 1.4     , 73.936584],\n",
       "       [ 1.54    , 68.80488 ],\n",
       "       [ 1.38    , 73.458534],\n",
       "       [ 1.32    , 79.02683 ],\n",
       "       [ 1.38    , 76.24634 ],\n",
       "       [ 1.95    , 80.99756 ],\n",
       "       [ 1.79    , 80.70244 ],\n",
       "       [ 1.656   , 74.3111  ],\n",
       "       [ 1.475   , 68.90371 ],\n",
       "       [ 1.422   , 74.61885 ],\n",
       "       [ 1.63    , 80.08781 ],\n",
       "       [ 1.55    , 73.268295],\n",
       "       [ 1.17    , 73.482925],\n",
       "       [ 2.898   , 69.36846 ],\n",
       "       [ 2.2893  , 69.88439 ],\n",
       "       [ 2.59    , 68.295364],\n",
       "       [ 1.4     , 81.73659 ],\n",
       "       [ 2.07    , 80.291954],\n",
       "       [ 2.2     , 81.45122 ],\n",
       "       [ 1.25    , 74.20731 ],\n",
       "       [ 1.44    , 80.3878  ],\n",
       "       [ 1.39    , 79.98781 ],\n",
       "       [ 1.555   , 73.32734 ],\n",
       "       [ 2.      , 81.36829 ],\n",
       "       [ 1.87    , 79.870735],\n",
       "       [ 1.63    , 75.42927 ],\n",
       "       [ 1.87    , 79.1     ],\n",
       "       [ 1.49    , 77.42439 ],\n",
       "       [ 1.476   , 79.380394],\n",
       "       [ 1.46    , 76.47561 ],\n",
       "       [ 1.46    , 79.832   ],\n",
       "       [ 1.49    , 73.51219 ],\n",
       "       [ 1.148   , 75.40044 ],\n",
       "       [ 1.84    , 79.936584],\n",
       "       [ 1.44    , 70.40488 ],\n",
       "       [ 2.3     , 70.506516],\n",
       "       [ 1.44    , 80.38293 ],\n",
       "       [ 1.736   , 73.78356 ],\n",
       "       [ 1.536   , 76.90095 ],\n",
       "       [ 1.8     , 79.18944 ],\n",
       "       [ 2.49    , 74.12732 ],\n",
       "       [ 1.9856  , 76.23683 ],\n",
       "       [ 1.639   , 69.755   ],\n",
       "       [ 1.896   , 75.22212 ],\n",
       "       [ 2.336   , 70.33532 ],\n",
       "       [ 2.064   , 72.11253 ],\n",
       "       [ 1.98    , 74.4399  ],\n",
       "       [ 1.797   , 78.91329 ],\n",
       "       [ 2.501   , 73.76498 ],\n",
       "       [ 2.954   , 72.277   ],\n",
       "       [ 2.484   , 75.97424 ],\n",
       "       [ 2.622   , 73.72922 ],\n",
       "       [ 2.32    , 76.68378 ],\n",
       "       [ 2.329   , 72.84712 ],\n",
       "       [ 3.139   , 72.82593 ],\n",
       "       [ 3.34    , 61.763   ],\n",
       "       [ 2.262   , 69.54915 ],\n",
       "       [ 3.983   , 70.82542 ],\n",
       "       [ 2.235   , 75.66044 ],\n",
       "       [ 2.25    , 71.73237 ],\n",
       "       [ 2.479   , 75.462296],\n",
       "       [ 2.58    , 73.20003 ],\n",
       "       [ 1.467   , 78.96415 ],\n",
       "       [ 1.848   , 79.19261 ],\n",
       "       [ 2.1     , 73.42968 ],\n",
       "       [ 1.862   , 78.885735],\n",
       "       [ 1.83    , 73.09953 ],\n",
       "       [ 3.348   , 66.26856 ],\n",
       "       [ 2.79    , 75.83995 ],\n",
       "       [ 1.551   , 76.57283 ],\n",
       "       [ 1.699   , 74.975174],\n",
       "       [ 2.211   , 75.63215 ],\n",
       "       [ 5.2     , 65.030464],\n",
       "       [ 4.453   , 72.643684],\n",
       "       [ 1.749   , 76.57361 ],\n",
       "       [ 2.04    , 74.6     ],\n",
       "       [ 2.934   , 75.70256 ],\n",
       "       [ 2.811   , 73.85042 ],\n",
       "       [ 2.271   , 78.09759 ],\n",
       "       [ 2.309   , 73.12461 ],\n",
       "       [ 2.279   , 71.86463 ],\n",
       "       [ 1.38    , 80.948784],\n",
       "       [ 2.564   , 74.75312 ],\n",
       "       [ 1.8     , 72.40875 ],\n",
       "       [ 2.295   , 74.60473 ],\n",
       "       [ 3.8     , 73.28966 ],\n",
       "       [ 3.03    , 81.504875],\n",
       "       [ 4.702   , 68.486046],\n",
       "       [ 1.67    , 72.751854],\n",
       "       [ 2.733   , 72.975266],\n",
       "       [ 3.75    , 57.52739 ],\n",
       "       [ 2.54    , 75.02383 ],\n",
       "       [ 2.264   , 72.85254 ],\n",
       "       [ 2.1     , 78.24146 ],\n",
       "       [ 1.677   , 80.797806],\n",
       "       [ 1.764   , 79.288536],\n",
       "       [ 2.313   , 74.72261 ],\n",
       "       [ 3.423   , 65.19885 ],\n",
       "       [ 2.727   , 68.39483 ],\n",
       "       [ 1.752   , 76.551414],\n",
       "       [ 2.625   , 65.13134 ],\n",
       "       [ 2.399   , 66.90885 ],\n",
       "       [ 2.245   , 68.6348  ],\n",
       "       [ 6.288   , 48.282196],\n",
       "       [ 3.29    , 49.860878],\n",
       "       [ 6.258   , 48.455486],\n",
       "       [ 6.149   , 53.614635],\n",
       "       [ 4.072   , 56.588707],\n",
       "       [ 4.896   , 58.160023],\n",
       "       [ 5.544   , 57.38749 ],\n",
       "       [ 3.364   , 48.342804],\n",
       "       [ 4.4     , 61.108242],\n",
       "       [ 2.458   , 52.08149 ],\n",
       "       [ 6.339   , 50.89554 ],\n",
       "       [ 4.982   , 47.402195],\n",
       "       [ 2.5     , 73.03415 ],\n",
       "       [ 4.819   , 58.954075],\n",
       "       [ 5.371   , 55.05712 ],\n",
       "       [ 5.525   , 51.410023],\n",
       "       [ 7.063   , 54.265633],\n",
       "       [ 3.217   , 62.0701  ],\n",
       "       [ 4.912   , 49.696926],\n",
       "       [ 1.47    , 72.967316],\n",
       "       [ 4.533   , 58.21695 ],\n",
       "       [ 6.294   , 50.95483 ],\n",
       "       [ 5.99    , 53.462635],\n",
       "       [ 4.651   , 66.46707 ],\n",
       "       [ 5.238   , 56.147587],\n",
       "       [ 3.199   , 47.365074],\n",
       "       [ 4.718   , 56.497074],\n",
       "       [ 5.063   , 47.700657],\n",
       "       [ 5.246   , 53.638584],\n",
       "       [ 4.17    , 63.83727 ],\n",
       "       [ 3.25    , 62.286682],\n",
       "       [ 4.193   , 58.7151  ],\n",
       "       [ 4.453   , 60.994194],\n",
       "       [ 5.185   , 50.840805],\n",
       "       [ 5.775   , 48.069584],\n",
       "       [ 4.544   , 56.960194],\n",
       "       [ 4.919   , 60.626266],\n",
       "       [ 5.981   , 49.194828],\n",
       "       [ 4.631   , 47.61846 ],\n",
       "       [ 2.405   , 73.77405 ],\n",
       "       [ 4.487   , 51.062756],\n",
       "       [ 4.338   , 49.87722 ],\n",
       "       [ 5.85    , 54.924194],\n",
       "       [ 2.75    , 53.109512],\n",
       "       [ 5.287   , 55.585587],\n",
       "       [ 5.443   , 50.65366 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, n_samples = utils.read_birth_life_data(DATA_FILE)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n",
      "[5.578, 61.999855]\n",
      "[1.579, 73.92766]\n",
      "[4.229, 67.465195]\n",
      "[1.15, 81.641464]\n",
      "[3.86, 72.30639]\n",
      "[3.142, 68.484314]\n",
      "[3.951, 62.44061]\n",
      "[2.16, 80.70244]\n",
      "[2.141, 76.30168]\n",
      "[2.002, 64.662094]\n",
      "[2.504, 68.19498]\n",
      "[3.451, 68.76483]\n",
      "[2.635, 74.02456]\n",
      "[1.092, 80.775314]\n",
      "[2.747, 67.064]\n",
      "[1.22, 80.76195]\n",
      "[1.39, 82.932686]\n",
      "[2.117, 68.889656]\n",
      "[1.108, 82.87805]\n",
      "[2.47, 75.99427]\n",
      "[2.09, 75.07688]\n",
      "[2.668, 69.22583]\n",
      "[2.022, 68.53214]\n",
      "[1.598, 73.273094]\n",
      "[2.581, 62.53622]\n",
      "[2.042, 77.93202]\n",
      "[1.92, 81.69512]\n",
      "[2.499, 68.001]\n",
      "[1.94, 80.402435]\n",
      "[1.445, 70.27561]\n",
      "[2.399, 64.86351]\n",
      "[2.088, 73.696655]\n",
      "[3.297, 67.2599]\n",
      "[1.5, 82.24634]\n",
      "[1.98, 81.45122]\n",
      "[1.39, 81.62683]\n",
      "[1.57, 79.42195]\n",
      "[1.4, 75.1122]\n",
      "[1.4, 73.936584]\n",
      "[1.54, 68.80488]\n",
      "[1.38, 73.458534]\n",
      "[1.32, 79.02683]\n",
      "[1.38, 76.24634]\n",
      "[1.95, 80.99756]\n",
      "[1.79, 80.70244]\n",
      "[1.656, 74.3111]\n",
      "[1.475, 68.90371]\n",
      "[1.422, 74.61885]\n",
      "[1.63, 80.08781]\n",
      "[1.55, 73.268295]\n",
      "[1.17, 73.482925]\n",
      "[2.898, 69.36846]\n",
      "[2.2893, 69.88439]\n",
      "[2.59, 68.295364]\n",
      "[1.4, 81.73659]\n",
      "[2.07, 80.291954]\n",
      "[2.2, 81.45122]\n",
      "[1.25, 74.20731]\n",
      "[1.44, 80.3878]\n",
      "[1.39, 79.98781]\n",
      "[1.555, 73.32734]\n",
      "[2.0, 81.36829]\n",
      "[1.87, 79.870735]\n",
      "[1.63, 75.42927]\n",
      "[1.87, 79.1]\n",
      "[1.49, 77.42439]\n",
      "[1.476, 79.380394]\n",
      "[1.46, 76.47561]\n",
      "[1.46, 79.832]\n",
      "[1.49, 73.51219]\n",
      "[1.148, 75.40044]\n",
      "[1.84, 79.936584]\n",
      "[1.44, 70.40488]\n",
      "[2.3, 70.506516]\n",
      "[1.44, 80.38293]\n",
      "[1.736, 73.78356]\n",
      "[1.536, 76.90095]\n",
      "[1.8, 79.18944]\n",
      "[2.49, 74.12732]\n",
      "[1.9856, 76.23683]\n",
      "[1.639, 69.755]\n",
      "[1.896, 75.22212]\n",
      "[2.336, 70.33532]\n",
      "[2.064, 72.11253]\n",
      "[1.98, 74.4399]\n",
      "[1.797, 78.91329]\n",
      "[2.501, 73.76498]\n",
      "[2.954, 72.277]\n",
      "[2.484, 75.97424]\n",
      "[2.622, 73.72922]\n",
      "[2.32, 76.68378]\n",
      "[2.329, 72.84712]\n",
      "[3.139, 72.82593]\n",
      "[3.34, 61.763]\n",
      "[2.262, 69.54915]\n",
      "[3.983, 70.82542]\n",
      "[2.235, 75.66044]\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "X, Y = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(100):\n",
    "        print(sess.run([X, Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset.shuffle**\n",
    "\n",
    "dataset을 섞어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.297, 67.2599]\n",
      "[3.86, 72.30639]\n",
      "[1.764, 79.288536]\n",
      "[1.44, 80.38293]\n",
      "[4.651, 66.46707]\n",
      "[1.148, 75.40044]\n",
      "[5.063, 47.700657]\n",
      "[1.87, 79.870735]\n",
      "[1.8, 72.40875]\n",
      "[4.718, 56.497074]\n",
      "[2.09, 75.07688]\n",
      "[1.87, 79.1]\n",
      "[4.487, 51.062756]\n",
      "[3.364, 48.342804]\n",
      "[2.042, 77.93202]\n",
      "[1.44, 80.3878]\n",
      "[2.501, 73.76498]\n",
      "[1.38, 80.948784]\n",
      "[2.484, 75.97424]\n",
      "[2.479, 75.462296]\n",
      "[1.9856, 76.23683]\n",
      "[1.39, 79.98781]\n",
      "[1.22, 80.76195]\n",
      "[6.288, 48.282196]\n",
      "[1.47, 72.967316]\n",
      "[3.869, 70.81949]\n",
      "[1.95, 80.99756]\n",
      "[2.405, 73.77405]\n",
      "[1.94, 80.402435]\n",
      "[4.072, 56.588707]\n",
      "[2.313, 74.72261]\n",
      "[1.79, 80.70244]\n",
      "[1.476, 79.380394]\n",
      "[1.677, 80.797806]\n",
      "[2.002, 64.662094]\n",
      "[1.63, 80.08781]\n",
      "[3.451, 68.76483]\n",
      "[5.238, 56.147587]\n",
      "[1.598, 73.273094]\n",
      "[2.022, 68.53214]\n",
      "[4.193, 58.7151]\n",
      "[3.139, 72.82593]\n",
      "[2.59, 68.295364]\n",
      "[1.475, 68.90371]\n",
      "[1.38, 76.24634]\n",
      "[4.819, 58.954075]\n",
      "[1.896, 75.22212]\n",
      "[2.1, 73.42968]\n",
      "[3.8, 73.28966]\n",
      "[5.287, 55.585587]\n",
      "[4.912, 49.696926]\n",
      "[5.578, 61.999855]\n",
      "[5.85, 54.924194]\n",
      "[1.67, 72.751854]\n",
      "[2.07, 80.291954]\n",
      "[2.1, 78.24146]\n",
      "[1.736, 73.78356]\n",
      "[2.47, 75.99427]\n",
      "[6.258, 48.455486]\n",
      "[2.733, 72.975266]\n",
      "[3.983, 70.82542]\n",
      "[2.79, 75.83995]\n",
      "[2.504, 68.19498]\n",
      "[2.625, 65.13134]\n",
      "[2.58, 73.20003]\n",
      "[1.38, 73.458534]\n",
      "[3.03, 81.504875]\n",
      "[1.467, 78.96415]\n",
      "[3.911, 72.15066]\n",
      "[3.34, 61.763]\n",
      "[4.453, 60.994194]\n",
      "[2.2893, 69.88439]\n",
      "[2.2, 81.45122]\n",
      "[2.934, 75.70256]\n",
      "[1.092, 80.775314]\n",
      "[4.631, 47.61846]\n",
      "[1.46, 76.47561]\n",
      "[1.108, 82.87805]\n",
      "[1.55, 73.268295]\n",
      "[1.536, 76.90095]\n",
      "[4.982, 47.402195]\n",
      "[5.185, 50.840805]\n",
      "[2.245, 68.6348]\n",
      "[2.54, 75.02383]\n",
      "[3.951, 62.44061]\n",
      "[4.4, 61.108242]\n",
      "[2.279, 71.86463]\n",
      "[2.211, 75.63215]\n",
      "[4.338, 49.87722]\n",
      "[2.622, 73.72922]\n",
      "[2.064, 72.11253]\n",
      "[1.39, 82.932686]\n",
      "[2.264, 72.85254]\n",
      "[2.271, 78.09759]\n",
      "[2.75, 53.109512]\n",
      "[2.235, 75.66044]\n",
      "[2.458, 52.08149]\n",
      "[5.544, 57.38749]\n",
      "[4.533, 58.21695]\n",
      "[2.399, 66.90885]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(1000)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(100):\n",
    "        print(sess.run([X, Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset.repeat**\n",
    "\n",
    "dataset을 반복해서 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n",
      "[5.578, 61.999855]\n",
      "[1.579, 73.92766]\n",
      "[4.229, 67.465195]\n",
      "[1.15, 81.641464]\n",
      "[3.86, 72.30639]\n",
      "[3.142, 68.484314]\n",
      "[3.951, 62.44061]\n",
      "[2.16, 80.70244]\n",
      "[2.141, 76.30168]\n",
      "[2.002, 64.662094]\n",
      "[2.504, 68.19498]\n",
      "[3.451, 68.76483]\n",
      "[2.635, 74.02456]\n",
      "[1.092, 80.775314]\n",
      "[2.747, 67.064]\n",
      "[1.22, 80.76195]\n",
      "[1.39, 82.932686]\n",
      "[2.117, 68.889656]\n",
      "[1.108, 82.87805]\n",
      "[2.47, 75.99427]\n",
      "[2.09, 75.07688]\n",
      "[2.668, 69.22583]\n",
      "[2.022, 68.53214]\n",
      "[1.598, 73.273094]\n",
      "[2.581, 62.53622]\n",
      "[2.042, 77.93202]\n",
      "[1.92, 81.69512]\n",
      "[2.499, 68.001]\n",
      "[1.94, 80.402435]\n",
      "[1.445, 70.27561]\n",
      "[2.399, 64.86351]\n",
      "[2.088, 73.696655]\n",
      "[3.297, 67.2599]\n",
      "[1.5, 82.24634]\n",
      "[1.98, 81.45122]\n",
      "[1.39, 81.62683]\n",
      "[1.57, 79.42195]\n",
      "[1.4, 75.1122]\n",
      "[1.4, 73.936584]\n",
      "[1.54, 68.80488]\n",
      "[1.38, 73.458534]\n",
      "[1.32, 79.02683]\n",
      "[1.38, 76.24634]\n",
      "[1.95, 80.99756]\n",
      "[1.79, 80.70244]\n",
      "[1.656, 74.3111]\n",
      "[1.475, 68.90371]\n",
      "[1.422, 74.61885]\n",
      "[1.63, 80.08781]\n",
      "[1.55, 73.268295]\n",
      "[1.17, 73.482925]\n",
      "[2.898, 69.36846]\n",
      "[2.2893, 69.88439]\n",
      "[2.59, 68.295364]\n",
      "[1.4, 81.73659]\n",
      "[2.07, 80.291954]\n",
      "[2.2, 81.45122]\n",
      "[1.25, 74.20731]\n",
      "[1.44, 80.3878]\n",
      "[1.39, 79.98781]\n",
      "[1.555, 73.32734]\n",
      "[2.0, 81.36829]\n",
      "[1.87, 79.870735]\n",
      "[1.63, 75.42927]\n",
      "[1.87, 79.1]\n",
      "[1.49, 77.42439]\n",
      "[1.476, 79.380394]\n",
      "[1.46, 76.47561]\n",
      "[1.46, 79.832]\n",
      "[1.49, 73.51219]\n",
      "[1.148, 75.40044]\n",
      "[1.84, 79.936584]\n",
      "[1.44, 70.40488]\n",
      "[2.3, 70.506516]\n",
      "[1.44, 80.38293]\n",
      "[1.736, 73.78356]\n",
      "[1.536, 76.90095]\n",
      "[1.8, 79.18944]\n",
      "[2.49, 74.12732]\n",
      "[1.9856, 76.23683]\n",
      "[1.639, 69.755]\n",
      "[1.896, 75.22212]\n",
      "[2.336, 70.33532]\n",
      "[2.064, 72.11253]\n",
      "[1.98, 74.4399]\n",
      "[1.797, 78.91329]\n",
      "[2.501, 73.76498]\n",
      "[2.954, 72.277]\n",
      "[2.484, 75.97424]\n",
      "[2.622, 73.72922]\n",
      "[2.32, 76.68378]\n",
      "[2.329, 72.84712]\n",
      "[3.139, 72.82593]\n",
      "[3.34, 61.763]\n",
      "[2.262, 69.54915]\n",
      "[3.983, 70.82542]\n",
      "[2.235, 75.66044]\n",
      "[2.25, 71.73237]\n",
      "[2.479, 75.462296]\n",
      "[2.58, 73.20003]\n",
      "[1.467, 78.96415]\n",
      "[1.848, 79.19261]\n",
      "[2.1, 73.42968]\n",
      "[1.862, 78.885735]\n",
      "[1.83, 73.09953]\n",
      "[3.348, 66.26856]\n",
      "[2.79, 75.83995]\n",
      "[1.551, 76.57283]\n",
      "[1.699, 74.975174]\n",
      "[2.211, 75.63215]\n",
      "[5.2, 65.030464]\n",
      "[4.453, 72.643684]\n",
      "[1.749, 76.57361]\n",
      "[2.04, 74.6]\n",
      "[2.934, 75.70256]\n",
      "[2.811, 73.85042]\n",
      "[2.271, 78.09759]\n",
      "[2.309, 73.12461]\n",
      "[2.279, 71.86463]\n",
      "[1.38, 80.948784]\n",
      "[2.564, 74.75312]\n",
      "[1.8, 72.40875]\n",
      "[2.295, 74.60473]\n",
      "[3.8, 73.28966]\n",
      "[3.03, 81.504875]\n",
      "[4.702, 68.486046]\n",
      "[1.67, 72.751854]\n",
      "[2.733, 72.975266]\n",
      "[3.75, 57.52739]\n",
      "[2.54, 75.02383]\n",
      "[2.264, 72.85254]\n",
      "[2.1, 78.24146]\n",
      "[1.677, 80.797806]\n",
      "[1.764, 79.288536]\n",
      "[2.313, 74.72261]\n",
      "[3.423, 65.19885]\n",
      "[2.727, 68.39483]\n",
      "[1.752, 76.551414]\n",
      "[2.625, 65.13134]\n",
      "[2.399, 66.90885]\n",
      "[2.245, 68.6348]\n",
      "[6.288, 48.282196]\n",
      "[3.29, 49.860878]\n",
      "[6.258, 48.455486]\n",
      "[6.149, 53.614635]\n",
      "[4.072, 56.588707]\n",
      "[4.896, 58.160023]\n",
      "[5.544, 57.38749]\n",
      "[3.364, 48.342804]\n",
      "[4.4, 61.108242]\n",
      "[2.458, 52.08149]\n",
      "[6.339, 50.89554]\n",
      "[4.982, 47.402195]\n",
      "[2.5, 73.03415]\n",
      "[4.819, 58.954075]\n",
      "[5.371, 55.05712]\n",
      "[5.525, 51.410023]\n",
      "[7.063, 54.265633]\n",
      "[3.217, 62.0701]\n",
      "[4.912, 49.696926]\n",
      "[1.47, 72.967316]\n",
      "[4.533, 58.21695]\n",
      "[6.294, 50.95483]\n",
      "[5.99, 53.462635]\n",
      "[4.651, 66.46707]\n",
      "[5.238, 56.147587]\n",
      "[3.199, 47.365074]\n",
      "[4.718, 56.497074]\n",
      "[5.063, 47.700657]\n",
      "[5.246, 53.638584]\n",
      "[4.17, 63.83727]\n",
      "[3.25, 62.286682]\n",
      "[4.193, 58.7151]\n",
      "[4.453, 60.994194]\n",
      "[5.185, 50.840805]\n",
      "[5.775, 48.069584]\n",
      "[4.544, 56.960194]\n",
      "[4.919, 60.626266]\n",
      "[5.981, 49.194828]\n",
      "[4.631, 47.61846]\n",
      "[2.405, 73.77405]\n",
      "[4.487, 51.062756]\n",
      "[4.338, 49.87722]\n",
      "[5.85, 54.924194]\n",
      "[2.75, 53.109512]\n",
      "[5.287, 55.585587]\n",
      "[5.443, 50.65366]\n",
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n",
      "[5.578, 61.999855]\n",
      "[1.579, 73.92766]\n",
      "[4.229, 67.465195]\n",
      "[1.15, 81.641464]\n",
      "[3.86, 72.30639]\n",
      "[3.142, 68.484314]\n",
      "[3.951, 62.44061]\n",
      "[2.16, 80.70244]\n",
      "[2.141, 76.30168]\n",
      "[2.002, 64.662094]\n",
      "[2.504, 68.19498]\n",
      "[3.451, 68.76483]\n",
      "[2.635, 74.02456]\n",
      "[1.092, 80.775314]\n",
      "[2.747, 67.064]\n",
      "[1.22, 80.76195]\n",
      "[1.39, 82.932686]\n",
      "[2.117, 68.889656]\n",
      "[1.108, 82.87805]\n",
      "[2.47, 75.99427]\n",
      "[2.09, 75.07688]\n",
      "[2.668, 69.22583]\n",
      "[2.022, 68.53214]\n",
      "[1.598, 73.273094]\n",
      "[2.581, 62.53622]\n",
      "[2.042, 77.93202]\n",
      "[1.92, 81.69512]\n",
      "[2.499, 68.001]\n",
      "[1.94, 80.402435]\n",
      "[1.445, 70.27561]\n",
      "[2.399, 64.86351]\n",
      "[2.088, 73.696655]\n",
      "[3.297, 67.2599]\n",
      "[1.5, 82.24634]\n",
      "[1.98, 81.45122]\n",
      "[1.39, 81.62683]\n",
      "[1.57, 79.42195]\n",
      "[1.4, 75.1122]\n",
      "[1.4, 73.936584]\n",
      "[1.54, 68.80488]\n",
      "[1.38, 73.458534]\n",
      "[1.32, 79.02683]\n",
      "[1.38, 76.24634]\n",
      "[1.95, 80.99756]\n",
      "[1.79, 80.70244]\n",
      "[1.656, 74.3111]\n",
      "[1.475, 68.90371]\n",
      "[1.422, 74.61885]\n",
      "[1.63, 80.08781]\n",
      "[1.55, 73.268295]\n",
      "[1.17, 73.482925]\n",
      "[2.898, 69.36846]\n",
      "[2.2893, 69.88439]\n",
      "[2.59, 68.295364]\n",
      "[1.4, 81.73659]\n",
      "[2.07, 80.291954]\n",
      "[2.2, 81.45122]\n",
      "[1.25, 74.20731]\n",
      "[1.44, 80.3878]\n",
      "[1.39, 79.98781]\n",
      "[1.555, 73.32734]\n",
      "[2.0, 81.36829]\n",
      "[1.87, 79.870735]\n",
      "[1.63, 75.42927]\n",
      "[1.87, 79.1]\n",
      "[1.49, 77.42439]\n",
      "[1.476, 79.380394]\n",
      "[1.46, 76.47561]\n",
      "[1.46, 79.832]\n",
      "[1.49, 73.51219]\n",
      "[1.148, 75.40044]\n",
      "[1.84, 79.936584]\n",
      "[1.44, 70.40488]\n",
      "[2.3, 70.506516]\n",
      "[1.44, 80.38293]\n",
      "[1.736, 73.78356]\n",
      "[1.536, 76.90095]\n",
      "[1.8, 79.18944]\n",
      "[2.49, 74.12732]\n",
      "[1.9856, 76.23683]\n",
      "[1.639, 69.755]\n",
      "[1.896, 75.22212]\n",
      "[2.336, 70.33532]\n",
      "[2.064, 72.11253]\n",
      "[1.98, 74.4399]\n",
      "[1.797, 78.91329]\n",
      "[2.501, 73.76498]\n",
      "[2.954, 72.277]\n",
      "[2.484, 75.97424]\n",
      "[2.622, 73.72922]\n",
      "[2.32, 76.68378]\n",
      "[2.329, 72.84712]\n",
      "[3.139, 72.82593]\n",
      "[3.34, 61.763]\n",
      "[2.262, 69.54915]\n",
      "[3.983, 70.82542]\n",
      "[2.235, 75.66044]\n",
      "[2.25, 71.73237]\n",
      "[2.479, 75.462296]\n",
      "[2.58, 73.20003]\n",
      "[1.467, 78.96415]\n",
      "[1.848, 79.19261]\n",
      "[2.1, 73.42968]\n",
      "[1.862, 78.885735]\n",
      "[1.83, 73.09953]\n",
      "[3.348, 66.26856]\n",
      "[2.79, 75.83995]\n",
      "[1.551, 76.57283]\n",
      "[1.699, 74.975174]\n",
      "[2.211, 75.63215]\n",
      "[5.2, 65.030464]\n",
      "[4.453, 72.643684]\n",
      "[1.749, 76.57361]\n",
      "[2.04, 74.6]\n",
      "[2.934, 75.70256]\n",
      "[2.811, 73.85042]\n",
      "[2.271, 78.09759]\n",
      "[2.309, 73.12461]\n",
      "[2.279, 71.86463]\n",
      "[1.38, 80.948784]\n",
      "[2.564, 74.75312]\n",
      "[1.8, 72.40875]\n",
      "[2.295, 74.60473]\n",
      "[3.8, 73.28966]\n",
      "[3.03, 81.504875]\n",
      "[4.702, 68.486046]\n",
      "[1.67, 72.751854]\n",
      "[2.733, 72.975266]\n",
      "[3.75, 57.52739]\n",
      "[2.54, 75.02383]\n",
      "[2.264, 72.85254]\n",
      "[2.1, 78.24146]\n",
      "[1.677, 80.797806]\n",
      "[1.764, 79.288536]\n",
      "[2.313, 74.72261]\n",
      "[3.423, 65.19885]\n",
      "[2.727, 68.39483]\n",
      "[1.752, 76.551414]\n",
      "[2.625, 65.13134]\n",
      "[2.399, 66.90885]\n",
      "[2.245, 68.6348]\n",
      "[6.288, 48.282196]\n",
      "[3.29, 49.860878]\n",
      "[6.258, 48.455486]\n",
      "[6.149, 53.614635]\n",
      "[4.072, 56.588707]\n",
      "[4.896, 58.160023]\n",
      "[5.544, 57.38749]\n",
      "[3.364, 48.342804]\n",
      "[4.4, 61.108242]\n",
      "[2.458, 52.08149]\n",
      "[6.339, 50.89554]\n",
      "[4.982, 47.402195]\n",
      "[2.5, 73.03415]\n",
      "[4.819, 58.954075]\n",
      "[5.371, 55.05712]\n",
      "[5.525, 51.410023]\n",
      "[7.063, 54.265633]\n",
      "[3.217, 62.0701]\n",
      "[4.912, 49.696926]\n",
      "[1.47, 72.967316]\n",
      "[4.533, 58.21695]\n",
      "[6.294, 50.95483]\n",
      "[5.99, 53.462635]\n",
      "[4.651, 66.46707]\n",
      "[5.238, 56.147587]\n",
      "[3.199, 47.365074]\n",
      "[4.718, 56.497074]\n",
      "[5.063, 47.700657]\n",
      "[5.246, 53.638584]\n",
      "[4.17, 63.83727]\n",
      "[3.25, 62.286682]\n",
      "[4.193, 58.7151]\n",
      "[4.453, 60.994194]\n",
      "[5.185, 50.840805]\n",
      "[5.775, 48.069584]\n",
      "[4.544, 56.960194]\n",
      "[4.919, 60.626266]\n",
      "[5.981, 49.194828]\n",
      "[4.631, 47.61846]\n",
      "[2.405, 73.77405]\n",
      "[4.487, 51.062756]\n",
      "[4.338, 49.87722]\n",
      "[5.85, 54.924194]\n",
      "[2.75, 53.109512]\n",
      "[5.287, 55.585587]\n",
      "[5.443, 50.65366]\n",
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n",
      "[5.578, 61.999855]\n",
      "[1.579, 73.92766]\n",
      "[4.229, 67.465195]\n",
      "[1.15, 81.641464]\n",
      "[3.86, 72.30639]\n",
      "[3.142, 68.484314]\n",
      "[3.951, 62.44061]\n",
      "[2.16, 80.70244]\n",
      "[2.141, 76.30168]\n",
      "[2.002, 64.662094]\n",
      "[2.504, 68.19498]\n",
      "[3.451, 68.76483]\n",
      "[2.635, 74.02456]\n",
      "[1.092, 80.775314]\n",
      "[2.747, 67.064]\n",
      "[1.22, 80.76195]\n",
      "[1.39, 82.932686]\n",
      "[2.117, 68.889656]\n",
      "[1.108, 82.87805]\n",
      "[2.47, 75.99427]\n",
      "[2.09, 75.07688]\n",
      "[2.668, 69.22583]\n",
      "[2.022, 68.53214]\n",
      "[1.598, 73.273094]\n",
      "[2.581, 62.53622]\n",
      "[2.042, 77.93202]\n",
      "[1.92, 81.69512]\n",
      "[2.499, 68.001]\n",
      "[1.94, 80.402435]\n",
      "[1.445, 70.27561]\n",
      "[2.399, 64.86351]\n",
      "[2.088, 73.696655]\n",
      "[3.297, 67.2599]\n",
      "[1.5, 82.24634]\n",
      "[1.98, 81.45122]\n",
      "[1.39, 81.62683]\n",
      "[1.57, 79.42195]\n",
      "[1.4, 75.1122]\n",
      "[1.4, 73.936584]\n",
      "[1.54, 68.80488]\n",
      "[1.38, 73.458534]\n",
      "[1.32, 79.02683]\n",
      "[1.38, 76.24634]\n",
      "[1.95, 80.99756]\n",
      "[1.79, 80.70244]\n",
      "[1.656, 74.3111]\n",
      "[1.475, 68.90371]\n",
      "[1.422, 74.61885]\n",
      "[1.63, 80.08781]\n",
      "[1.55, 73.268295]\n",
      "[1.17, 73.482925]\n",
      "[2.898, 69.36846]\n",
      "[2.2893, 69.88439]\n",
      "[2.59, 68.295364]\n",
      "[1.4, 81.73659]\n",
      "[2.07, 80.291954]\n",
      "[2.2, 81.45122]\n",
      "[1.25, 74.20731]\n",
      "[1.44, 80.3878]\n",
      "[1.39, 79.98781]\n",
      "[1.555, 73.32734]\n",
      "[2.0, 81.36829]\n",
      "[1.87, 79.870735]\n",
      "[1.63, 75.42927]\n",
      "[1.87, 79.1]\n",
      "[1.49, 77.42439]\n",
      "[1.476, 79.380394]\n",
      "[1.46, 76.47561]\n",
      "[1.46, 79.832]\n",
      "[1.49, 73.51219]\n",
      "[1.148, 75.40044]\n",
      "[1.84, 79.936584]\n",
      "[1.44, 70.40488]\n",
      "[2.3, 70.506516]\n",
      "[1.44, 80.38293]\n",
      "[1.736, 73.78356]\n",
      "[1.536, 76.90095]\n",
      "[1.8, 79.18944]\n",
      "[2.49, 74.12732]\n",
      "[1.9856, 76.23683]\n",
      "[1.639, 69.755]\n",
      "[1.896, 75.22212]\n",
      "[2.336, 70.33532]\n",
      "[2.064, 72.11253]\n",
      "[1.98, 74.4399]\n",
      "[1.797, 78.91329]\n",
      "[2.501, 73.76498]\n",
      "[2.954, 72.277]\n",
      "[2.484, 75.97424]\n",
      "[2.622, 73.72922]\n",
      "[2.32, 76.68378]\n",
      "[2.329, 72.84712]\n",
      "[3.139, 72.82593]\n",
      "[3.34, 61.763]\n",
      "[2.262, 69.54915]\n",
      "[3.983, 70.82542]\n",
      "[2.235, 75.66044]\n",
      "[2.25, 71.73237]\n",
      "[2.479, 75.462296]\n",
      "[2.58, 73.20003]\n",
      "[1.467, 78.96415]\n",
      "[1.848, 79.19261]\n",
      "[2.1, 73.42968]\n",
      "[1.862, 78.885735]\n",
      "[1.83, 73.09953]\n",
      "[3.348, 66.26856]\n",
      "[2.79, 75.83995]\n",
      "[1.551, 76.57283]\n",
      "[1.699, 74.975174]\n",
      "[2.211, 75.63215]\n",
      "[5.2, 65.030464]\n",
      "[4.453, 72.643684]\n",
      "[1.749, 76.57361]\n",
      "[2.04, 74.6]\n",
      "[2.934, 75.70256]\n",
      "[2.811, 73.85042]\n",
      "[2.271, 78.09759]\n",
      "[2.309, 73.12461]\n",
      "[2.279, 71.86463]\n",
      "[1.38, 80.948784]\n",
      "[2.564, 74.75312]\n",
      "[1.8, 72.40875]\n",
      "[2.295, 74.60473]\n",
      "[3.8, 73.28966]\n",
      "[3.03, 81.504875]\n",
      "[4.702, 68.486046]\n",
      "[1.67, 72.751854]\n",
      "[2.733, 72.975266]\n",
      "[3.75, 57.52739]\n",
      "[2.54, 75.02383]\n",
      "[2.264, 72.85254]\n",
      "[2.1, 78.24146]\n",
      "[1.677, 80.797806]\n",
      "[1.764, 79.288536]\n",
      "[2.313, 74.72261]\n",
      "[3.423, 65.19885]\n",
      "[2.727, 68.39483]\n",
      "[1.752, 76.551414]\n",
      "[2.625, 65.13134]\n",
      "[2.399, 66.90885]\n",
      "[2.245, 68.6348]\n",
      "[6.288, 48.282196]\n",
      "[3.29, 49.860878]\n",
      "[6.258, 48.455486]\n",
      "[6.149, 53.614635]\n",
      "[4.072, 56.588707]\n",
      "[4.896, 58.160023]\n",
      "[5.544, 57.38749]\n",
      "[3.364, 48.342804]\n",
      "[4.4, 61.108242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.458, 52.08149]\n",
      "[6.339, 50.89554]\n",
      "[4.982, 47.402195]\n",
      "[2.5, 73.03415]\n",
      "[4.819, 58.954075]\n",
      "[5.371, 55.05712]\n",
      "[5.525, 51.410023]\n",
      "[7.063, 54.265633]\n",
      "[3.217, 62.0701]\n",
      "[4.912, 49.696926]\n",
      "[1.47, 72.967316]\n",
      "[4.533, 58.21695]\n",
      "[6.294, 50.95483]\n",
      "[5.99, 53.462635]\n",
      "[4.651, 66.46707]\n",
      "[5.238, 56.147587]\n",
      "[3.199, 47.365074]\n",
      "[4.718, 56.497074]\n",
      "[5.063, 47.700657]\n",
      "[5.246, 53.638584]\n",
      "[4.17, 63.83727]\n",
      "[3.25, 62.286682]\n",
      "[4.193, 58.7151]\n",
      "[4.453, 60.994194]\n",
      "[5.185, 50.840805]\n",
      "[5.775, 48.069584]\n",
      "[4.544, 56.960194]\n",
      "[4.919, 60.626266]\n",
      "[5.981, 49.194828]\n",
      "[4.631, 47.61846]\n",
      "[2.405, 73.77405]\n",
      "[4.487, 51.062756]\n",
      "[4.338, 49.87722]\n",
      "[5.85, 54.924194]\n",
      "[2.75, 53.109512]\n",
      "[5.287, 55.585587]\n",
      "[5.443, 50.65366]\n",
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n",
      "[5.578, 61.999855]\n",
      "[1.579, 73.92766]\n",
      "[4.229, 67.465195]\n",
      "[1.15, 81.641464]\n",
      "[3.86, 72.30639]\n",
      "[3.142, 68.484314]\n",
      "[3.951, 62.44061]\n",
      "[2.16, 80.70244]\n",
      "[2.141, 76.30168]\n",
      "[2.002, 64.662094]\n",
      "[2.504, 68.19498]\n",
      "[3.451, 68.76483]\n",
      "[2.635, 74.02456]\n",
      "[1.092, 80.775314]\n",
      "[2.747, 67.064]\n",
      "[1.22, 80.76195]\n",
      "[1.39, 82.932686]\n",
      "[2.117, 68.889656]\n",
      "[1.108, 82.87805]\n",
      "[2.47, 75.99427]\n",
      "[2.09, 75.07688]\n",
      "[2.668, 69.22583]\n",
      "[2.022, 68.53214]\n",
      "[1.598, 73.273094]\n",
      "[2.581, 62.53622]\n",
      "[2.042, 77.93202]\n",
      "[1.92, 81.69512]\n",
      "[2.499, 68.001]\n",
      "[1.94, 80.402435]\n",
      "[1.445, 70.27561]\n",
      "[2.399, 64.86351]\n",
      "[2.088, 73.696655]\n",
      "[3.297, 67.2599]\n",
      "[1.5, 82.24634]\n",
      "[1.98, 81.45122]\n",
      "[1.39, 81.62683]\n",
      "[1.57, 79.42195]\n",
      "[1.4, 75.1122]\n",
      "[1.4, 73.936584]\n",
      "[1.54, 68.80488]\n",
      "[1.38, 73.458534]\n",
      "[1.32, 79.02683]\n",
      "[1.38, 76.24634]\n",
      "[1.95, 80.99756]\n",
      "[1.79, 80.70244]\n",
      "[1.656, 74.3111]\n",
      "[1.475, 68.90371]\n",
      "[1.422, 74.61885]\n",
      "[1.63, 80.08781]\n",
      "[1.55, 73.268295]\n",
      "[1.17, 73.482925]\n",
      "[2.898, 69.36846]\n",
      "[2.2893, 69.88439]\n",
      "[2.59, 68.295364]\n",
      "[1.4, 81.73659]\n",
      "[2.07, 80.291954]\n",
      "[2.2, 81.45122]\n",
      "[1.25, 74.20731]\n",
      "[1.44, 80.3878]\n",
      "[1.39, 79.98781]\n",
      "[1.555, 73.32734]\n",
      "[2.0, 81.36829]\n",
      "[1.87, 79.870735]\n",
      "[1.63, 75.42927]\n",
      "[1.87, 79.1]\n",
      "[1.49, 77.42439]\n",
      "[1.476, 79.380394]\n",
      "[1.46, 76.47561]\n",
      "[1.46, 79.832]\n",
      "[1.49, 73.51219]\n",
      "[1.148, 75.40044]\n",
      "[1.84, 79.936584]\n",
      "[1.44, 70.40488]\n",
      "[2.3, 70.506516]\n",
      "[1.44, 80.38293]\n",
      "[1.736, 73.78356]\n",
      "[1.536, 76.90095]\n",
      "[1.8, 79.18944]\n",
      "[2.49, 74.12732]\n",
      "[1.9856, 76.23683]\n",
      "[1.639, 69.755]\n",
      "[1.896, 75.22212]\n",
      "[2.336, 70.33532]\n",
      "[2.064, 72.11253]\n",
      "[1.98, 74.4399]\n",
      "[1.797, 78.91329]\n",
      "[2.501, 73.76498]\n",
      "[2.954, 72.277]\n",
      "[2.484, 75.97424]\n",
      "[2.622, 73.72922]\n",
      "[2.32, 76.68378]\n",
      "[2.329, 72.84712]\n",
      "[3.139, 72.82593]\n",
      "[3.34, 61.763]\n",
      "[2.262, 69.54915]\n",
      "[3.983, 70.82542]\n",
      "[2.235, 75.66044]\n",
      "[2.25, 71.73237]\n",
      "[2.479, 75.462296]\n",
      "[2.58, 73.20003]\n",
      "[1.467, 78.96415]\n",
      "[1.848, 79.19261]\n",
      "[2.1, 73.42968]\n",
      "[1.862, 78.885735]\n",
      "[1.83, 73.09953]\n",
      "[3.348, 66.26856]\n",
      "[2.79, 75.83995]\n",
      "[1.551, 76.57283]\n",
      "[1.699, 74.975174]\n",
      "[2.211, 75.63215]\n",
      "[5.2, 65.030464]\n",
      "[4.453, 72.643684]\n",
      "[1.749, 76.57361]\n",
      "[2.04, 74.6]\n",
      "[2.934, 75.70256]\n",
      "[2.811, 73.85042]\n",
      "[2.271, 78.09759]\n",
      "[2.309, 73.12461]\n",
      "[2.279, 71.86463]\n",
      "[1.38, 80.948784]\n",
      "[2.564, 74.75312]\n",
      "[1.8, 72.40875]\n",
      "[2.295, 74.60473]\n",
      "[3.8, 73.28966]\n",
      "[3.03, 81.504875]\n",
      "[4.702, 68.486046]\n",
      "[1.67, 72.751854]\n",
      "[2.733, 72.975266]\n",
      "[3.75, 57.52739]\n",
      "[2.54, 75.02383]\n",
      "[2.264, 72.85254]\n",
      "[2.1, 78.24146]\n",
      "[1.677, 80.797806]\n",
      "[1.764, 79.288536]\n",
      "[2.313, 74.72261]\n",
      "[3.423, 65.19885]\n",
      "[2.727, 68.39483]\n",
      "[1.752, 76.551414]\n",
      "[2.625, 65.13134]\n",
      "[2.399, 66.90885]\n",
      "[2.245, 68.6348]\n",
      "[6.288, 48.282196]\n",
      "[3.29, 49.860878]\n",
      "[6.258, 48.455486]\n",
      "[6.149, 53.614635]\n",
      "[4.072, 56.588707]\n",
      "[4.896, 58.160023]\n",
      "[5.544, 57.38749]\n",
      "[3.364, 48.342804]\n",
      "[4.4, 61.108242]\n",
      "[2.458, 52.08149]\n",
      "[6.339, 50.89554]\n",
      "[4.982, 47.402195]\n",
      "[2.5, 73.03415]\n",
      "[4.819, 58.954075]\n",
      "[5.371, 55.05712]\n",
      "[5.525, 51.410023]\n",
      "[7.063, 54.265633]\n",
      "[3.217, 62.0701]\n",
      "[4.912, 49.696926]\n",
      "[1.47, 72.967316]\n",
      "[4.533, 58.21695]\n",
      "[6.294, 50.95483]\n",
      "[5.99, 53.462635]\n",
      "[4.651, 66.46707]\n",
      "[5.238, 56.147587]\n",
      "[3.199, 47.365074]\n",
      "[4.718, 56.497074]\n",
      "[5.063, 47.700657]\n",
      "[5.246, 53.638584]\n",
      "[4.17, 63.83727]\n",
      "[3.25, 62.286682]\n",
      "[4.193, 58.7151]\n",
      "[4.453, 60.994194]\n",
      "[5.185, 50.840805]\n",
      "[5.775, 48.069584]\n",
      "[4.544, 56.960194]\n",
      "[4.919, 60.626266]\n",
      "[5.981, 49.194828]\n",
      "[4.631, 47.61846]\n",
      "[2.405, 73.77405]\n",
      "[4.487, 51.062756]\n",
      "[4.338, 49.87722]\n",
      "[5.85, 54.924194]\n",
      "[2.75, 53.109512]\n",
      "[5.287, 55.585587]\n",
      "[5.443, 50.65366]\n",
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n",
      "[5.578, 61.999855]\n",
      "[1.579, 73.92766]\n",
      "[4.229, 67.465195]\n",
      "[1.15, 81.641464]\n",
      "[3.86, 72.30639]\n",
      "[3.142, 68.484314]\n",
      "[3.951, 62.44061]\n",
      "[2.16, 80.70244]\n",
      "[2.141, 76.30168]\n",
      "[2.002, 64.662094]\n",
      "[2.504, 68.19498]\n",
      "[3.451, 68.76483]\n",
      "[2.635, 74.02456]\n",
      "[1.092, 80.775314]\n",
      "[2.747, 67.064]\n",
      "[1.22, 80.76195]\n",
      "[1.39, 82.932686]\n",
      "[2.117, 68.889656]\n",
      "[1.108, 82.87805]\n",
      "[2.47, 75.99427]\n",
      "[2.09, 75.07688]\n",
      "[2.668, 69.22583]\n",
      "[2.022, 68.53214]\n",
      "[1.598, 73.273094]\n",
      "[2.581, 62.53622]\n",
      "[2.042, 77.93202]\n",
      "[1.92, 81.69512]\n",
      "[2.499, 68.001]\n",
      "[1.94, 80.402435]\n",
      "[1.445, 70.27561]\n",
      "[2.399, 64.86351]\n",
      "[2.088, 73.696655]\n",
      "[3.297, 67.2599]\n",
      "[1.5, 82.24634]\n",
      "[1.98, 81.45122]\n",
      "[1.39, 81.62683]\n",
      "[1.57, 79.42195]\n",
      "[1.4, 75.1122]\n",
      "[1.4, 73.936584]\n",
      "[1.54, 68.80488]\n",
      "[1.38, 73.458534]\n",
      "[1.32, 79.02683]\n",
      "[1.38, 76.24634]\n",
      "[1.95, 80.99756]\n",
      "[1.79, 80.70244]\n",
      "[1.656, 74.3111]\n",
      "[1.475, 68.90371]\n",
      "[1.422, 74.61885]\n",
      "[1.63, 80.08781]\n",
      "[1.55, 73.268295]\n",
      "[1.17, 73.482925]\n",
      "[2.898, 69.36846]\n",
      "[2.2893, 69.88439]\n",
      "[2.59, 68.295364]\n",
      "[1.4, 81.73659]\n",
      "[2.07, 80.291954]\n",
      "[2.2, 81.45122]\n",
      "[1.25, 74.20731]\n",
      "[1.44, 80.3878]\n",
      "[1.39, 79.98781]\n",
      "[1.555, 73.32734]\n",
      "[2.0, 81.36829]\n",
      "[1.87, 79.870735]\n",
      "[1.63, 75.42927]\n",
      "[1.87, 79.1]\n",
      "[1.49, 77.42439]\n",
      "[1.476, 79.380394]\n",
      "[1.46, 76.47561]\n",
      "[1.46, 79.832]\n",
      "[1.49, 73.51219]\n",
      "[1.148, 75.40044]\n",
      "[1.84, 79.936584]\n",
      "[1.44, 70.40488]\n",
      "[2.3, 70.506516]\n",
      "[1.44, 80.38293]\n",
      "[1.736, 73.78356]\n",
      "[1.536, 76.90095]\n",
      "[1.8, 79.18944]\n",
      "[2.49, 74.12732]\n",
      "[1.9856, 76.23683]\n",
      "[1.639, 69.755]\n",
      "[1.896, 75.22212]\n",
      "[2.336, 70.33532]\n",
      "[2.064, 72.11253]\n",
      "[1.98, 74.4399]\n",
      "[1.797, 78.91329]\n",
      "[2.501, 73.76498]\n",
      "[2.954, 72.277]\n",
      "[2.484, 75.97424]\n",
      "[2.622, 73.72922]\n",
      "[2.32, 76.68378]\n",
      "[2.329, 72.84712]\n",
      "[3.139, 72.82593]\n",
      "[3.34, 61.763]\n",
      "[2.262, 69.54915]\n",
      "[3.983, 70.82542]\n",
      "[2.235, 75.66044]\n",
      "[2.25, 71.73237]\n",
      "[2.479, 75.462296]\n",
      "[2.58, 73.20003]\n",
      "[1.467, 78.96415]\n",
      "[1.848, 79.19261]\n",
      "[2.1, 73.42968]\n",
      "[1.862, 78.885735]\n",
      "[1.83, 73.09953]\n",
      "[3.348, 66.26856]\n",
      "[2.79, 75.83995]\n",
      "[1.551, 76.57283]\n",
      "[1.699, 74.975174]\n",
      "[2.211, 75.63215]\n",
      "[5.2, 65.030464]\n",
      "[4.453, 72.643684]\n",
      "[1.749, 76.57361]\n",
      "[2.04, 74.6]\n",
      "[2.934, 75.70256]\n",
      "[2.811, 73.85042]\n",
      "[2.271, 78.09759]\n",
      "[2.309, 73.12461]\n",
      "[2.279, 71.86463]\n",
      "[1.38, 80.948784]\n",
      "[2.564, 74.75312]\n",
      "[1.8, 72.40875]\n",
      "[2.295, 74.60473]\n",
      "[3.8, 73.28966]\n",
      "[3.03, 81.504875]\n",
      "[4.702, 68.486046]\n",
      "[1.67, 72.751854]\n",
      "[2.733, 72.975266]\n",
      "[3.75, 57.52739]\n",
      "[2.54, 75.02383]\n",
      "[2.264, 72.85254]\n",
      "[2.1, 78.24146]\n",
      "[1.677, 80.797806]\n",
      "[1.764, 79.288536]\n",
      "[2.313, 74.72261]\n",
      "[3.423, 65.19885]\n",
      "[2.727, 68.39483]\n",
      "[1.752, 76.551414]\n",
      "[2.625, 65.13134]\n",
      "[2.399, 66.90885]\n",
      "[2.245, 68.6348]\n",
      "[6.288, 48.282196]\n",
      "[3.29, 49.860878]\n",
      "[6.258, 48.455486]\n",
      "[6.149, 53.614635]\n",
      "[4.072, 56.588707]\n",
      "[4.896, 58.160023]\n",
      "[5.544, 57.38749]\n",
      "[3.364, 48.342804]\n",
      "[4.4, 61.108242]\n",
      "[2.458, 52.08149]\n",
      "[6.339, 50.89554]\n",
      "[4.982, 47.402195]\n",
      "[2.5, 73.03415]\n",
      "[4.819, 58.954075]\n",
      "[5.371, 55.05712]\n",
      "[5.525, 51.410023]\n",
      "[7.063, 54.265633]\n",
      "[3.217, 62.0701]\n",
      "[4.912, 49.696926]\n",
      "[1.47, 72.967316]\n",
      "[4.533, 58.21695]\n",
      "[6.294, 50.95483]\n",
      "[5.99, 53.462635]\n",
      "[4.651, 66.46707]\n",
      "[5.238, 56.147587]\n",
      "[3.199, 47.365074]\n",
      "[4.718, 56.497074]\n",
      "[5.063, 47.700657]\n",
      "[5.246, 53.638584]\n",
      "[4.17, 63.83727]\n",
      "[3.25, 62.286682]\n",
      "[4.193, 58.7151]\n",
      "[4.453, 60.994194]\n",
      "[5.185, 50.840805]\n",
      "[5.775, 48.069584]\n",
      "[4.544, 56.960194]\n",
      "[4.919, 60.626266]\n",
      "[5.981, 49.194828]\n",
      "[4.631, 47.61846]\n",
      "[2.405, 73.77405]\n",
      "[4.487, 51.062756]\n",
      "[4.338, 49.87722]\n",
      "[5.85, 54.924194]\n",
      "[2.75, 53.109512]\n",
      "[5.287, 55.585587]\n",
      "[5.443, 50.65366]\n",
      "[1.822, 74.82825]\n",
      "[3.869, 70.81949]\n",
      "[3.911, 72.15066]\n",
      "[5.578, 61.999855]\n",
      "[1.579, 73.92766]\n",
      "[4.229, 67.465195]\n",
      "[1.15, 81.641464]\n",
      "[3.86, 72.30639]\n",
      "[3.142, 68.484314]\n",
      "[3.951, 62.44061]\n",
      "[2.16, 80.70244]\n",
      "[2.141, 76.30168]\n",
      "[2.002, 64.662094]\n",
      "[2.504, 68.19498]\n",
      "[3.451, 68.76483]\n",
      "[2.635, 74.02456]\n",
      "[1.092, 80.775314]\n",
      "[2.747, 67.064]\n",
      "[1.22, 80.76195]\n",
      "[1.39, 82.932686]\n",
      "[2.117, 68.889656]\n",
      "[1.108, 82.87805]\n",
      "[2.47, 75.99427]\n",
      "[2.09, 75.07688]\n",
      "[2.668, 69.22583]\n",
      "[2.022, 68.53214]\n",
      "[1.598, 73.273094]\n",
      "[2.581, 62.53622]\n",
      "[2.042, 77.93202]\n",
      "[1.92, 81.69512]\n",
      "[2.499, 68.001]\n",
      "[1.94, 80.402435]\n",
      "[1.445, 70.27561]\n",
      "[2.399, 64.86351]\n",
      "[2.088, 73.696655]\n",
      "[3.297, 67.2599]\n",
      "[1.5, 82.24634]\n",
      "[1.98, 81.45122]\n",
      "[1.39, 81.62683]\n",
      "[1.57, 79.42195]\n",
      "[1.4, 75.1122]\n",
      "[1.4, 73.936584]\n",
      "[1.54, 68.80488]\n",
      "[1.38, 73.458534]\n",
      "[1.32, 79.02683]\n",
      "[1.38, 76.24634]\n",
      "[1.95, 80.99756]\n",
      "[1.79, 80.70244]\n",
      "[1.656, 74.3111]\n",
      "[1.475, 68.90371]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "dataset = dataset.repeat(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(100 * 10):\n",
    "        print(sess.run([X, Y]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset.batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[1.822  3.869  3.911  5.578  1.579  4.229  1.15   3.86   3.142  3.951\n",
      " 2.16   2.141  2.002  2.504  3.451  2.635  1.092  2.747  1.22   1.39\n",
      " 2.117  1.108  2.47   2.09   2.668  2.022  1.598  2.581  2.042  1.92\n",
      " 2.499  1.94   1.445  2.399  2.088  3.297  1.5    1.98   1.39   1.57\n",
      " 1.4    1.4    1.54   1.38   1.32   1.38   1.95   1.79   1.656  1.475\n",
      " 1.422  1.63   1.55   1.17   2.898  2.2893 2.59   1.4    2.07   2.2\n",
      " 1.25   1.44   1.39   1.555  2.     1.87   1.63   1.87   1.49   1.476\n",
      " 1.46   1.46   1.49   1.148  1.84   1.44   2.3    1.44   1.736  1.536\n",
      " 1.8    2.49   1.9856 1.639  1.896  2.336  2.064  1.98   1.797  2.501\n",
      " 2.954  2.484  2.622  2.32   2.329  3.139  3.34   2.262  3.983  2.235\n",
      " 2.25   2.479  2.58   1.467  1.848  2.1    1.862  1.83   3.348  2.79\n",
      " 1.551  1.699  2.211  5.2    4.453  1.749  2.04   2.934  2.811  2.271\n",
      " 2.309  2.279  1.38   2.564  1.8    2.295  3.8    3.03  ]\n",
      "(128,)\n",
      "[74.82825  70.81949  72.15066  61.999855 73.92766  67.465195 81.641464\n",
      " 72.30639  68.484314 62.44061  80.70244  76.30168  64.662094 68.19498\n",
      " 68.76483  74.02456  80.775314 67.064    80.76195  82.932686 68.889656\n",
      " 82.87805  75.99427  75.07688  69.22583  68.53214  73.273094 62.53622\n",
      " 77.93202  81.69512  68.001    80.402435 70.27561  64.86351  73.696655\n",
      " 67.2599   82.24634  81.45122  81.62683  79.42195  75.1122   73.936584\n",
      " 68.80488  73.458534 79.02683  76.24634  80.99756  80.70244  74.3111\n",
      " 68.90371  74.61885  80.08781  73.268295 73.482925 69.36846  69.88439\n",
      " 68.295364 81.73659  80.291954 81.45122  74.20731  80.3878   79.98781\n",
      " 73.32734  81.36829  79.870735 75.42927  79.1      77.42439  79.380394\n",
      " 76.47561  79.832    73.51219  75.40044  79.936584 70.40488  70.506516\n",
      " 80.38293  73.78356  76.90095  79.18944  74.12732  76.23683  69.755\n",
      " 75.22212  70.33532  72.11253  74.4399   78.91329  73.76498  72.277\n",
      " 75.97424  73.72922  76.68378  72.84712  72.82593  61.763    69.54915\n",
      " 70.82542  75.66044  71.73237  75.462296 73.20003  78.96415  79.19261\n",
      " 73.42968  78.885735 73.09953  66.26856  75.83995  76.57283  74.975174\n",
      " 75.63215  65.030464 72.643684 76.57361  74.6      75.70256  73.85042\n",
      " 78.09759  73.12461  71.86463  80.948784 74.75312  72.40875  74.60473\n",
      " 73.28966  81.504875]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "dataset = dataset.batch(128)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        x,y = sess.run([X, Y])\n",
    "        print(type(x))\n",
    "        print(x)\n",
    "        print(x.shape)\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset.map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-c950dfbb041d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m    836\u001b[0m     \"\"\"\n\u001b[0;32m    837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mParallelMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_map_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1826\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1828\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[1;34m(self, g)\u001b[0m\n\u001b[0;32m    486\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;31m# Adds this function into 'g'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;31m# Call func and gather the output tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m       \u001b[1;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mtf_map_func\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1787\u001b[0m           input_dataset.output_classes)\n\u001b[0;32m   1788\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0m_should_unpack_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1789\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1790\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "dataset = dataset.map(lambda x: tf.one_hot(x, 10))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "X,Y = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(100):\n",
    "        print(sess.run([X, Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should we always use tf.data?\n",
    "\n",
    "* For prototyping, feed dict can be faster and easier to write\n",
    "\n",
    "* tf.data is tricky to use when you have complicated preprocessing or multiple data sources\n",
    "\n",
    "* NLP data is noramlly just a sequence of intengers. In this case, transferring the data over to GPU is pretty quick, so the speedup of tf.data isn't that large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Optimizers\n",
    "\n",
    "Session looks at all **trainable variables** that loss depends on and update them\n",
    "\n",
    "## Trainable variables\n",
    "\n",
    "Specify aif a variable should be trained or not,\n",
    "In double q-learning, you want to alternate which q-value functions to update\n",
    "\n",
    "`tf.Variable(initial_value=None, trainable=True)`\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "Advanced optimizers work better when tuned, but are generally harder to tune.\n",
    "\n",
    "Kingma show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Logistic Regression\n",
    "\n",
    "Y are one-hot vector\n",
    "\n",
    "Inference : Y_predicted = softmax(X*w + b)\n",
    "\n",
    "Cross entropy loss  : -log(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c193d79f7d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "DATA_FILE = './birth_life_2010.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "n_train = 60000\n",
    "n_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-3-5f6904e150ed>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Step1 : Read in data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create placeholder for features and labels\n",
    "# each image is represented with a 1x784 tensor\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder')\n",
    "Y = tf.placeholder(tf.int32, [batch_size, 10], name='Y_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: create weights and bias\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10], name='bais'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: build model\n",
    "logits = tf.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-9b6976706780>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "# compute the mean over all the examples in the batch\n",
    "loss = tf.reduce_mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.3668923990362452\n",
      "Average loss epoch 1: 0.29197537907700993\n",
      "Average loss epoch 2: 0.288578315145208\n",
      "Average loss epoch 3: 0.2779807690140251\n",
      "Average loss epoch 4: 0.27164025085441995\n",
      "Average loss epoch 5: 0.2716409575307008\n",
      "Average loss epoch 6: 0.27066237960523104\n",
      "Average loss epoch 7: 0.2718017105629672\n",
      "Average loss epoch 8: 0.26546696114998597\n",
      "Average loss epoch 9: 0.26620330996724556\n",
      "Average loss epoch 10: 0.2627633216254639\n",
      "Average loss epoch 11: 0.2620634049750291\n",
      "Average loss epoch 12: 0.26336513008132123\n",
      "Average loss epoch 13: 0.25907011766047466\n",
      "Average loss epoch 14: 0.2605843072917773\n",
      "Average loss epoch 15: 0.26021302776250527\n",
      "Average loss epoch 16: 0.2581083356361567\n",
      "Average loss epoch 17: 0.2561597439063179\n",
      "Average loss epoch 18: 0.2595723558451746\n",
      "Average loss epoch 19: 0.25346415804260536\n",
      "Average loss epoch 20: 0.258368986990902\n",
      "Average loss epoch 21: 0.2538311272894308\n",
      "Average loss epoch 22: 0.25271753350268433\n",
      "Average loss epoch 23: 0.25572791198889416\n",
      "Average loss epoch 24: 0.2554723637434708\n",
      "Average loss epoch 25: 0.2512926055348559\n",
      "Average loss epoch 26: 0.2539871418045395\n",
      "Average loss epoch 27: 0.2550227825110887\n",
      "Average loss epoch 28: 0.25268675810668295\n",
      "Average loss epoch 29: 0.2501917178015331\n",
      "Total time: 16.296414852142334 seconds\n",
      "Accuracy 0.007146875\n"
     ]
    }
   ],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    writer = tf.summary.FileWriter('./graphs/logistic_reg', sess.graph)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch})\n",
    "            total_loss += loss_batch\n",
    "            \n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        \n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    \n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch})\n",
    "        total_correct_preds += accuracy_batch\n",
    "        \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
