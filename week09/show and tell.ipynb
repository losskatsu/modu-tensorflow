{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_base\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig(object):\n",
    "    def __init__(self):\n",
    "        # File pattern of sharded TFRecord file containing SequenceExample protos.\n",
    "        # Must be provided in training and evaluation modes.\n",
    "        # 이미지의 TFRecord 파일 위치를 나타냄.\n",
    "        self.input_file_pattern = None\n",
    "        # Image format (\"jpeg\" or \"png\").\n",
    "        self.image_format = \"jpeg\"\n",
    "        # Approximate number of values per input shard. Used to ensure sufficient\n",
    "        # mixing between shards in training.\n",
    "        self.values_per_input_shard = 2300\n",
    "        # Minimum number of shards to keep in the input queue.\n",
    "        # 인풋 queue를 유지하기 위한 최소 shard수\n",
    "        self.input_queue_capacity_factor = 2\n",
    "        # Number of threads for prefetching SequenceExample protos.\n",
    "        self.num_input_reader_threads = 1\n",
    "        \n",
    "        # Name of the SequenceExample context feature containing image data.\n",
    "        self.image_feature_name = \"image/data\"\n",
    "        # Name of the SequenceExample feature list containing integer captions.\n",
    "        self.caption_feature_name = \"image/caption_ids\"\n",
    "        # Number of unique words in the vocab (plus 1, for <UNK>).\n",
    "        self.vocab_size = 12000\n",
    "        # Number of threads for image preprocessing. Should be a multiple of 2.\n",
    "        self.num_preprocess_threads = 4\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        # File containing an Inception v3 checkpoint to initialize the variables\n",
    "        # of the Inception model.\n",
    "        self.inception_checkpoint_file = None\n",
    "        \n",
    "        # Dimensions of Inception v3 input images.\n",
    "        self.image_height = 299\n",
    "        self.image_width = 299\n",
    "        # Scale used to initialize model variables.\n",
    "        self.initializer_scale = 0.08\n",
    "        \n",
    "        # LSTM input and output dimensionality, respectively.\n",
    "        self.embedding_size = 512\n",
    "        self.num_lstm_units = 512\n",
    "        # If < 1.0, the dropout keep probability applied to LSTM variables.\n",
    "        self.lstm_dropout_keep_prob = 0.7\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(object):\n",
    "    def __init__(self):\n",
    "        # Number of examples per epoch of training data.\n",
    "        # epoch당 학습용 데이터 수\n",
    "        self.num_examples_per_epoch = 586363\n",
    "        \n",
    "        # Optimizer for training the model.\n",
    "        self.optimizer = \"SGD\"\n",
    "        \n",
    "        # Learning rate for the initial phase of training.\n",
    "        self.initial_learning_rate = 2.0\n",
    "        self.learning_rate_decay_factor = 0.5\n",
    "        self.num_epochs_per_decay = 8.0\n",
    "        \n",
    "        # Learning rate when fine tuning the Inception v3 parameters.\n",
    "        self.train_inception_learning_rate = 0.0005\n",
    "        \n",
    "        # If not None, clip gradients to this value.\n",
    "        self.clip_gradients = 5.0\n",
    "        \n",
    "        # How many model checkpoints to keep.\n",
    "        self.max_checkpoints_to_keep = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception\n",
    "\n",
    "이미지를 받아서 representation을 내놓는 inception 모델을 그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3(images,\n",
    "                trainable=True,\n",
    "                is_training=True,\n",
    "                weight_decay=0.00004,\n",
    "                stddev=0.1,\n",
    "                dropout_keep_prob=0.8,\n",
    "                use_batch_norm=True,\n",
    "                batch_norm_params=None,\n",
    "                add_summaries=True,\n",
    "                scope=\"InceptionV3\"):\n",
    "    \n",
    "    \"\"\"Builds an Inception V3  subgraph for image embeddings\n",
    "    \n",
    "    Args:\n",
    "        images : A float32 Tensor of shape [batch, height, width, channels]\n",
    "        trainable : Whether the inception submodel should be trainable or not.\n",
    "        is_training : Boolean indicationg traing mode or not\n",
    "        weight_decay : Coefficinet for weight regularization.\n",
    "        stddev  :The standard deviation of the truncated normal weight initializer\n",
    "        dropout_keep_prob :  Dropout keep probability\n",
    "        use_batch_norm : Whether to use batch normalization\n",
    "        batch_norm_params : Parameters for batch nomalization\n",
    "        add_summaries\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        end_points : A dictionary of activations from inception_v3 layers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Only consider the inception model to be in training mode if it's trainable\n",
    "    # trainable은 인셉션 모델 자체의 파라미터까지 train할 것인지 결정\n",
    "    # is_training은 그냥 train과 eval, inference를 구분하는 모드값\n",
    "    is_inception_model_training = trainable and is_training\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        # Default parameters for batch normlization.\n",
    "        if not batch_norm_params:\n",
    "            batch_norm_params = {\n",
    "                \"is_training\" : is_inception_model_training,\n",
    "                \"trainable\" : trainable,\n",
    "                # Decay for the moving averages\n",
    "                \"decay\": 0.9997,\n",
    "                # Epsilon ot prevent 0s in variance.\n",
    "                \"epsilon\": 0.001,\n",
    "                # Collection containing the moving mean and moving variance.\n",
    "                \"variables_collections\":{\n",
    "                    \"beta\":None,\n",
    "                    \"gamma\":None,\n",
    "                    \"moving_mean\":[\"moving_vars\"],\n",
    "                    \"moving_variance\":[\"moving_vars\"],\n",
    "                }\n",
    "            }\n",
    "    else:\n",
    "        batch_norm_params=None\n",
    "    \n",
    "    # 만약 inception model 자체도 학습한다면 regularizer를 같이 사용한다.\n",
    "    if trainable:\n",
    "        weights_regularizer = tf.contrib.layers.l2_regularizer(scale=weight_decay)\n",
    "        \n",
    "    else:\n",
    "        weights_regularizer = None\n",
    "    \n",
    "    # values: The list of `Tensor` arguments that are passed to the op function.\n",
    "    with tf.variable_scope(scope, \"InceptionV3\", [images]) as scope:\n",
    "        with slim.arg_scope(\n",
    "        [slim.conv2d, slim.fully_connected],\n",
    "        weights_regularizer=weights_regularizer,\n",
    "        trainable=trainable):\n",
    "            # conv2d의 경우에는 relu, batch_norm을 쓰도록 한다.\n",
    "            with slim.arg_scope(\n",
    "            [slim.conv2d],\n",
    "            weights_initalizer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "            activation_fn=tf.nn.relu,\n",
    "            normalizer_fn=slim.batch_norm,\n",
    "            normalizer_params=batch_norm_params):\n",
    "                # mixed_8x8x2048b   | Mixed_7c\n",
    "                # net=tensor_out: output tensor corresponding to the final_endpoint\n",
    "                # end_points: a set of activations for external use, for example summaries or losses.\n",
    "                net, end_points = inception_v3_base(inputs=images, scope=scope)\n",
    "                \n",
    "                with tf.variable_scope(\"logits\"):\n",
    "                    # [bacth x 8 x 8 x 2048]\n",
    "                    shape = net.get_shape()\n",
    "                    # 8x8 커널을 사용한 avg_pooling -> 1x1으로 만든다.\n",
    "                    net = slim.avg_pool2d(net, shape[1:3], padding=\"VALID\", scope=\"pool\")\n",
    "                    # incetpion 모델을 같이 학습하는 경우에만 dropout을 적용\n",
    "                    net = slim.dropout(\n",
    "                    net, keep_prob=dropout_keep_prob, \n",
    "                    is_training=is_inception_model_training,\n",
    "                    scope=\"dropout\")\n",
    "                    # [bacth, 1x1x2048]로 만든다.\n",
    "                    net = slim.flatten(net, scope=\"flatten\")\n",
    "    \n",
    "    # Add summaries:\n",
    "    if add_summaries:\n",
    "        for v in end_points.values():\n",
    "            tf.contrib.layers.summaries.summarize_activation(v)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process_image\n",
    "TFRecords에 인코딩된 이미지를 실제 float32 텐서로 바꾸고 전처리를 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(encoded_image, is_training, height, width, resize_height=346,\n",
    "                 resize_width=346, thread_id=0, image_format=\"jpeg\"):\n",
    "    \"\"\"\n",
    "    Decode an image, resize and apply random distortions.\n",
    "    Args:\n",
    "        encoded_image: String Tensor containing the image.\n",
    "        is_training: Boolean; whether preprocessing for training or eval.\n",
    "        height: Height of the output image.\n",
    "        width: Width of the output image.\n",
    "        resize_height: If > 0, resize height before crop to final dimensions.\n",
    "        resize_width: If > 0, resize width before crop to final dimensions.\n",
    "        thread_id: Preprocessing thread id used to select the ordering of color\n",
    "        distortions. There should be a multiple of 2 preprocessing threads.\n",
    "        image_format: \"jpeg\" or \"png\"\n",
    "        \n",
    "    Returns:\n",
    "        A float32 Tensor of shape [height, width, 3] with values in [-1, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    # String 텐서를 이미지로 디코딩한다. unit8로 기본적으로 디코딩된다.\n",
    "    with tf.name_scope(\"decode\", values=[encoded_image]):\n",
    "        if image_foramt == \"jpeg\":\n",
    "            # Decode a JPEG-encoded image to a uint8 tensor.\n",
    "            image = tf.image.decode_jpeg(contents=encoded_image, channels=3)\n",
    "        elif image_format == \"png\":\n",
    "            image = tf.image.decode_png(encoded_image, channels=3)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid image format: %s\" % image_format)\n",
    "    # unit8에서 float32로 바꾼다.        \n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    \n",
    "    # 이미지 크기를 리사이즈 - cropping을 걸기전에 크게 만든다.\n",
    "    assert (resize_height > 0) == (resize_width > 0)\n",
    "    if resize_height:\n",
    "        image = tf.image.resize_images(image,\n",
    "                                      size=[resize_height, resize_width],\n",
    "                                      method=tf.image.ResizeMethod.BILINEAR)\n",
    "    # 만약 학습 과정이라면 정중앙을 기준으로 하는게 아니라 랜덤한 위치를 기준으로 crop하여 \n",
    "    # 일반화 능력을 높인다.\n",
    "    if is_training:\n",
    "        image = tf.random_crop(image, [height, width, 3])\n",
    "    # eval과정이라면 정중앙으로 crop한다.    \n",
    "    else:\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n",
    "        \n",
    "    # 이미지 값의 범위를 [-1,1]로 맞춘다.\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefetch_input_data(reader,\n",
    "                       file_pattern,\n",
    "                       is_training,\n",
    "                       batch_size,\n",
    "                       values_per_shard,\n",
    "                       input_queue_capacity_factor=16,\n",
    "                       num_reader_threads=1,\n",
    "                       shard_queue_name=\"filename_queue\",\n",
    "                       value_queue_name=\"input_queue\"):\n",
    "    \"\"\"Prefetches string values from disk into an input queue.\n",
    "    \n",
    "    In training the capacity of the queue is important because a larger queue\n",
    "    means better mixing of training examples between shards.\n",
    "    학습을 할때에 queue의 용량이 중요한데 queue가 클수록 분할파일 사이의 training example이\n",
    "    더 잘 섞이기 때문이다.\n",
    "    \n",
    "    The minimum number of values kept in the queue is \n",
    "    values_per_shard * input_queue_capacity_factor,\n",
    "    where input_queue_memory factor should be chosen to trade-off better mixing with memory usage.\n",
    "    \n",
    "    input_queue_memory factor는 메모리 사용량과 더 잘 mixing되느냐 사이에 trade-off 관계이다.\n",
    "    Args:\n",
    "        reader: Instance of tf.ReaderBase.\n",
    "        file_pattern: Comma-separated list of file patterns (e.g. /tmp/train_data-?????-of-00100).\n",
    "        is_training: Boolean; whether prefetching for training or eval.\n",
    "        batch_size: Model batch size used to determine queue capacity.\n",
    "        values_per_shard: Approximate number of values per shard.\n",
    "        input_queue_capacity_factor: Minimum number of values to keep in the queue\n",
    "            in multiples of values_per_shard. See comments above.\n",
    "        num_reader_threads: Number of reader threads to fill the queue.\n",
    "        shard_queue_name: Name for the shards filename queue.\n",
    "        value_queue_name: Name for the values input queue.\n",
    "    Returns:\n",
    "        A Queue containing prefetched string values.\n",
    "    \"\"\"\n",
    "    data_files = []\n",
    "    for pattern in file_pattern.split(\",\"):\n",
    "        data_files.extend(tf.gfile.Glob(pattern))\n",
    "    if not data_files:\n",
    "        tf.logging.fatal(\"Found no input files matching %s\", file_pattern)\n",
    "    else:\n",
    "        tf.logging.info(\"Prefetching values from %d files matching %s\",len(data_files), file_pattern)\n",
    "    \n",
    "    # 학습 단계라면 랜덤하게 shuffle을 하는 queue를 사용한다.\n",
    "    if is_training:\n",
    "        # string_input_producer: Output strings (e.g. filenames) to a queue for an input pipeline.\n",
    "        filename_queue = tf.train.string_input_producer(\n",
    "            data_files, shuffle=True, capacity=16, name=shard_queue_name)\n",
    "        # min_queue_examples = 2300 * 2\n",
    "        min_queue_examples = values_per_shard * input_queue_capacity_factor\n",
    "        capacity = min_queue_examples + 100 * batch_size\n",
    "        # RandomShuffleQueue:A queue implementation that dequeues elements in a random order.\n",
    "        values_queue = tf.RandomShuffleQueue(capacity=capacity,\n",
    "                                             min_after_dequeue=min_queue_examples,\n",
    "                                             dtypes=[tf.string],\n",
    "                                             name=\"random_\" + value_queue_name)\n",
    "    # 학습이 아니라면 FIFOqueue를 사용한다. (순서대로 캡션이 나오도록)    \n",
    "    else:\n",
    "        filename_queue = tf.train.string_input_producer(data_files, shuffle=False, \n",
    "                                                        capacity=1, name=shard_queue_name)\n",
    "        capacity = values_per_shard + 3 * batch_size\n",
    "        # FIFOQueue:A queue implementation that dequeues elements in first-in first-out order.\n",
    "        values_queue = tf.FIFOQueue(capacity=capacity, dtypes=[tf.string],\n",
    "                                    name=\"fifo_\" + value_queue_name)\n",
    "        \n",
    "    \n",
    "    # enqueue_ops: List of enqueue ops to run in threads later.\n",
    "    enqueue_ops = []\n",
    "    for _ in range(num_reader_threads):\n",
    "        # queue=A Queue or a mutable string Tensor representing a handle to a Queue\n",
    "        # read():Returns the next record (key, value) pair produced by a reader.\n",
    "        _, value = reader.read(queue=filename_queue)\n",
    "        # enqueue() : Enqueues one element to this queue.\n",
    "        enqueue_ops.append(values_queue.enqueue([value]))\n",
    "    \n",
    "    # add_queue_runne : Adds a `QueueRunner` to a collection in the graph.\n",
    "    # queue_runners라는 collection에 QueueRunner를 더한다.\n",
    "    tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(values_queue, enqueue_ops))\n",
    "    tf.summary.scalar(\"queue/%s/fraction_of_%d_full\" % (values_queue.name, capacity),\n",
    "                      tf.cast(values_queue.size(), tf.float32) * (1. / capacity))\n",
    "    return values_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dequeue해서 나온 SequenceExample proto를 parsing 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequence_example(serialized, image_feature, caption_feature):\n",
    "    \"\"\"\n",
    "    Parses a tensorflow.SequenceExample into an image and caption.\n",
    "    Args:\n",
    "       serialized: A scalar string Tensor; a single serialized SequenceExample.\n",
    "       image_feature: Name of SequenceExample context feature containing image data.\n",
    "       caption_feature: Name of SequenceExample feature list containing integer captions.\n",
    "       \n",
    "    \n",
    "    Returns:\n",
    "        encoded_image: A scalar string Tensor containing a JPEG encoded image.\n",
    "        caption: A 1-D uint64 Tensor with dynamically specified length.\n",
    "    \"\"\"\n",
    "    # parse_single_sequence_example : Parses a single `SequenceExample` proto.\n",
    "    # Returns : The first dict contains the context key/values.\n",
    "    # The second dict contains the feature_list key/values\n",
    "    context, sequence = tf.parse_single_sequence_example(serialized=serialized,\n",
    "                                                        context_features={\n",
    "                                                            image_feature: tf.FixedLenFeature([], dtype=tf.string)\n",
    "                                                        },\n",
    "                                                        sequence_features={\n",
    "                                                            caption_feature: tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "                                                        })\n",
    "    \n",
    "    encoded_image = context[image_feature]\n",
    "    caption = sequence[caption_feature]\n",
    "    return encoded_image, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_with_dynamic_pad(images_and_captions, batch_size, queue_capacity,add_summaries=True):\n",
    "    \"\"\"\n",
    "    Batches input images and captions.\n",
    "    \n",
    "    This function splits the caption into an input sequence and a target sequence,\n",
    "    where the target sequence is the input sequence right-shifted by 1. Input and\n",
    "    target sequences are batched and padded up to the maximum length of sequences\n",
    "    in the batch. A mask is created to distinguish real words from padding words.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        images_and_captions: A list of pairs [image, caption], where image is a\n",
    "        Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n",
    "        any length. Each pair will be processed and added to the queue in a\n",
    "        separate thread.\n",
    "        \n",
    "        batch_size: Batch size\n",
    "        queue_capacity: Queue capacity.\n",
    "        add_summaries: If true, add caption length summaries.\n",
    "        \n",
    "    Returns:\n",
    "        images: A Tensor of shape [batch_size, height, width, channels].\n",
    "        input_seqs: An int32 Tensor of shape [batch_size, padded_length].\n",
    "        target_seqs: An int32 Tensor of shape [batch_size, padded_length].\n",
    "        mask: An int32 0/1 Tensor of shape [batch_size, padded_length].\n",
    "    \"\"\"\n",
    "    enqueue_list = []\n",
    "    for image, caption in images_and_captions:\n",
    "        caption_length = tf.shape(caption)[0]\n",
    "        # 배치 차원을 더한다. intput_length = [caption_length-1]\n",
    "        input_length = tf.expand_dims(tf.subtract(caption_length, 1), 0)\n",
    "        # 0번째 idx부터 input_length까지 자른다. 제일 마지막 위치 idx가 잘림.\n",
    "        input_seq = tf.slice(input_=caption, begin=[0], size=input_length)\n",
    "        # 1번째부터 idx부터 input_length만큼 자른다. 제일 첫번째 0번째 idx가 잘림.\n",
    "        target_seq = tf.slice(input_=caption, begin=[1], size=input_length)\n",
    "        # input_length 길이만큼 1로채운 벡터\n",
    "        indicator = tf.ones(input_length, dtype=tf.int32)\n",
    "        enqueue_list.append([image, input_seq, target_seq, indicator])\n",
    "    \n",
    "    # batch_join: Runs a list of tensors to fill a queue to create batches of examples.\n",
    "    # dynamic_pad :The given dimensions are padded upon dequeue so that tensors within a\n",
    "    # batch have the same shapes.\n",
    "    # tensors_list: A list of tuples or dictionaries of tensors to enqueue.\n",
    "    \n",
    "    # Enqueues a different list of tensors in different threads.\n",
    "    # Implemented using a queue -- a `QueueRunner` for the queue\n",
    "    # is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
    "    images, input_seqs, target_seqs, mask = tf.train.batch_join(enqueue_list,\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                capacity=queue_capacity,\n",
    "                                                                dynamic_pad=True,\n",
    "                                                                name=\"batch_and_pad\")\n",
    "    \n",
    "    if add_summaries:\n",
    "        lengths = tf.add(tf.reduce_sum(mask, 1), 1)\n",
    "        tf.summary.scalar(\"caption_length/batch_min\", tf.reduce_min(lengths))\n",
    "        tf.summary.scalar(\"caption_length/batch_max\", tf.reduce_max(lengths))\n",
    "        tf.summary.scalar(\"caption_length/batch_mean\", tf.reduce_mean(lengths))\n",
    "        \n",
    "    return images, input_seqs, target_seqs, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show and Tell model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowAndTellModel(object):\n",
    "    \n",
    "    def __init__(self, config, mode, train_inception=False):\n",
    "        \n",
    "        assert mode in [\"train\", \"eval\", \"inference\"]\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        # inception도 같이 학습할 것인지 여부\n",
    "        self.train_inception = train_inception\n",
    "        # A Reader that outputs the records from a TFRecords file.\n",
    "        self.reader = tf.TFRecordReader()\n",
    "        # 기본적으로 사용될 initializer\n",
    "        self.initializer = tf.random_uniform_initializer(minval=-self.config.initializer_scale,\n",
    "                                                        maxval=self.config.initializer_scale)\n",
    "        \n",
    "        # float32 Tensor with shape [batch_size, height, width, channels]\n",
    "        self.images = None\n",
    "        # int32 Tensor with shape [batch_size, padded_length]\n",
    "        self.input_seqs = None\n",
    "        # int32 Tensor with shape [batch_size, padded_length]\n",
    "        self.target_seqs = None\n",
    "        # int32 0/1 Tesnor with shape [batch_size, padded_length]\n",
    "        self.input_mask = None\n",
    "        # float32 Tensor with shape [batch_size, embeddign_size]\n",
    "        self.image_embeddings = None\n",
    "        # A float32 Tensor with shape [batch_size, padded_length, embedding_size].\n",
    "        self.seq_embeddings = None\n",
    "        \n",
    "        # A float32 scalar Tensor; the total loss for the trainer to optimize.\n",
    "        self.total_loss = None\n",
    "        # A float32 Tensor with shape [batch_size * padded_length].\n",
    "        self.target_cross_entropy_losses = None\n",
    "        # A float32 Tensor with shape [batch_size * padded_length].\n",
    "        # seq2seq loss를 계산하기 위한 mask값\n",
    "        self.target_cross_entropy_loss_weights = None\n",
    "        \n",
    "        # Collection of variables from the inception submodel.\n",
    "        self.inception_variables = []\n",
    "        # Function to restore the inception submodel from checkpoint.\n",
    "        self.init_fn = None\n",
    "        # Global step Tensor.\n",
    "        self.global_step = None\n",
    "    \n",
    "    # 그냥 train 모드인지 확인하는 함수\n",
    "    def is_training(self):\n",
    "        \"\"\"Returns true if the model is built for training mode.\"\"\"\n",
    "        return self.mode == \"train\"\n",
    "    \n",
    "    # config에 맞춰서 이미지 string으로 부터 처리하는 함수\n",
    "    def process_image(self, encoded_image, thread_id=0):\n",
    "        \"\"\"\n",
    "        Decodes and processes an image string.\n",
    "        Args:\n",
    "            encoded_image: A scalar string Tensor; the encoded image.\n",
    "            thread_id: Preprocessing thread id used to select the ordering of color distortions.\n",
    "            \n",
    "        Returns:\n",
    "            A float32 Tensor of shape [height, width, 3]; the processed image.\n",
    "        \"\"\"\n",
    "        \n",
    "        return process_image(encoded_image, is_training=self.is_training(),\n",
    "                            height=self.config.image_height,\n",
    "                            width=self.config.image_width,\n",
    "                            thread_id=thread_id,\n",
    "                            image_format=self.config.image_format)\n",
    "    \n",
    "    \n",
    "    def build_inputs(self):\n",
    "        \"\"\"Input prefetching, preprocessing and batching\n",
    "        \n",
    "        Outputs:\n",
    "            self.images\n",
    "            self.input_seqs\n",
    "            self.target_seqs\n",
    "            self.input_mask\n",
    "        \"\"\"\n",
    "        # 만약 inference 모드라면 사용자가 임의의 주는 데이터를 받아야 한다. \n",
    "        if self.mode == \"inference\":\n",
    "            # In Inference mode, images and inputs are fed via placeholders\n",
    "            # encoded image를 받는 placeholder\n",
    "            image_feed = tf.placeholder(dtype=tf.string, shape=[], name=\"image_feed\")\n",
    "            # Inference 할때에 [batch_size] 짜리 input_feed를 받는다.\n",
    "            input_feed = tf.placeholder(dtype=tf.int64, shape=[None],  name=\"input_feed\")\n",
    "            \n",
    "            # Process image and insert batch dimensions\n",
    "            # tf.expand_dims: Inserts a dimension of 1 into a tensor's shape.\n",
    "            # [height, width, 3] -> [batch,height, width, 3] \n",
    "            images = tf.expand_dims(self.process_image(image_feed), 0)\n",
    "            # [batch] -> [batch, length]\n",
    "            input_seqs = tf.expand_dims(input_feed, 1)\n",
    "            \n",
    "            # No target sequneces or input mask in inference mode\n",
    "            # inference 중에는 loss를 계산할 필요가 없으므로\n",
    "            target_seqs = None\n",
    "            input_mask = None\n",
    "        # mode is train or evaluation    \n",
    "        else:\n",
    "            # Prefetch serialized SequenceExample protos.\n",
    "            # 자신의 TFRecoder reader와 인풋 파일 위치, configuration 값을 넘겨서 queue를 얻는다.\n",
    "            # 이 queue에는 prefetched된 string 값들이 있다.\n",
    "            input_queue = prefetch_input_data(self.reader,\n",
    "                                             self.config.input_file_pattern,\n",
    "                                             is_training=self.is_training(),\n",
    "                                             batch_size=self.config.batch_size,\n",
    "                                             values_per_shard=self.config.values_per_input_shard,\n",
    "                                             input_queue_capacity_factor=self.config.input_queue_capacity_factor,\n",
    "                                             num_reader_threads=self.config.num_input_reader_threads)\n",
    "            \n",
    "            \n",
    "            # Image processing and radnom distortion. Split across multiple threads\n",
    "            # with each thread applying a slightly difference distortion.\n",
    "            assert self.config.num_preprocess_threads % 2 ==0\n",
    "            images_and_captions = []\n",
    "            # thread 갯수만큼 TFRecorde에 있는 SeqeunceExample protos를 본격적으로FIFOQueue float32 텐서로 바꾼다.\n",
    "            for thread_id in range(self.config.num_preprocess_threads):\n",
    "                # queue에서 SequenceExample을 dequeue를 한다.\n",
    "                serialized_sequence_example = input_queue.dequeue()\n",
    "                \n",
    "                # dequeue한 녀석에서 실제 이미지에 해당되는 위치(string)와 이에 대응되는 캡션을 얻는다.\n",
    "                encoded_image, caption = parse_sequence_example(\n",
    "                serialized_seqeunce_example,\n",
    "                image_feature=self.config.image_feature_name,\n",
    "                caption_feature=self.config.caption_feature_name)\n",
    "                \n",
    "                # thread 별로 distorting을 걸 수 있게 된다.\n",
    "                image = self.process_image(encoded_image, thread_id=thread_id)\n",
    "                # float32 텐서와 캡션을 리스트로 엮는다.\n",
    "                images_and_captions.append([image, caption])\n",
    "            \n",
    "            # Batch inputs\n",
    "            queue_capacity = (2*self.config.num_preprocess_threads * \n",
    "                             self.config.batch_size)\n",
    "            \n",
    "            # 배치단위로 나오는 이미지, 그리고 padding이 적용된 input_seq, target_seqs, \n",
    "            # padding words를 구분짓는 mask를 얻는다.\n",
    "            # target은 그냥 input_seq가 오른쪽으로 한칸 shift된 것이다.\n",
    "            images, input_seqs, target_seqs, input_masks = batch_with_dynamic_pad(images_and_captions,\n",
    "                                            batch_size=self.config.batch_size,\n",
    "                                            queue_capacity=queue_capacity)\n",
    "        \n",
    "        # [batch_size, height, width, channels]\n",
    "        self.images = images\n",
    "        # [batch_size, padded_length].\n",
    "        self.input_seqs = input_seqs\n",
    "        # [batch_size, padded_length].\n",
    "        self.target_seqs = target_seqs\n",
    "        # [batch_size, padded_length].\n",
    "        self.input_masks = input_masks\n",
    "    \n",
    "\n",
    "    # 배치단위로 이미지를 inception에 넣고 나온 결과를 LSTM에 들어갈 수 있도록\n",
    "    # embedding 사이즈에 맞추도록 한다.\n",
    "    def build_image_embeddings(self):\n",
    "        \"\"\"Builds the image model subgraph and generates image embeddigns\n",
    "        \n",
    "        Inputs:\n",
    "        self.images\n",
    "        \n",
    "        Outputs:\n",
    "        self.image_embeddings\n",
    "        \"\"\"\n",
    "        inception_output = inception_v3(images=self.images,\n",
    "                                        trainable=self.train_inception,\n",
    "                                        is_training=self.is_training())\n",
    "        self.inception_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"InceptionV3\")\n",
    "        \n",
    "        # Map inception output into embedding space.\n",
    "        with tf.variable_scope(\"image_embedding\") as scope:\n",
    "            # self.config.embedding_size = 512\n",
    "            image_embeddings = tf.contrib.layers.fully_connected(inputs=inception_output,\n",
    "                                                                num_outputs=self.config.embedding_size,\n",
    "                                                                activation_fn=None,\n",
    "                                                                weights_initializer=self.initializer,\n",
    "                                                                biases_initializer=None,\n",
    "                                                                scope=scope)\n",
    "        # Save the embedding size in the graph.\n",
    "        tf.constant(self.config.embedding_size, name=\"embedding_size\")\n",
    "        \n",
    "        # [bactch, embedding_size]\n",
    "        self.image_embeddings = image_embeddings\n",
    "    \n",
    "    # word embedding을 사이즈에 맞춰서 얻도록 한다. \n",
    "    def build_seq_embeddings(self):\n",
    "        \"\"\"Builds the input sequence embeddings\n",
    "        \n",
    "        Inputs:\n",
    "        self.input_seqs\n",
    "        \n",
    "        Outputs:\n",
    "        self.seq_embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.variable_scope(\"seq_embedding\"), tf.device(\"/cpu:0\"):\n",
    "            embedding_map = tf.get_variable(name=\"map\", shape=[self.config.vocab_size, self.config.embedding_size],\n",
    "                                           initializer=self.initializer)\n",
    "            # [batch_size, padded_length]\n",
    "            seq_embeddings = tf.nn.embedding_lookup(embedding_map, self.input_seqs)\n",
    "        # [batch_size, padded_length, embedding_size]    \n",
    "        self.seq_embeddings = seq_embeddings\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the model.\n",
    "        \n",
    "        Inputs:\n",
    "            self.image_embeddings\n",
    "            self.seq_embeddings\n",
    "            self.target_seqeunces(training and eval only)\n",
    "            self.input_mask(training and eval only)\n",
    "        \n",
    "        Outputs:\n",
    "            self.total_loss(training and eval only)\n",
    "            self.target_cross_entropy_losses(training and eval only)\n",
    "            self.target_cross_entropy_loss_weights(training and eval only)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.config.num_lstm_units, \n",
    "                                                state_is_tuple=True)\n",
    "        # 만약 train 모드일 경우 LSTM cell의 인풋과 아웃풋 connection에 dropout을 건다.\n",
    "        if self.mode == \"train\":\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cell, \n",
    "                                                      input_keep_prob=self.config.lstm_dropout_keep_prob,\n",
    "                                                     output_keep_prob=self.config.lstm_dropout_keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "            # Feed the image embeddings to set the initial LSTM state.\n",
    "            # self.image_embeddings : [bactch, embedding_size]\n",
    "            zero_state = lstm_cell.zero_state(batch_size=self.image_embeddings.get_shape()[0], \n",
    "                                              dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(self.image_embeddings, zero_state)\n",
    "            \n",
    "            # Allow the LSTM variavles to be reused.\n",
    "            # scope안에 AUTO_REUSE 하는 것과 같다.\n",
    "            lstm_scope.reuse_variables()\n",
    "            self.state_size = lstm_cell.state_size\n",
    "            \n",
    "            # inference일 경우에는 state_feed placeholder로 생성된 이전 step의 캡션에 대한\n",
    "            # state값을 넘겨서 결과를 계산한다.\n",
    "            if self.mode == \"inference\":\n",
    "                # In inference mode, use concatenated states for convinient feeding and fetcing\n",
    "                # [bacth_size, 2*num_lstm_units]\n",
    "                tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n",
    "                \n",
    "                # Placeholder for feeding a batch of concatenated states.\n",
    "                # lstm_cell.state_size=LSTMStateTuple(c=512, h=512)\n",
    "                # sum(lstm_cell.state_size) = 1024\n",
    "                state_feed = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None, sum(lstm_cell.state_size)],\n",
    "                                           name=\"state_feed\")\n",
    "                # Splits a tensor into sub tensors.\n",
    "                # [<tf.Tensor 'split:0' shape=(?, 512) dtype=float32>,\n",
    "                # <tf.Tensor 'split:1' shape=(?, 512) dtype=float32>]\n",
    "                state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n",
    "                \n",
    "                # Run a single LSTM step.\n",
    "                # embedding된 단어와 placeholder로 동작하는 state_tuple을 넣어서\n",
    "                # 새로운 time step에 해당되는 결과와 state_tuple을 얻는다.\n",
    "                lstm_outputs, state_tuple = lstm_cell(inputs=tf.squeeze(self.seq_embeddings, axis=[1]),\n",
    "                                                     state=state_tuple)\n",
    "                \n",
    "                # Concatenate the resulting state.\n",
    "                tf.concat(values=state_tuple, axis=1, name=\"state\")\n",
    "            \n",
    "            # train or eval 이라면, 훈련용 seq_embedding이 들어가서\n",
    "            # 전체 step에 해당되는 lstm_output을 얻는다.\n",
    "            else:\n",
    "                # Run the batch of sequence embeddings through the LSTM.\n",
    "                # 1의 갯수가 groud truth가 되는 sequence의 길이이다.\n",
    "                # sequence_length = [batch_size]\n",
    "                seqeunce_length = tf.reduce_sum(self.input_mask, axis=1)\n",
    "                lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                                   inputs=self.seq_embeddings,\n",
    "                                                   sequence_length=sequence_length,\n",
    "                                                   initial_state=initial_state,\n",
    "                                                   dtype=tf.float32,\n",
    "                                                   scope=lstm_scope)\n",
    "        \n",
    "        # Stack batches vertically. [batch_size*sequence_length, output_size]\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            # [batch_size*sequence_length, self.config.vocab_size]\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "            inputs=lstm_outputs, num_outputs=self.config.vocab_size,\n",
    "            activation_fn=None, weights_initializer=self.initializer,\n",
    "            scope=logits_scope)\n",
    "        \n",
    "        # Inference일떄에는 loss를 구할 필요가 없다.\n",
    "        # 바로 softmax를 돌려서 vocab에 대한 확률분포를 얻도록 하자\n",
    "        if self.mode == \"inference\":\n",
    "            tf.nn.softmax(logits, name=\"softmax\")\n",
    "            \n",
    "        else:\n",
    "            # int32, [batch_size, padded_length] -> [-1]\n",
    "            targets = tf.reshape(self.target_seqs, [-1])\n",
    "            \n",
    "            # int32 0/1 Tesnor with shape [batch_size, padded_length] -> [-1], float\n",
    "            weights = tf.to_float(tf.reshape(self.input_mask, [-1]))\n",
    "            \n",
    "            \n",
    "            # Compute losses\n",
    "            # logits =[d_0, d_1, ..., d_{r-1}, num_classes]\n",
    "            # lables = [d_0, d_1, ..., d_{r-1}]\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(lables=targets, logits=logits)\n",
    "            # 배치 단위로 loss를 나눈다.\n",
    "            batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "                               tf.reduce_sum(weights), name=\"batch_loss\")\n",
    "            # tf.losses.add_loss(loss, loss_collection='losses')\n",
    "            tf.losses.add_loss(batch_loss)\n",
    "            # tf.losses.get_total_loss(add_regularization_losses=True, name='total_loss')\n",
    "            total_loss = tf.losses.get_total_loss()\n",
    "            \n",
    "            # Add summaries.\n",
    "            tf.summary.scalar(\"losses/batch_loss\", batch_loss)\n",
    "            tf.summary.scalar(\"losses/total_loss\", total_loss)\n",
    "            \n",
    "            for var in tf.trainable_variables():\n",
    "                tf.summary.histogram(\"parameters/\" + var.op.name, var)\n",
    "                \n",
    "            self.total_loss = total_loss\n",
    "            self.target_cross_entropy_losses = losses\n",
    "            self.target_cross_entropy_loss_weights = weights\n",
    "    \n",
    "    # inception의 파라미터를 불러오는 saver와 closure를 설정한다.\n",
    "    def setup_inception_initializer(self):\n",
    "        \"\"\"Set ip the function to restore inception variables from checkpoint.\"\"\"\n",
    "        if self.mode != \"inference\":\n",
    "            # Restroe inception variables only.\n",
    "            # self.inception_variables은 build_image_embeddings()에서 만들어진다.\n",
    "            saver = tf.train.Saver(var_list=self.inception_variables)\n",
    "            \n",
    "            def restore_fn(sess):\n",
    "                tf.logging.info(\"Restoring Inception variables from checkpoint file %s\",\n",
    "                               self.config.inception_checkpoint_file)\n",
    "                saver.restore(sess, self.config.inception_checkpoint_file)\n",
    "                \n",
    "            self.init_fn = restore_fn\n",
    "            \n",
    "    def setup_global_step(self):\n",
    "        \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "        # collections: List of graph collections keys. The new variable is added to\n",
    "        # these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]\n",
    "        global_step = tf.Variable(initial_value=0, name=\"global_step\", trainable=False, \n",
    "                                  collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "    def build(self):\n",
    "        \"\"\"Creates all ops for training and evaluation\"\"\"\n",
    "        # image, image_seq, target_seq, target_mask를 얻는다.\n",
    "        self.build_inputs()\n",
    "        # inceptionV3에서 나온 결과물에 fully connected를 걸어서 512짜리 representation을 얻는다.\n",
    "        self.build_image_embeddings()\n",
    "        # input으로 사용되는 sequence에 대해 embedding lookup을 적용한다. \n",
    "        self.build_seq_embeddings()\n",
    "        # total_loss, cross_entropy_loss를 구한다.\n",
    "        self.build_model()\n",
    "        # Train와 eval일 경우 inception 파라미터를 복구하도록 하는 closures에 대한 alias를 가진다.\n",
    "        self.setup_inception_intiailizer()\n",
    "        # global_step을 만든다.\n",
    "        self.setup_global_step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로깅 모드 변경 : 기본 WARN에서 INFO로\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig()\n",
    "model_config.input_file_pattern = \"E:/mscoco/outputs\"\n",
    "# 사전 학습이 된 inception 체크포인트\n",
    "model_config.inception_checkpoint_file = \"./inception_v3/inception_v3.ckpt\"\n",
    "training_config = TrainingConfig()\n",
    "log_every_n_steps = 1\n",
    "number_of_steps = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for saving and loading model checkpoints.\n",
    "train_dir = \"D:\\PythonLab\\CS20\\Show and Tell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception 모델도 학습할 것인지 여부\n",
    "train_inception = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Tensorflow graph\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    # Build model\n",
    "    model = ShowAndTellModel(config=model_config, mode=\"train\", train_inception=train_inception)\n",
    "    # 모델의 그래프를 그린다.\n",
    "    model.build()\n",
    "    \n",
    "    t_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(t_vars, print_info=True)\n",
    "    # Set up the learning rate.\n",
    "    learnin_rate_decay_fn = None\n",
    "    # 만약 inception 모델 파라미터 또한 학습한다면\n",
    "    if train_inception:\n",
    "        # 0.0005로 설정\n",
    "        learning_rate = tf.constant(training_config.train_inception_learning_rate)\n",
    "    # 첫번째 phase로만 학습한다면(inception model의 파라미터는 학습에서 제외)->learning rate decay\n",
    "    # learning_rate과 global_step을 받아들이는 함수를 만든다.\n",
    "    else:\n",
    "        learning_rate = tf.constant(training_config.initial_learning_rate)\n",
    "        # learning rate decay를 쓴다면\n",
    "        if training_config.learning_rate_decay_factor > 0:\n",
    "            # epoch당 배치수\n",
    "            num_batches_per_epoch = (training_config.num_examples_per_epoch/\n",
    "                                     model_config.batch_size)\n",
    "            \n",
    "            # decay한번 할때마다의 배치수\n",
    "            decay_steps = int(num_batches_per_epoch * training_config.num_epochs_per_decay)\n",
    "            \n",
    "            \n",
    "            def _learning_rate_decay_fn(learning_rate, global_step):\n",
    "                #Applies exponential decay to the learning rate.\n",
    "                return tf.train.exponential_decay(learning_rate,global_step,\n",
    "                                                 decay_steps=decay_steps,\n",
    "                                                  decay_rate=training_config.learning_rate_decay_factor,\n",
    "                                                 staricase=True)\n",
    "            # closure에 alias를 준다.\n",
    "            learning_rate_decay_fn = _learning_rate_decay_fn\n",
    "            \n",
    "    # Set up the training ops.\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss=model.total_loss,\n",
    "                                              global_step=model.global_step,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              optimizer=training_config.optimizer,\n",
    "                                              clip_gradients=training_config.clip_gradients,\n",
    "                                              learning_rate_decay_fn=learning_rate_decay_fn)\n",
    "    # Set up the saver for saving and restoring model checkpoints.\n",
    "    saver = tf.train.Saver(max_to_keep=training_config.max_checkpoints_to_keep)\n",
    "    \n",
    "    # Run training\n",
    "    # logdir: The directory where training logs are written to.\n",
    "    # init_fn: An optional callable to be executed after `init_op` is called. The\n",
    "    # callable must accept one argument, the session being initialized.\n",
    "    tf.contrib.slim.learning.train(train_op=train_op, logdir=train_dir, log_every_n_steps=log_every_n_steps,\n",
    "                                  graph=g, global_step=model.global_step,\n",
    "                                  number_of_steps=number_of_steps, init_fn=model.init_fn,\n",
    "                                  saver=saver)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary를 담당하는 클래스\n",
    "class Vocabulary(object):\n",
    "    \n",
    "    def __init__(self, vocab_file, start_word=\"<S>\", end_word=\"</S>\", unk_word=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Initializes the vocabulary.\n",
    "        \n",
    "        \"\"\"\n",
    "        # 만약 vocab_file이 없다면 에러를 띄운다.\n",
    "        if not tf.gfile.Exists(vocab_file):\n",
    "            tf.logging.fatal(\"Vocab file %s not found.\", vocab_file)\n",
    "            \n",
    "        tf.logging.info(\"Initializing vocabulary from file: %s\", vocab_file)\n",
    "        \n",
    "        with tf.gfile.GFile(name=vocab_file, mode=\"r\") as f:\n",
    "            reverse_vocab = list(f.readlines())\n",
    "        # escape sequence를 제거하는 듯    \n",
    "        reverse_vocab = [line.split()[0] for line in reverse_vocab]\n",
    "        # 각 token이 vocab 리스트에 들어있는지 확인한다.\n",
    "        assert start_word in reverse_vocab\n",
    "        assert end_word in reverse_vocab\n",
    "        if unk_word not in reverse_vocab:\n",
    "            reverse_vocab.append(unk_word)\n",
    "        \n",
    "        # 순서를 바로 잡아서 dict로 만든다. 단어가 key이고 value가 id이다.\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(reverse_vocab)]) \n",
    "        \n",
    "        tf.logging.info(\"Created vocabulary with %d words\" % len(vocab))\n",
    "        \n",
    "        # Attribute로 등록한다.\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = reverse_vocab\n",
    "        \n",
    "        self.start_id = vocab[start_word]\n",
    "        self.end_id = vocab[end_word]\n",
    "        self.unk_id = vocab[unk_word]\n",
    "        \n",
    "    def word_to_id(self, word):\n",
    "        if word in self.vocab:\n",
    "            return self.vocab[word]\n",
    "        \n",
    "        else:\n",
    "            return self.unk_id\n",
    "        \n",
    "    def id_to_word(self, word_id):\n",
    "        if word_id >= len(self.reverse_vocab):\n",
    "            return self.reverse_vocab[self.unk_id]\n",
    "        else:\n",
    "            return self.reverse_vocab[word_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceWrapper(InferenceWrapperBase):\n",
    "    \"\"\"Base wrapper class for performing inference with an image-to-text model.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _create_restore_fn(self, checkpoint_path, saver):\n",
    "        \"\"\"Creates a function that restores a model from checkpoint.\"\"\"\n",
    "        # 만약 체크포인트 디렉토리가 있다면\n",
    "        if tf.gfile.IsDirectory(checkpoint_path):\n",
    "            # 체크포인트 파일중 가장 최신 파일의 이름을 가져온다.\n",
    "            # Finds the filename of latest saved checkpoint file.\n",
    "            checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n",
    "            if not checkpoint_path:\n",
    "                raise ValueError(\"No checkpoint file found in: %s\" % checkpoint_path)\n",
    "                \n",
    "        # 가장 최근의 체크포인트로 복구하는 함수        \n",
    "        def _restore_fn(sess):\n",
    "            tf.logging.info(\"Loading model from checkpoint: %s\", checkpoint_path)\n",
    "            saver.restore(sess, checkpoint_path)\n",
    "            tf.logging.info(\"Successfully loaded checkpoint: %s\", os.path.basename(checkpoint_path))\n",
    "        \n",
    "        return _restore_fn\n",
    "    \n",
    "    # 모델 그래프를 그리고 체크포인트를 복구하는 함수를 얻는다.\n",
    "    def build_graph_from_config(self, model_config, checkpoint_path):\n",
    "        \"\"\"Builds the inference graph from a configuration object.\"\"\"\n",
    "        tf.logging.info(\"Building model.\")\n",
    "        # build model method를 실행한다. \n",
    "        self.build_model(model_config)\n",
    "        # 체크포인트 복구를 위한 saver\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # 여기의 리턴값으로 _restore_fn closure를 다시 리턴한다.\n",
    "        return self._create_restore_fn(checkpoint_path, saver)\n",
    "    \n",
    "    \n",
    "    def build_model(self, model_config):\n",
    "        \"\"\"Builds the model for inference.\n",
    "        Args:\n",
    "            model_config: Object containing configuration for building the model.\n",
    "        Returns:\n",
    "            model: The model object.\n",
    "        \"\"\"\n",
    "        model = ShowAndTellModel(model_config, mode=\"inference\")\n",
    "        # 모델 그래프를 그린다.\n",
    "        model.build()\n",
    "        return model\n",
    "    \n",
    "    # 인코딩된 이미지를 넣어서 초기 state를 얻는 함수.\n",
    "    def feed_image(self, sess, encoded_image):\n",
    "        \"\"\"Feeds an image and returns the initial model state.\n",
    "        Args:\n",
    "            sess: TensorFlow Session object.\n",
    "            encoded_image: An encoded image string.\n",
    "        \n",
    "        Returns:\n",
    "            state: A numpy array of shape [1, state_size].\n",
    "        \"\"\"\n",
    "        \n",
    "        # image_feed placeholder에 인코딩된 이미지를 넣어서 lstm의 initial state opeartion node을 실행한다.\n",
    "        initial_state = sess.run(fetches=\"lstm/initial_state:0\",\n",
    "                                feed_dict={\"image_feed:0\": encoded_image})\n",
    "        # [bacth_size, 2*num_lstm_units]\n",
    "        return initial_state\n",
    "    \n",
    "    # LSTM에서 나온 logit값과 state값을 리턴한다.\n",
    "    def inference_step(self, sess, input_feed, state_feed):\n",
    "        \"\"\"\n",
    "        Runs one step of inference.\n",
    "        Args:\n",
    "            sess: TensorFlow Session object.\n",
    "            input_feed: A numpy array of shape [batch_size].\n",
    "            state_feed: A numpy array of shape [batch_size, state_size].\n",
    "            \n",
    "        Returns:\n",
    "            softmax_output: A numpy array of shape [batch_size, vocab_size].\n",
    "            new_state: A numpy array of shape [batch_size, state_size].\n",
    "            \n",
    "        \"\"\"\n",
    "        # input_feed값은 infernece일떄의 build_inputs() 함수에 있는\n",
    "        # input_feed = tf.placeholder(dtype=tf.int64, shape=[None],  name=\"input_feed\")\n",
    "        # state_feed값은 build_model() 함수에 있는\n",
    "        # tf.placeholder(dtype=tf.float32,shape=[None, sum(lstm_cell.state_size)],name=\"state_feed\")\n",
    "        # 결과로 tf.nn.softmax(logits, name=\"softmax\")와\n",
    "        # tf.concat(values=state_tuple, axis=1, name=\"state\")을 state_output으로 받는다.\n",
    "        softmax_output, state_output = sess.run(fetches=[\"softmax:0\", \"lstm/state:0\"],\n",
    "                                               feed_dict={\n",
    "                                                   \"input_feed:0\": input_feed,\n",
    "                                                   \"lstm/state_feed:0\" : state_feed,\n",
    "                                               })\n",
    "        return softmax_output, state_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class Caption(object):\n",
    "    \"\"\"Represents a complete or partial caption.\"\"\"\n",
    "    def __init__(self, sentence, state, logprob, score, metadata=None):\n",
    "        self.sentence = sentence\n",
    "        self.state = state\n",
    "        self.logprob = logprob\n",
    "        self.score = score\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __cmp__(self, other):\n",
    "        \"\"\"Compares Captions by score.\"\"\"\n",
    "        assert isinstance(other, Caption)\n",
    "        if self.score == other.score:\n",
    "            return 0\n",
    "        elif self.score < other.score:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    # For Python 3 compatibility (__cmp__ is deprecated).\n",
    "    def __lt__(self, other):\n",
    "        assert isinstance(other, Caption)\n",
    "        return self.score < other.score\n",
    "    \n",
    "    # Also for Python 3 compatibility.\n",
    "    def __eq__(self, other):\n",
    "        assert isinstance(other, Caption)\n",
    "        return self.score == other.score\n",
    "    \n",
    "class TopN(object):\n",
    "    \"\"\"\n",
    "    Maintains the top n elements of an incrementally provided set.\n",
    "    \n",
    "    \"\"\"\n",
    "    # n=beam_size\n",
    "    def __init__(self, n):\n",
    "        self._n = n\n",
    "        self._data = []\n",
    "        \n",
    "    def size(self):\n",
    "        assert self._data is not None\n",
    "        return len(self._data)\n",
    "    \n",
    "    def push(self, x):\n",
    "        \"\"\"Pushes a new element.\"\"\"\n",
    "        assert self._data is not None\n",
    "        if len(self._data) < self._n:\n",
    "            heapq.heappush(self._data, x)\n",
    "            \n",
    "        else:\n",
    "            heapq.heappushpop(self._data, x)\n",
    "            \n",
    "    def extract(self, sort=False):\n",
    "        \"\"\"\n",
    "        Extracts all elements from the TopN. This is a destructive operation.\n",
    "        \n",
    "        Args:\n",
    "            sort: Whether to return the elements in descending sorted order.\n",
    "        Returns:\n",
    "            A list of data; the top n elements provided to the set.\n",
    "        \"\"\"\n",
    "        assert self._data is not None\n",
    "        data = self._data\n",
    "        self._data = None\n",
    "        if sort:\n",
    "            data.sort(reverse=True)\n",
    "            \n",
    "        return data\n",
    "    def reset(self):\n",
    "        \"\"\"Returns the TopN to an empty state.\"\"\"\n",
    "        self._data = []\n",
    "        \n",
    "\n",
    "class CaptionGenerator(object):\n",
    "    \"\"\"\n",
    "    Class to generate captions from an image-to-text model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, beam_size=3, max_caption_length=20, \n",
    "                length_normalization_factor=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the generator.\n",
    "        Args:\n",
    "            model: Object encapsulating a trained image-to-text model. Must have\n",
    "            methods feed_image() and inference_step(). For example, an instance of\n",
    "            InferenceWrapperBase.\n",
    "            \n",
    "            vocab: A Vocabulary object.\n",
    "            beam_size: Beam size to use when generating captions.\n",
    "            max_caption_length: The maximum caption length before stopping the search.\n",
    "            length_normalization_factor: If != 0, a number x such that captions are\n",
    "            scored by logprob/length^x, rather than logprob. This changes the\n",
    "            relative scores of captions depending on their lengths. For example, if\n",
    "            x > 0 then longer captions will be favored.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        \n",
    "        self.beam_size = beam_size\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.length_normalization_factor = length_normalization_factor\n",
    "        \n",
    "    def beam_search(self, sess, encoded_image):\n",
    "        \"\"\"\n",
    "        Runs beam search caption generation on a single image.\n",
    "        \n",
    "        Args:\n",
    "            sess: TensorFlow Session object.\n",
    "            encoded_image: An encoded image string.\n",
    "        Returns:\n",
    "            A list of Caption sorted by descending score.\n",
    "        \"\"\"\n",
    "        # Feed in the image to get the initial state.\n",
    "        # 먼저 이미지를 넣어서 LSTM의 time step0을 돌릴 수 있게끔 initial state를 확보한다. \n",
    "        # -1st time step에 해당되는 과정\n",
    "        initial_state = self.model.feed_image(sess, encoded_image)\n",
    "        \n",
    "        initial_beam = Caption(sentence=[self.vocab.start_id],\n",
    "                               # 1024짜리로 h와 c state를 뭉쳐져 저장한다.\n",
    "                              state=initial_state[0],\n",
    "                              logprob=0.0,\n",
    "                              score=0.0,\n",
    "                              metadata=[\"\"])\n",
    "        # 3개만 고려하는 partial_captions\n",
    "        partial_captions = TopN(self.beam_size)\n",
    "        # initial caption을 hqueue에 \n",
    "        partial_captions.push(initial_beam)\n",
    "        complete_captions = TopN(self.beam_size)\n",
    "        \n",
    "        for _ in range(self.max_caption_length - 1):\n",
    "            # 부분 캡션리스트에 저장된 캡션들 beam_size만큼을 뽑아낸다.\n",
    "            partial_captions_list = partial_captions.extract()\n",
    "            partial_captions.reset()\n",
    "            # 부분 캡션 리스트에 있는 캡션들의 마지막 단어의 id값들을 numpy array로 만든다.\n",
    "            input_feed = np.array([c.sentence[-1] for c in partial_captions_list])\n",
    "            # 이전 단어를 만들고 난 후의 모델의 state값들을 numpy array로 만든다.\n",
    "            state_feed = np.array([c.state for c in partial_captions_list])\n",
    "            # LSTM cell에 이전 state와 마지막 문장 id값을 넣어서 \n",
    "            # 다음 state와 다음 단어에 대한 softmax값을 얻는다. \n",
    "            softmax, new_states, metadata = self.model.inference_step(sess,\n",
    "                                                                      input_feed,\n",
    "                                                                      state_feed)\n",
    "            # beam_size만큼 for문을 돌린다.\n",
    "            for i, partial_caption in enumerate(partial_captions_list):\n",
    "                word_probabilities = softmax[i]\n",
    "                state = new_states[i]\n",
    "                # For this partial caption, get the beam_size most probable next words.\n",
    "                words_and_probs = list(enumerate(word_probabilities))\n",
    "                words_and_probs.sort(key=lambda x: -x[1])\n",
    "                # beam_size 만큼만 뽑\n",
    "                words_and_probs = words_and_probs[0:self.beam_size]\n",
    "                # Each next word gives a new partial caption.\n",
    "                for w, p in words_and_probs:\n",
    "                    if p < 1e-12:\n",
    "                        continue  # Avoid log(0).\n",
    "                    sentence = partial_caption.sentence + [w]\n",
    "                    logprob = partial_caption.logprob + math.log(p)\n",
    "                    score = logprob\n",
    "                    if metadata:\n",
    "                        metadata_list = partial_caption.metadata + [metadata[i]]\n",
    "                        \n",
    "                    else:\n",
    "                        metadata_list = None\n",
    "                    \n",
    "                    # 만약 eos token을 만났을 경우\n",
    "                    if w == self.vocab.end_id:\n",
    "                        if self.length_normalization_factor > 0:\n",
    "                            score /= len(sentence)**self.length_normalization_factor\n",
    "                        beam = Caption(sentence, state, logprob, score, metadata_list)\n",
    "                        complete_captions.push(beam)\n",
    "                    else:\n",
    "                        beam = Caption(sentence, state, logprob, score, metadata_list)\n",
    "                        partial_captions.push(beam)\n",
    "            if partial_captions.size() == 0:\n",
    "                # We have run out of partial candidates; happens when beam_size = 1.\n",
    "                break\n",
    "                \n",
    "            # If we have no complete captions then fall back to the partial captions.\n",
    "            # But never output a mixture of complete and partial captions because a\n",
    "            # partial caption could have a higher score than all the complete captions.\n",
    "            if not complete_captions.size():\n",
    "                complete_captions = partial_captions\n",
    "                \n",
    "            # 완전한 캡션 3개를 내놓는다.\n",
    "            return complete_captions.extract(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 끝난 후 저장된 체크포인트 파일이 있는 위치\n",
    "checkpoint_path = \"\"\n",
    "# vocab이 담긴 text파일\n",
    "vocab_file = \"E:/mscoco/outputs/word_counts.txt\"\n",
    "# 이미지 파일\n",
    "input_file = \"E:/mscoco/test.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용법\n",
    "\n",
    "- 1. build_graph_from_config()을 써서 model inference graph를 만든다.\n",
    "- 2. 결과로 나오는 restore_fn을 불러서 모델의 체크포인트를 로드한다.\n",
    "- 3. 배치에 있는 각 이미지에 대해\n",
    "    - feed_image()를 불러서 initial_state를 얻는다.\n",
    "    - 캡션 생성의 각 단계마다 inference_step()을 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    model = InferenceWrapper()\n",
    "    restore_fn = model.build_graph_from_config(model_config, checkpoint_path)\n",
    "g.finalize()\n",
    "\n",
    "# vocab을 담당하는 객체를 만든다.\n",
    "vocab = Vocabulary(vocab_file=vocab_file)\n",
    "\n",
    "# 각 이미지의 위치를 저장하는 리스트\n",
    "filenames=[]\n",
    "# 이미지 인풋 파일이 여러개 일 경우 ,을 기준으로 나눈다.\n",
    "for file_pattern in FLAGS.input_files.split(\",\"):\n",
    "    # Returns a list of files that match the given pattern(s)\n",
    "    filenames.extend(tf.gfile.Glob(file_pattern))\n",
    "    \n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    \n",
    "    # 체크포인트 폴더에 있는 파일을 현재 sess으로 복구한다.\n",
    "    restore_fn(sess)\n",
    "    \n",
    "    # beam search를 사용하는 caption generator\n",
    "    generator = caption_generator.CaptionGenerator(model, vocab)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # 이미지 파일을 연다\n",
    "        with tf.gfile.GFile(name=filename, \"rb\") as f:\n",
    "            image = f.read()\n",
    "        # 캡션이 생성된다.\n",
    "        captions = generator.beam_search(sess, image)\n",
    "        print(\"Captions for image %s:\" % os.path.basename(filename))\\\n",
    "        # 3개의 캡션에 대해서 \n",
    "        for i, caption in enumerate(captions):\n",
    "            # 캡션의 제일 첫번째와 마지막 token은 무시한다. id->word로 바꾼 리스트\n",
    "            sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]\n",
    "            # str 리스트를 연결한다.\n",
    "            sentence = \" \".join(sentence)\n",
    "            print(\"  %d) %s (p=%f)\" % (i, sentence, math.exp(caption.logprob)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
