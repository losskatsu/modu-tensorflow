{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer leaning using Inception-v3 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"$HOME/models/research/slim/\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/flowers/flowers_train_00003-of-00005.tfrecord', '../data/flowers/flowers_train_00002-of-00005.tfrecord', '../data/flowers/flowers_train_00001-of-00005.tfrecord', '../data/flowers/flowers_train_00004-of-00005.tfrecord', '../data/flowers/flowers_train_00000-of-00005.tfrecord']\n",
      "['../data/flowers/flowers_validation_00004-of-00005.tfrecord', '../data/flowers/flowers_validation_00001-of-00005.tfrecord', '../data/flowers/flowers_validation_00000-of-00005.tfrecord', '../data/flowers/flowers_validation_00003-of-00005.tfrecord', '../data/flowers/flowers_validation_00002-of-00005.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/flowers/'\n",
    "tfrecord_filenames = os.listdir(data_path)\n",
    "train_data_filenames = [os.path.join(data_path, name) for name in tfrecord_filenames if 'train' in name]\n",
    "validation_data_filenames = [os.path.join(data_path, name) for name in tfrecord_filenames if 'validation' in name]\n",
    "print(train_data_filenames)\n",
    "print(validation_data_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a scalar string `example_proto` into a pair of a scalar string and\n",
    "# a scalar integer, representing an image and its label, respectively.\n",
    "def _parse_function(example_proto):\n",
    "  features = {'image/encoded': tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "              'image/format': tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "              'image/class/label': tf.FixedLenFeature((), tf.int64, default_value=0),\n",
    "              'image/height': tf.FixedLenFeature((), tf.int64, default_value=0),\n",
    "              'image/width': tf.FixedLenFeature((), tf.int64, default_value=0)}\n",
    "  parsed_features = tf.parse_single_example(example_proto, features)\n",
    "  image = tf.image.decode_jpeg(parsed_features[\"image/encoded\"], channels=3)\n",
    "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "  label = tf.cast(parsed_features[\"image/class/label\"], dtype=tf.int32)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocessing(image, label):\n",
    "  \"\"\"data augmentation function for training\n",
    "  augmentation method is borrowed by inception code\n",
    "  \n",
    "  Args:\n",
    "    image (3-rank Tensor): [?, ?, 3] for flower data\n",
    "    label (0-rank Tensor): scalar value of corresponding image\n",
    "    \n",
    "  Returns:\n",
    "    image (3-rank Tensor): [299, 299, 3] image transformed\n",
    "    label (0-rank Tensor): scalar value of corresponding image\n",
    "  \"\"\"\n",
    "  image = tf.image.resize_image_with_crop_or_pad(image, 299, 299)\n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "  image = tf.image.random_brightness(image, max_delta=32./255.)\n",
    "  image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "  image = tf.image.random_hue(image, max_delta=0.2)\n",
    "  image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "  image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "  # Finally, rescale to [-1, 1] instead of [0, 1)\n",
    "  image = tf.subtract(image, 0.5)\n",
    "  image = tf.multiply(image, 2.0)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _central_crop(image, label):\n",
    "  \"\"\"data augmentation function for training\n",
    "  augmentation method is borrowed by inception code\n",
    "  \n",
    "  Args:\n",
    "    image (3-rank Tensor): [?, ?, 3] for flower data\n",
    "    label (0-rank Tensor): scalar value of corresponding image\n",
    "    \n",
    "  Returns:\n",
    "    image (3-rank Tensor): [299, 299, 3] image transformed\n",
    "    label (0-rank Tensor): scalar value of corresponding image\n",
    "  \"\"\"\n",
    "  image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "  image = tf.image.resize_images(image, [299, 299])\n",
    "  # Finally, rescale to [-1, 1] instead of [0, 1)\n",
    "  image = tf.subtract(image, 0.5)\n",
    "  image = tf.multiply(image, 2.0)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 299, 299, 3), (?,)), types: (tf.float32, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 299, 299, 3), (?,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.TFRecordDataset(train_data_filenames)\n",
    "train_dataset = train_dataset.map(_parse_function)\n",
    "train_dataset = train_dataset.map(_preprocessing)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for validation\n",
    "validation_dataset = tf.data.TFRecordDataset(validation_data_filenames)\n",
    "validation_dataset = validation_dataset.map(_parse_function)\n",
    "validation_dataset = validation_dataset.map(_central_crop)\n",
    "validation_dataset = validation_dataset.shuffle(buffer_size = 10000)\n",
    "validation_dataset = validation_dataset.batch(batch_size = batch_size)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Iterator.from_string_handle의 output_shapes는 default = None이지만 꼭 값을 넣는 게 좋음\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle,\n",
    "                                               train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "inputs, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Inception-v3 graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool)\n",
    "with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\n",
    "  logits, _ = inception_v3.inception_v3(inputs, num_classes=5, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"InceptionV3/Logits/SpatialSqueeze:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_one_hot = tf.one_hot(y, depth=10)\n",
    "#cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_one_hot, logits=y_pred)\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "l2_regualrization_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "with tf.name_scope('total_loss'):\n",
    "  total_loss = cross_entropy + l2_regualrization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization update\n",
    "batchnorm_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# Add dependency to compute batchnorm_updates.\n",
    "with tf.control_dependencies(batchnorm_update_ops):\n",
    "  train_step = tf.train.RMSPropOptimizer(1e-4).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter(\"./graphs/code14_transfer_learning\", sess.graph)\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Inception-v3 checkpoint: \n",
    "\n",
    "```\n",
    "$ CHECKPOINT_DIR='checkpoints'\n",
    "$ mkdir ${CHECKPOINT_DIR}\n",
    "$ wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n",
    "$ tar -xvf inception_v3_2016_08_28.tar.gz\n",
    "$ mv inception_v3_2016_08_28.tar.gz ${CHECKPOINT_DIR}\n",
    "$ rm inception_v3_2016_08_28.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Inception_v3 weights using `tf.saver.restore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../checkpoints/inception_v3.ckpt\n",
      "epochs: 0, step: 0, loss: 1.83767, (0.34 examples/sec; 92.832 sec/batch)\n",
      "epochs: 0, step: 1, loss: 1.88018, (0.92 examples/sec; 34.608 sec/batch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a229c30709b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m       _, loss = sess.run([train_step, total_loss],\n\u001b[1;32m     22\u001b[0m                          feed_dict={handle: train_handle,\n\u001b[0;32m---> 23\u001b[0;31m                                     is_training: True})\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables()[:-8])\n",
    "\n",
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# use saver object to load variables from the saved model\n",
    "saver.restore(sess, \"../checkpoints/inception_v3.ckpt\")\n",
    "\n",
    "# train_iterator\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "train_handle = sess.run(train_iterator.string_handle())\n",
    "\n",
    "# Train\n",
    "max_epochs = 1\n",
    "step = 0\n",
    "for epochs in range(max_epochs):\n",
    "  sess.run(train_iterator.initializer)\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      start_time = time.time()\n",
    "      _, loss = sess.run([train_step, total_loss],\n",
    "                         feed_dict={handle: train_handle,\n",
    "                                    is_training: True})\n",
    "      if step % 1 == 0:\n",
    "        duration = time.time() - start_time\n",
    "        examples_per_sec = batch_size / float(duration)\n",
    "        print(\"epochs: {}, step: {}, loss: {:g}, ({:.2f} examples/sec; {:.3f} sec/batch)\".format(epochs, step, loss, examples_per_sec, duration))\n",
    "        \n",
    "      #if step % 200 == 0:\n",
    "      #  # summary\n",
    "      #  summary_str = sess.run(summary_op, feed_dict={handle: train_handle, is_training: False})\n",
    "      #  train_writer.add_summary(summary_str, global_step=step)\n",
    "        \n",
    "      step += 1\n",
    "      #if step > 100:\n",
    "      #  break\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "\n",
    "train_writer.close()\n",
    "print(\"training done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
