{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://arxiv.org/abs/1409.2329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import reader\n",
    "\n",
    "import util\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC = \"basic\"\n",
    "CUDNN = \"cudnn\"\n",
    "BLOCK = \"block\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 8\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumConfig(object):\n",
    "    \"\"\"Medium config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 6\n",
    "    max_max_epoch = 39\n",
    "    keep_prob = 0.5\n",
    "    lr_decay = 0.8\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeConfig(object):\n",
    "    \"\"\"Large config.\"\"\"\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1 / 1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestConfig(object):\n",
    "    \"\"\"Tiny config, for testing.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 2\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 1\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    \"\"\"The input data\"\"\"\n",
    "    \n",
    "    def __init__(self, config, data, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data)//batch_size)-1) // num_steps\n",
    "        self.input_data, self.targets = reader.ptb_producer(data, batch_size,\n",
    "                                                           num_steps, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    \n",
    "    def __init__(self, is_training, config, input_):\n",
    "        self.is_training = is_training\n",
    "        self._input = input_\n",
    "        self._rnn_params = None\n",
    "        self._cell = None\n",
    "        self.batch_size = input_.batch_size\n",
    "        self.num_steps = input_.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        # 특정 작업에 특정 디바이스를 배치\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # embedding matrix를 variable로 만들 수 있다.\n",
    "            embedding = tf.get_variable(name=\"embedding\", shape=[vocab_size, size], dtype=tf.float32)\n",
    "            # embedding layer\n",
    "            # inputs : [batch_size, n_step, hidden_size(size)]\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "            \n",
    "        # 만약 training이고 dropout의 1이하라면\n",
    "        # input단에 적용한다.\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "            \n",
    "        # 여기서 RNN을 만들어서 그래프 위에 올린다\n",
    "        # ouput : [batch*n_step, hidden_size]\n",
    "        # states h,c : [batch, hidden_size]\n",
    "        self.output, self.state = self._build_rnn_graph(inputs, config, is_training)\n",
    "        \n",
    "        # logits : [batch, n_step, vocab_size]를 만들기 위해 matmul(dense layer)로 보낸다.\n",
    "        # 이 logits이 sequence loss로 들어간다.\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        # x : 2D tensor, w : 2D tensor\n",
    "        logits = tf.nn.xw_plus_b(self.output, softmax_w, softmax_b)\n",
    "        logits = tf.reshape(logits, [self.batch_size,self.num_steps,vocab_size])\n",
    "        \n",
    "        # average_across_batch가 True이면 batch dimension을 기준으로 sum을 하고 batch_size로 나눈다.\n",
    "        # average_across_timesteps가 True이면 sequence dimension을 기준으로 sum하고 \n",
    "        # timestep에 대한 total label weight로 나누게 된다.\n",
    "        # targets : [batch_size , n_steps]\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits,\n",
    "                                                input_.targets,\n",
    "                                               tf.ones([self.batch_size,self.num_steps], dtype=tf.float32),\n",
    "                                               average_across_timesteps=False,\n",
    "                                               average_across_batch=True)\n",
    "        # 1-dimension짜리 cost로 만들어준다.\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = self.state\n",
    "        \n",
    "        # test라면 모델까지만 그래프에 올린다.\n",
    "        if not is_training:\n",
    "            return\n",
    "        \n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # 그래프에 올라온 trainable한 변수를 불러온다.\n",
    "        tvars = tf.trainable_variables()\n",
    "        # gradient matrix의 norm을 조정한다\n",
    "        grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(self._cost, tvars), clip_norm=config.max_grad_norm)\n",
    "                                      \n",
    "        # optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        # learning rate decay\n",
    "        self._new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        \n",
    "    def _build_rnn_graph(self, inputs, config, is_training):\n",
    "    # CUDNN 모드에 따라서 다르게 동작한다. \n",
    "    # CUDNN 버전 LSTM이 존재한다.\n",
    "        if config.rnn_mode == CUDNN:\n",
    "            return self._build_rnn_graph_cudnn(inputs, config, is_training)\n",
    "        else:\n",
    "            return self._build_rnn_graph_lstm(inputs, config, is_training)\n",
    "    \n",
    "    def _build_rnn_graph_lstm(self, inputs, config, is_training):\n",
    "        # forget gate biase를 1로 주는게 더 좋은 결과가 나오지만 원본 논문에선 그걸 고려안해서\n",
    "        # 원본 논문의 hyper parameter를 쓰기 위해 1을 주지 않는다.\n",
    "        def make_cell():\n",
    "            cell = self._get_lstm_cell(config, is_training)\n",
    "            # 만약에 training이고 keep_prob값이 1 이하라면 output connection에\n",
    "            # Dropout을 걸어준다.\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=config.keep_prob)\n",
    "                \n",
    "            return cell\n",
    "        # deep layered LSTM을 구성한다.\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "        \n",
    "        # [batch_size x hidden_size]\n",
    "        self._initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        state = self._initial_state\n",
    "        \n",
    "        \n",
    "        self.outputs = []\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(self.num_steps):\n",
    "                # time_step이 1인 이후부터는 cell의 wieght를 다시 이용\n",
    "                # inputs : [batch_size, n_step, hidden_size(size)]\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                self.outputs.append(cell_output)\n",
    "                \n",
    "            # outputs : [ n_step, batch, hidden]\n",
    "            # tf.concat(self.outputs, 1) [n_steps, hidden*batch]\n",
    "            # self.concat = tf.concat(self.outputs, 1)\n",
    "            output = tf.reshape(tf.concat(self.outputs, 1), [-1, config.hidden_size])\n",
    "            # output [batch_size*n_step, hidden_size]\n",
    "            return output, state\n",
    "    \n",
    "    # CUDNN모드의 LSTM이 아니면 basic버전이나 block버전의 cell을 만든다. \n",
    "    def _get_lstm_cell(self, config, is_training):\n",
    "        if config.rnn_mode == BASIC:\n",
    "            return tf.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0, reuse=not is_training)\n",
    "        if config.rnn_mode == BLOCK:\n",
    "            return tf.contrib.rnn.LSTMBlockCell(config.hidden_size, forget_bias=0.0)\n",
    "        raise ValueError(\"rnn_mode %s not supported\" % config.rnn_mode)\n",
    "        \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "            \n",
    "     \n",
    "    # getters\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def initial_state_name(self):\n",
    "        return self._initial_state_name\n",
    "\n",
    "    @property\n",
    "    def final_state_name(self):\n",
    "        return self._final_state_name\n",
    "    \n",
    "    # Parallel Processing을 하지 않으면 필요없다.\n",
    "    def export_ops(self, name):\n",
    "        \"\"\"Exports ops to collections.\"\"\"\n",
    "        # name으로 오는 값이 \"Train\", \"Valid\", \"Test\"이다.\n",
    "        # 각각 Train, Valid, Test마다 ops의 collection을 따로 주려고 한다.\n",
    "        # self._name/cost : self._cost의 dictinary를 만든다.\n",
    "        self._name = name\n",
    "        ops = {util.with_prefix(self._name, \"cost\"): self._cost}\n",
    "        # training mode라면 learning rate관련 ops도 dict에 추가한다.\n",
    "        if self._is_training:\n",
    "            ops.update(lr=self._lr, new_lr=self._new_lr, lr_update=self._lr_update)\n",
    "            # cudnn 모드용 lstm cell을 만들어서 이에 관련된 ops가 있다면\n",
    "            # 이것도 추가한다.\n",
    "            if self._rnn_params:\n",
    "                ops.update(rnn_params=self._rnn_params)\n",
    "        \n",
    "        for name, op in ops.items():\n",
    "            # 모아놓은 op에 name이라는 collection으로 등록한다.\n",
    "            tf.add_to_collection(name, op)\n",
    "                \n",
    "        #lstm의 initial state와 final state도 collection을 등록한다.\n",
    "        # self._name/initial\n",
    "        self._initial_state_name = util.with_prefix(self._name, \"initial\")\n",
    "        # self._name/final\n",
    "        self._final_state_name = util.with_prefix(self._name, \"final\")\n",
    "        # lstm의 initial c, h sate \n",
    "        util.export_state_tuples(self._initial_state, self._initial_state_name)\n",
    "        # lstm의 final c, h sate\n",
    "        util.export_state_tuples(self._final_state, self._final_state_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "m._input.targets\n",
    "<tf.Tensor 'Train/TrainInput/StridedSlice_1:0' shape=(20, 20) dtype=int32>\n",
    "\n",
    "\n",
    "m.initial_state\n",
    "\n",
    "(LSTMStateTuple(c=<tf.Tensor 'Train/Model/MultiRNNCellZeroState/LSTMBlockCellZeroState/zeros:0' shape=(20, 200) dtype=float32>, h=<tf.Tensor 'Train/Model/MultiRNNCellZeroState/LSTMBlockCellZeroState/zeros_1:0' shape=(20, 200) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'Train/Model/MultiRNNCellZeroState/LSTMBlockCellZeroState_1/zeros:0' shape=(20, 200) dtype=float32>, h=<tf.Tensor 'Train/Model/MultiRNNCellZeroState/LSTMBlockCellZeroState_1/zeros_1:0' shape=(20, 200) dtype=float32>))\n",
    "\n",
    "\n",
    "m._cost\n",
    "Tensor(\"Train/Model/Sum:0\", shape=(), dtype=float32)\n",
    "\n",
    "\n",
    "m.output\n",
    "<tf.Tensor 'Train/Model/RNN/Reshape:0' shape=(400, 200) dtype=float32>\n",
    "\n",
    "m.state\n",
    "(LSTMStateTuple(c=<tf.Tensor 'Train/Model/RNN/RNN/multi_rnn_cell/cell_0/lstm_cell/LSTMBlockCell_19:1' shape=(20, 200) dtype=float32>, h=<tf.Tensor 'Train/Model/RNN/RNN/multi_rnn_cell/cell_0/lstm_cell/LSTMBlockCell_19:6' shape=(20, 200) dtype=float32>),\n",
    " LSTMStateTuple(c=<tf.Tensor 'Train/Model/RNN/RNN/multi_rnn_cell/cell_1/lstm_cell/LSTMBlockCell_19:1' shape=(20, 200) dtype=float32>, h=<tf.Tensor 'Train/Model/RNN/RNN/multi_rnn_cell/cell_1/lstm_cell/LSTMBlockCell_19:6' shape=(20, 200) dtype=float32>))\n",
    "\n",
    "\n",
    "m.concat\n",
    "<tf.Tensor 'Train/Model/RNN/concat:0' shape=(20, 4000) dtype=float32>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"epoch하나를 실행시켜서 model을 돌리는 함수\n",
    "    출력값으로 perplexity를 출력한다.\"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    # initial state를 0으로 초기화시킨다.\n",
    "    state = session.run(model.initial_state)\n",
    "    \n",
    "    fetches = {\n",
    "      \"cost\": model.cost,\n",
    "      \"final_state\": model.final_state,}\n",
    "    \n",
    "    # Train 모델만 m.train_op를 eval_op로 넘겨준다.\n",
    "    # Train일 경우에 eval_op를 fetch dict에 추가한다\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "        \n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        # lstm의 initial state\n",
    "        for i, (c,h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        # model.cost, model_final_state, m.train_op를 돌린다.\n",
    "        # fetches가 dict이기 때문에 vals은 dict로 나온다.\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "        \n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "        \n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                  (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),\n",
    "                   iters * model.input.batch_size/\n",
    "                   (time.time() - start_time)))\n",
    "            \n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929589,)\n"
     ]
    }
   ],
   "source": [
    "raw_data = reader.ptb_raw_data('D:\\PythonLab\\CS20\\RNN')\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "print(np.shape(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SmallConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config = SmallConfig()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\tensorflow_1_7\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "    \n",
    "# Train용 모델을 만든다.\n",
    "with tf.name_scope(\"Train\"):\n",
    "    # train_input.input_data : [batch_size, n_step] = [20,20]\n",
    "    # train_input.targets : [batch_size, n_step] = [20,20]\n",
    "    # train_input.epoch_size : 2323\n",
    "    train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "    # variable_scope로 묶어서 initializer를 한꺼번에 적용하자\n",
    "    with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "    tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "    tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation용 모델을 만든다\n",
    "with tf.name_scope('Valid'):\n",
    "    # valid_input.input_data : [batch_size, n_step] = [20,20]\n",
    "    # valid_input.targets : [batch_size, n_step] = [20,20]\n",
    "    # valid_input.epoch_size : 184\n",
    "    valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "    # 앞에서 만든 Train model을 재활용한다.\n",
    "    with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "    tf.summary.scalar(\"Validation_Loss\", mvalid.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test용 모델을 만든다\n",
    "with tf.name_scope(\"Test\"):\n",
    "    test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "    with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mtest = PTBModel(is_training=False, config=eval_config,input_=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\"Train\": m, \"Valid\": mvalid, \"Test\": mtest}\n",
    "# for name, model in models.items():\n",
    "#     # learning_rate ops, final, initaial state, cost를 collection에 등록한다.\n",
    "#     model.export_ops(name)\n",
    "# metagraph = tf.train.export_meta_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'D:\\PythonLab\\CS20\\RNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-e4bee99ee4fe>:2: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Restoring parameters from D:\\PythonLab\\CS20\\RNN\\model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path D:\\PythonLab\\CS20\\RNN\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "0.004 perplexity: 5386.678 speed: 6207 wps\n",
      "0.104 perplexity: 836.998 speed: 10822 wps\n",
      "0.204 perplexity: 623.433 speed: 11250 wps\n",
      "0.304 perplexity: 504.161 speed: 11380 wps\n",
      "0.404 perplexity: 435.992 speed: 11526 wps\n",
      "0.504 perplexity: 390.409 speed: 11469 wps\n",
      "0.604 perplexity: 351.864 speed: 11509 wps\n",
      "0.703 perplexity: 325.033 speed: 11474 wps\n",
      "0.803 perplexity: 303.831 speed: 11509 wps\n",
      "0.903 perplexity: 284.428 speed: 11517 wps\n",
      "Epoch: 1 Train Perplexity: 270.040\n",
      "Epoch: 1 Valid Perplexity: 182.787\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "0.004 perplexity: 200.202 speed: 12060 wps\n",
      "0.104 perplexity: 151.237 speed: 11445 wps\n",
      "0.204 perplexity: 158.717 speed: 11688 wps\n",
      "0.304 perplexity: 153.474 speed: 11658 wps\n",
      "0.404 perplexity: 150.523 speed: 11667 wps\n",
      "INFO:tensorflow:Recording summary at step 3427.\n",
      "INFO:tensorflow:Model/global_step/sec: 28.6431\n",
      "0.504 perplexity: 147.831 speed: 11637 wps\n",
      "0.604 perplexity: 143.250 speed: 11547 wps\n",
      "0.703 perplexity: 141.094 speed: 11499 wps\n",
      "0.803 perplexity: 139.012 speed: 11514 wps\n",
      "0.903 perplexity: 135.358 speed: 11460 wps\n",
      "Epoch: 2 Train Perplexity: 133.354\n",
      "Epoch: 2 Valid Perplexity: 144.951\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "0.004 perplexity: 138.989 speed: 10212 wps\n",
      "0.104 perplexity: 105.067 speed: 11419 wps\n",
      "0.204 perplexity: 114.489 speed: 11380 wps\n",
      "0.304 perplexity: 111.567 speed: 11289 wps\n",
      "0.404 perplexity: 110.663 speed: 11389 wps\n",
      "0.504 perplexity: 109.690 speed: 11463 wps\n",
      "0.604 perplexity: 106.999 speed: 11502 wps\n",
      "0.703 perplexity: 106.356 speed: 11541 wps\n",
      "0.803 perplexity: 105.651 speed: 11570 wps\n",
      "0.903 perplexity: 103.424 speed: 11588 wps\n",
      "INFO:tensorflow:Recording summary at step 6855.\n",
      "INFO:tensorflow:Model/global_step/sec: 28.5651\n",
      "Epoch: 3 Train Perplexity: 102.569\n",
      "Epoch: 3 Valid Perplexity: 132.924\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "0.004 perplexity: 108.045 speed: 11068 wps\n",
      "0.104 perplexity: 85.272 speed: 11703 wps\n",
      "0.204 perplexity: 93.774 speed: 11692 wps\n",
      "0.304 perplexity: 91.540 speed: 11716 wps\n",
      "0.404 perplexity: 91.196 speed: 11703 wps\n",
      "0.504 perplexity: 90.657 speed: 11723 wps\n",
      "0.604 perplexity: 88.873 speed: 11721 wps\n",
      "0.703 perplexity: 88.701 speed: 11722 wps\n",
      "0.803 perplexity: 88.383 speed: 11711 wps\n",
      "0.903 perplexity: 86.776 speed: 11569 wps\n",
      "Epoch: 4 Train Perplexity: 86.285\n",
      "Epoch: 4 Valid Perplexity: 127.571\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "0.004 perplexity: 91.313 speed: 9696 wps\n",
      "0.104 perplexity: 71.545 speed: 10292 wps\n",
      "0.204 perplexity: 77.666 speed: 10118 wps\n",
      "0.304 perplexity: 74.799 speed: 10060 wps\n",
      "INFO:tensorflow:Recording summary at step 10149.\n",
      "INFO:tensorflow:Model/global_step/sec: 27.4529\n",
      "0.404 perplexity: 73.753 speed: 10303 wps\n",
      "0.504 perplexity: 72.586 speed: 10526 wps\n",
      "0.604 perplexity: 70.493 speed: 10583 wps\n",
      "0.703 perplexity: 69.730 speed: 10702 wps\n",
      "0.803 perplexity: 68.821 speed: 10765 wps\n",
      "0.903 perplexity: 66.965 speed: 10695 wps\n",
      "Epoch: 5 Train Perplexity: 66.024\n",
      "Epoch: 5 Valid Perplexity: 119.672\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "0.004 perplexity: 75.498 speed: 11356 wps\n",
      "0.104 perplexity: 58.892 speed: 10212 wps\n",
      "0.204 perplexity: 64.331 speed: 10260 wps\n",
      "0.304 perplexity: 61.764 speed: 10219 wps\n",
      "0.404 perplexity: 60.867 speed: 10228 wps\n",
      "0.504 perplexity: 59.791 speed: 10246 wps\n",
      "0.604 perplexity: 57.959 speed: 10320 wps\n",
      "0.703 perplexity: 57.213 speed: 10419 wps\n",
      "INFO:tensorflow:Recording summary at step 13331.\n",
      "INFO:tensorflow:Model/global_step/sec: 26.5141\n",
      "0.803 perplexity: 56.329 speed: 10548 wps\n",
      "0.903 perplexity: 54.647 speed: 10482 wps\n",
      "Epoch: 6 Train Perplexity: 53.727\n",
      "Epoch: 6 Valid Perplexity: 119.169\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "0.004 perplexity: 67.928 speed: 10594 wps\n",
      "0.104 perplexity: 52.231 speed: 10837 wps\n",
      "0.204 perplexity: 57.293 speed: 10824 wps\n",
      "0.304 perplexity: 54.861 speed: 10743 wps\n",
      "0.404 perplexity: 54.091 speed: 10808 wps\n",
      "0.504 perplexity: 53.037 speed: 10959 wps\n",
      "0.604 perplexity: 51.388 speed: 10981 wps\n",
      "0.703 perplexity: 50.672 speed: 11038 wps\n",
      "0.803 perplexity: 49.824 speed: 11049 wps\n",
      "0.903 perplexity: 48.263 speed: 11085 wps\n",
      "Epoch: 7 Train Perplexity: 47.381\n",
      "Epoch: 7 Valid Perplexity: 119.906\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "0.004 perplexity: 63.684 speed: 10504 wps\n",
      "0.104 perplexity: 48.936 speed: 10880 wps\n",
      "INFO:tensorflow:Saving checkpoint to path D:\\PythonLab\\CS20\\RNN\\model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 16573.\n",
      "INFO:tensorflow:Model/global_step/sec: 27.0031\n",
      "0.204 perplexity: 53.712 speed: 11112 wps\n",
      "0.304 perplexity: 51.378 speed: 11269 wps\n",
      "0.404 perplexity: 50.650 speed: 11336 wps\n",
      "0.504 perplexity: 49.625 speed: 11330 wps\n",
      "0.604 perplexity: 48.069 speed: 11417 wps\n",
      "0.703 perplexity: 47.369 speed: 11286 wps\n",
      "0.803 perplexity: 46.521 speed: 11270 wps\n",
      "0.903 perplexity: 45.028 speed: 11200 wps\n",
      "Epoch: 8 Train Perplexity: 44.177\n",
      "Epoch: 8 Valid Perplexity: 120.693\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "0.004 perplexity: 63.541 speed: 11702 wps\n",
      "0.104 perplexity: 47.204 speed: 11353 wps\n",
      "0.204 perplexity: 51.881 speed: 11150 wps\n",
      "0.304 perplexity: 49.610 speed: 10990 wps\n",
      "0.404 perplexity: 48.890 speed: 11046 wps\n",
      "0.504 perplexity: 47.884 speed: 10992 wps\n",
      "INFO:tensorflow:Recording summary at step 19916.\n",
      "INFO:tensorflow:Model/global_step/sec: 27.8718\n",
      "0.604 perplexity: 46.371 speed: 11130 wps\n",
      "0.703 perplexity: 45.676 speed: 11152 wps\n",
      "0.803 perplexity: 44.821 speed: 11122 wps\n",
      "0.903 perplexity: 43.362 speed: 11099 wps\n",
      "Epoch: 9 Train Perplexity: 42.537\n",
      "Epoch: 9 Valid Perplexity: 121.069\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "0.004 perplexity: 63.078 speed: 10656 wps\n",
      "0.104 perplexity: 46.140 speed: 11330 wps\n",
      "0.204 perplexity: 50.820 speed: 10879 wps\n",
      "0.304 perplexity: 48.581 speed: 11180 wps\n",
      "0.404 perplexity: 47.922 speed: 11283 wps\n",
      "0.504 perplexity: 46.898 speed: 11394 wps\n",
      "0.604 perplexity: 45.415 speed: 11441 wps\n",
      "0.703 perplexity: 44.728 speed: 11505 wps\n",
      "0.803 perplexity: 43.878 speed: 11386 wps\n",
      "0.903 perplexity: 42.438 speed: 11372 wps\n",
      "Epoch: 10 Train Perplexity: 41.622\n",
      "Epoch: 10 Valid Perplexity: 120.942\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "0.004 perplexity: 62.137 speed: 11534 wps\n",
      "INFO:tensorflow:Recording summary at step 23265.\n",
      "INFO:tensorflow:Model/global_step/sec: 27.9109\n",
      "0.104 perplexity: 45.617 speed: 11850 wps\n",
      "0.204 perplexity: 50.242 speed: 11692 wps\n",
      "0.304 perplexity: 48.009 speed: 11775 wps\n",
      "0.404 perplexity: 47.376 speed: 11728 wps\n",
      "0.504 perplexity: 46.344 speed: 11772 wps\n",
      "0.604 perplexity: 44.894 speed: 11757 wps\n",
      "0.703 perplexity: 44.199 speed: 11788 wps\n",
      "0.803 perplexity: 43.348 speed: 11768 wps\n",
      "0.903 perplexity: 41.921 speed: 11771 wps\n",
      "Epoch: 11 Train Perplexity: 41.129\n",
      "Epoch: 11 Valid Perplexity: 120.711\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "0.004 perplexity: 61.525 speed: 10773 wps\n",
      "0.104 perplexity: 45.160 speed: 11568 wps\n",
      "0.204 perplexity: 49.821 speed: 11567 wps\n",
      "0.304 perplexity: 47.642 speed: 11584 wps\n",
      "0.404 perplexity: 47.036 speed: 11472 wps\n",
      "0.504 perplexity: 46.019 speed: 11469 wps\n",
      "INFO:tensorflow:Recording summary at step 26733.\n",
      "INFO:tensorflow:Model/global_step/sec: 28.9048\n",
      "0.604 perplexity: 44.593 speed: 11484 wps\n",
      "0.703 perplexity: 43.890 speed: 11478 wps\n",
      "0.803 perplexity: 43.035 speed: 11472 wps\n",
      "0.903 perplexity: 41.615 speed: 11490 wps\n",
      "Epoch: 12 Train Perplexity: 40.848\n",
      "Epoch: 12 Valid Perplexity: 120.386\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "0.004 perplexity: 60.813 speed: 11671 wps\n",
      "0.104 perplexity: 44.964 speed: 11039 wps\n",
      "0.204 perplexity: 49.627 speed: 10468 wps\n",
      "0.304 perplexity: 47.424 speed: 10453 wps\n",
      "0.404 perplexity: 46.851 speed: 10662 wps\n",
      "0.504 perplexity: 45.840 speed: 10816 wps\n",
      "0.604 perplexity: 44.427 speed: 10898 wps\n",
      "0.703 perplexity: 43.727 speed: 10996 wps\n",
      "0.803 perplexity: 42.876 speed: 11064 wps\n",
      "0.903 perplexity: 41.461 speed: 10911 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recording summary at step 30026.\n",
      "INFO:tensorflow:Model/global_step/sec: 27.4074\n",
      "Epoch: 13 Train Perplexity: 40.711\n",
      "Epoch: 13 Valid Perplexity: 120.142\n",
      "INFO:tensorflow:Recording summary at step 30199.\n",
      "INFO:tensorflow:Saving checkpoint to path D:\\PythonLab\\CS20\\RNN\\model.ckpt\n",
      "Test Perplexity: 114.624\n",
      "Saving model to D:\\PythonLab\\CS20\\RNN.\n"
     ]
    }
   ],
   "source": [
    "# High-level Monitored Session\n",
    "sv = tf.train.Supervisor(logdir=save_path)\n",
    "#sv = tf.train.MonitoredTrainingSession(checkpoint_dir='D:\\PythonLab\\CS20\\RNN')\n",
    "config_proto = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "with sv.managed_session(config=config_proto) as session:\n",
    "    for i in range(config.max_max_epoch):\n",
    "        # 왜 max_epoch과 max_max_epoch이 다를까?\n",
    "        # 논문에서 제시된 epoch당 줄이는 factor값을 맞추기 위해서\n",
    "        lr_decay = config.lr_decay ** max(i+1-config.max_epoch, 0.0)\n",
    "        # Tranin Model의 learning rate decay하는 operation 실행\n",
    "        m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "        \n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        # Train\n",
    "        train_perplexity = run_epoch(session, m, eval_op=m.train_op,verbose=True)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Valid\n",
    "        valid_perplexity = run_epoch(session, mvalid)\n",
    "        print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "        \n",
    "    # Test\n",
    "    test_perplexity = run_epoch(session, mtest)\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "    # Model 저장\n",
    "    print(\"Saving model to %s.\" % save_path)\n",
    "    sv.saver.save(session, save_path, global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
