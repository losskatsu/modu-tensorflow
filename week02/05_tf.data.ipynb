{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tf.data` API\n",
    "\n",
    "`tf.data` API has two new abstractions\n",
    "* `tf.data.Dataset` represents a sequence of elements, in which each element contains one or more Tensor objects. For example, in an image pipeline, the image data and a label\n",
    "  * Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more `tf.Tensor` objects.\n",
    "    * `tf.data.Dataset.from_tensors()`\n",
    "    * `tf.data.Dataset.from_tensor_slices()`\n",
    "    * `tf.data.TFRecordDataset`:  TFRecord format을 읽을 때\n",
    "  * Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more `tf.data.Dataset` objects.\n",
    "* `tf.data.Iterator` provides the main way to extract elements from a dataset. The operation returned by `Iterator.get_next()` yields the next element of a `Dataset` when executed, and typically acts as the interface between input pipeline code and your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset from `tf.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set N=50 for small dataset loading\n",
    "N = 50\n",
    "train_data = train_data[:N]\n",
    "train_labels = train_labels[:N]\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data[:N]\n",
    "test_labels = test_labels[:N]\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "1. You must define a source. `tf.data.Dataset`.\n",
    "  * To construct a Dataset from some tensors in memory, you can use `tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()`.\n",
    "  * Other methods\n",
    "    * `tf.data.TextLineDataset(filenames)`\n",
    "    * `tf.data.FixedLengthRecordDataset(filenames)`\n",
    "    * `tf.data.TFRecordDataset(filenames)`\n",
    "2. Transformation\n",
    "  * `Dataset.map()`: to apply a function to each element\n",
    "  * `Dataset.batch()`\n",
    "  * `Dataset.shuffle()`\n",
    "3. `Iterator`\n",
    "  * `Iterator.initializer`: which enables you to (re)initialize the iterator's state\n",
    "  * `Iterator.get_next()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Store data in `tf.data.Dataset`\n",
    "\n",
    "* `tf.data.Dataset.from_tensor_slices((features, labels))`\n",
    "* `tf.data.Dataset.from_generator(gen, output_types, output_shapes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.int32)>\n",
      "(TensorShape([Dimension(28), Dimension(28)]), TensorShape([]))\n",
      "(tf.float64, tf.int32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "print(train_dataset)\n",
    "print(train_dataset.output_shapes)\n",
    "print(train_dataset.output_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transformaion\n",
    "\n",
    "* `apply(transformation_func)`\n",
    "* `batch(batch_size)`\n",
    "* `concatenate(dataset)`\n",
    "* `flat_map(map_func)`\n",
    "* `repeat(count=None)`\n",
    "  * count=max_epochs\n",
    "* `shuffle(buffer_size, seed=None, reshuffle_each_iteration=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=2)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterator\n",
    "\n",
    "#### 3.1 `make_one_shot_iterator()`\n",
    "\n",
    "* Creates an Iterator for enumerating the elements of this dataset.\n",
    "  * Note: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization.\n",
    "\n",
    "###### Common pattern\n",
    "```python\n",
    "while True:\n",
    "  try:\n",
    "    sess.run(result)\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "x, y = train_iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `for`문으로 epoch control 시 유의점\n",
    "\n",
    "* 사실상 `dataset.repeat(count=2)` 함수로 max_epochs 조절함\n",
    "* `for`문으로 epoch을 조절하고 싶으면 `dataset.repeat()` 쓰지 않으면 됨\n",
    "  * `while`문을 다 돌면 count 만큼의 epochs이 끝남\n",
    "* `for`문으로 control 하고 싶으면 `for`문 시작할 때마다 `iterator.initializer`를 해야 함\n",
    "  * 다음 예제 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now u r in 1th epoch\n",
      "step: 0  labels: [3 4 4 1 5 7 0 9 6 6 3 1 9 9 9 2]\n",
      "step: 1  labels: [5 1 9 1 7 0 6 3 4 8 5 8 7 0 2 8]\n",
      "step: 2  labels: [3 9 3 6 7 1 2 8 5 1 1 3 4 3 2 9]\n",
      "step: 3  labels: [0 6 8 1 9 8 2 6 3 3 5 3 5 4 9 3]\n",
      "step: 4  labels: [1 5 3 7 9 0 6 3 7 9 2 4 0 7 0 7]\n",
      "step: 5  labels: [1 2 9 4 1 0 6 1 6 9 9 3 1 8 6 8]\n",
      "step: 6  labels: [5 2 4 1]\n",
      "End of dataset\n",
      "now u r in 2th epoch\n",
      "step: 7  labels: [4 1 6 7 5 3 9 8 5 6 6 4 8 3 1 1]\n",
      "step: 8  labels: [3 5 1 2 0 3 6 6 0 5 7 2 3 2 1 2]\n",
      "step: 9  labels: [9 9 7 0 8 1 9 4 3 4 9 9 7 9 3 0]\n",
      "step: 10  labels: [1 8 7 3 8 2 6 8 3 7 9 9 8 2 9 5]\n",
      "step: 11  labels: [1 6 0 5 4 9 5 0 7 8 6 1 4 7 9 9]\n",
      "step: 12  labels: [0 1 4 4 3 0 2 3 9 2 3 1 1 1 3 3]\n",
      "step: 13  labels: [5 1 6 6]\n",
      "End of dataset\n",
      "now u r in 3th epoch\n",
      "step: 14  labels: [6 6 5 9 9 3 7 1 9 1 8 1 1 0 2 3]\n",
      "step: 15  labels: [7 6 3 8 2 0 3 3 1 7 9 4 1 2 5 6]\n",
      "step: 16  labels: [9 9 2 3 4 4 6 0 9 5 4 0 7 8 8 1]\n",
      "step: 17  labels: [3 5 4 2 4 0 1 7 3 7 8 0 2 1 3 9]\n",
      "step: 18  labels: [6 5 3 9 8 9 1 9 5 9 3 3 4 5 3 1]\n",
      "step: 19  labels: [7 1 8 6 0 1 2 2 5 8 1 9 7 9 6 6]\n",
      "step: 20  labels: [3 0 4 6]\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "#sess.run(iterator.initializer) 할 필요 없음\n",
    "\n",
    "step = 0\n",
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "  print(\"now u r in {}th epoch\".format(epoch+1))\n",
    "  sess.run(train_iterator.initializer)\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      x_, y_ = sess.run([x, y])\n",
    "\n",
    "      print(\"step: {}  labels: {}\".format(step, y_))\n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 `make_initializable_iterator()`\n",
    "\n",
    "* Creates an Iterator for enumerating the elements of this dataset.\n",
    "* Should `run` the `iterator.initializer`.\n",
    "\n",
    "사용법\n",
    "```python\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "# ...\n",
    "sess.run(iterator.initializer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "\n",
    "x, y = train_iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `for`문으로 epoch control 시 유의점\n",
    "\n",
    "* `iterator.initializer`를 통해서 매 `for`문 마다 dataset을 initial 해야함\n",
    "* `N / batch_size`가 나누어 떨어지지 않으면 맨 마지막 배치는 `N % batch_size` 만큼의 데이터만 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  labels: [1 2 6 3 3 0 1 3 8 9 3 1 3 9 2 5]\n",
      "step: 1  labels: [6 2 1 4 9 7 1 0 9 4 7 3 0 9 4 4]\n",
      "step: 2  labels: [8 9 6 3 8 7 8 9 7 1 2 6 5 1 0 5]\n",
      "step: 3  labels: [5 6 1 2 1 9 1 4 1 4 5 3 1 3 6 6]\n",
      "step: 4  labels: [9 9 3 3 9 2 8 6 5 0 2 8 8 8 3 0]\n",
      "step: 5  labels: [0 7 9 3 2 5 4 7 9 6 7 7 1 3 1 5]\n",
      "step: 6  labels: [0 4 9 6]\n",
      "End of dataset\n",
      "step: 7  labels: [1 4 9 1 9 0 3 9 2 5 1 4 7 5 4 7]\n",
      "step: 8  labels: [1 3 1 3 6 9 9 6 0 2 6 0 1 9 9 8]\n",
      "step: 9  labels: [7 6 6 2 0 5 3 8 8 7 1 5 3 3 3 4]\n",
      "step: 10  labels: [8 2 1 4 1 2 1 9 3 2 4 5 6 1 2 9]\n",
      "step: 11  labels: [4 3 2 5 6 3 4 1 0 3 8 9 8 9 1 1]\n",
      "step: 12  labels: [0 7 3 9 5 3 0 7 6 8 5 0 9 6 3 7]\n",
      "step: 13  labels: [6 8 9 7]\n",
      "End of dataset\n",
      "step: 14  labels: [8 8 7 8 3 3 4 2 1 7 9 5 5 0 4 1]\n",
      "step: 15  labels: [8 6 0 3 1 1 9 2 7 5 6 6 4 7 0 6]\n",
      "step: 16  labels: [0 9 3 2 1 9 9 6 1 9 1 9 2 3 5 3]\n",
      "step: 17  labels: [4 3 4 3 2 8 5 3 5 9 1 7 5 6 9 9]\n",
      "step: 18  labels: [0 8 7 4 3 2 9 2 4 5 0 6 1 6 3 0]\n",
      "step: 19  labels: [3 7 4 1 0 1 9 2 6 3 6 9 8 1 1 9]\n",
      "step: 20  labels: [1 8 7 3]\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "step = 0\n",
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "  sess.run(train_iterator.initializer)\n",
    "  while True:\n",
    "    try:\n",
    "      x_, y_ = sess.run([x, y])\n",
    "\n",
    "      print(\"step: {}  labels: {}\".format(step, y_))\n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [From TensorFlow official site](https://www.tensorflow.org/programmers_guide/datasets)\n",
    "\n",
    "* 밑에 예제들은 TF 홈페이지에서 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Initialize an iterator over a dataset with 10 elements.\n",
    "  sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "  for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)\n",
    "\n",
    "  # Initialize the same iterator over a dataset with 100 elements.\n",
    "  sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "  for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
