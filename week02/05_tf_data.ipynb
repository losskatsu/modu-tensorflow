{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tf.data` API\n",
    "\n",
    "`tf.data` API has two new abstractions\n",
    "* `tf.data.Dataset` represents a sequence of elements, in which each element contains one or more Tensor objects. For example, in an image pipeline, the image data and a label\n",
    "  * Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more `tf.Tensor` objects.\n",
    "    * `tf.data.Dataset.from_tensors()`\n",
    "    * `tf.data.Dataset.from_tensor_slices()`\n",
    "    * `tf.data.TFRecordDataset`:  TFRecord format을 읽을 때\n",
    "  * Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more `tf.data.Dataset` objects.\n",
    "* `tf.data.Iterator` provides the main way to extract elements from a dataset. The operation returned by `Iterator.get_next()` yields the next element of a `Dataset` when executed, and typically acts as the interface between input pipeline code and your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shatapy/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset from `tf.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 20s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "N = 50\n",
    "train_data = train_data[:N]\n",
    "train_labels = train_labels[:N]\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data[:N]\n",
    "test_labels = test_labels[:N]\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "1. You must define a source. <font color='red'>tf.data.Dataset</font>\n",
    "  * To construct a Dataset from some tensors in memory, you can use `tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()`.\n",
    "  * Other methods\n",
    "    * `tf.data.TextLineDataset(filenames)`\n",
    "    * `tf.data.FixedLengthRecordDataset(filenames)`\n",
    "    * `tf.data.TFRecordDataset(filenames)`\n",
    "2. Transformation\n",
    "  * `Dataset.map()`: to apply a function to each element\n",
    "  * `Dataset.batch()`\n",
    "3. `Iterator`\n",
    "  * `Iterator.initializer`: which enables you to (re)initialize the iterator's state\n",
    "  * `Iterator.get_next()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Store data in `tf.data.Dataset`\n",
    "\n",
    "* `tf.data.Dataset.from_tensor_slices((features, labels))`\n",
    "* `tf.data.Dataset.from_generator(gen, output_types, output_shapes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.int32)>\n",
      "(TensorShape([Dimension(28), Dimension(28)]), TensorShape([]))\n",
      "(tf.float64, tf.int32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "print(train_dataset)\n",
    "print(train_dataset.output_shapes)\n",
    "print(train_dataset.output_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(train_dataset.output_shapes)`\n",
    "  * `(TensorShape([Dimension(28), Dimension(28)]), TensorShape([]))`\n",
    "  * train_data: `28 * 28`\n",
    "  * train_labels: `([])`, which is scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transformaion\n",
    "\n",
    "* `apply(transformation_func)`\n",
    "* `batch(batch_size)`\n",
    "* `concatenate(dataset)`\n",
    "* `flat_map(map_func)`\n",
    "* `repeat(count=None)`\n",
    "* `shuffle(buffer_size, seed=None, reshuffle_each_iteration=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=2)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `d.shuffle`\n",
    "  * Randomly shuffles the elements of this dataset.\n",
    "  * buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements from this dataset from which the new dataset will sample.  \n",
    "    * Q. shuffle the elements of the buffer size only and leave the others? or shuffle them and return them?\n",
    "*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterator\n",
    "\n",
    "#### 3.1 `make_one_shot_iterator()`\n",
    "\n",
    "* Creates an Iterator for enumerating the elements of this dataset.\n",
    "  * Note: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization.\n",
    "\n",
    "###### Common pattern\n",
    "```python\n",
    "sess.run(iterator.initializer)\n",
    "while True:\n",
    "  try:\n",
    "    sess.run(result)\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "x, y = train_iterator.get_next()\n",
    "x = tf.cast(x, dtype=tf.float32)\n",
    "y = tf.cast(y, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "#sess.run(iterator.initializer) 할 필요 없음\n",
    "\n",
    "step = 0\n",
    "while True:\n",
    "  try:\n",
    "    x_, y_ = sess.run([x, y])\n",
    "\n",
    "    print(\"step: {}  labels: {}\".format(step, y_))\n",
    "    step += 1\n",
    "\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 `make_initializable_iterator()`\n",
    "\n",
    "* Creates an Iterator for enumerating the elements of this dataset.\n",
    "* Should `run` the `iterator.initializer`.\n",
    "\n",
    "사용법\n",
    "```python\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "# ...\n",
    "sess.run(iterator.initializer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the difference between initializing and not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=2)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "\n",
    "x, y = train_iterator.get_next()\n",
    "x = tf.cast(x, dtype=tf.float32)\n",
    "y = tf.cast(y, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  labels: [1 3 9 0 3 1 8 6 7 0 0 0 2 5 2 8]\n",
      "step: 1  labels: [3 6 1 6 5 3 9 9 1 9 7 8 4 6 2 9]\n",
      "step: 2  labels: [4 5 5 7 8 7 3 9 4 1 3 2 4 3 9 1]\n",
      "step: 3  labels: [6 1 1 0 5 6 4 8 8 1 0 2 9 3 1 6]\n",
      "step: 4  labels: [1 3 5 4 9 6 1 3 9 6 7 9 2 5 7 8]\n",
      "step: 5  labels: [5 3 7 4 3 6 2 9 2 1 7 4 3 9 9 1]\n",
      "step: 6  labels: [0 0 8 3]\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(train_iterator.initializer)\n",
    "\n",
    "step = 0\n",
    "while True:\n",
    "  try:\n",
    "    x_, y_ = sess.run([x, y])\n",
    "\n",
    "    print(\"step: {}  labels: {}\".format(step, y_))\n",
    "    step += 1\n",
    "\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [From TensorFlow official site](https://www.tensorflow.org/programmers_guide/datasets)\n",
    "\n",
    "* 밑에 예제들은 TF 홈페이지에서 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "\n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "print()\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "print()\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': tf.int32, 'a': tf.float32}\n",
      "{'b': TensorShape([Dimension(100)]), 'a': TensorShape([])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  7 11  8  3 12  6 10 17 16  1  2 19]\n",
      "[18  5 24  9 20  4 15 21 28 22 23 30 13]\n",
      "[29 36 34 32 33 14 39 37 41 26 45 38 40]\n",
      "[44 47 48 46 31 27 43 35 25 49 42  6  2]\n",
      "[10  9  3 11  8 14 13 17  1  0 15 22 18]\n",
      "[16 23 25  4 12 24  5 20 26 32 31  7 19]\n",
      "[30 29 38 40 34 21 39 41 43 36 44 35 33]\n",
      "[47 42 45 48 46 37 28 49 27]\n",
      "end of dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(50)\n",
    "dataset = dataset.shuffle(10)\n",
    "dataset = dataset.repeat(count=2)  # double the dataset\n",
    "dataset = dataset.batch(13)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            value = sess.run(next_element)\n",
    "            print(value)\n",
    "        except:\n",
    "            print('end of dataset')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Initialize an iterator over a dataset with 10 elements.\n",
    "  sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "  for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)\n",
    "\n",
    "  # Initialize the same iterator over a dataset with 100 elements.\n",
    "  sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "  for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
