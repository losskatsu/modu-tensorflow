{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with MNIST\n",
    "\n",
    "* [참고: TensorFlow.org](https://www.tensorflow.org/get_started/mnist/beginners)\n",
    "* [소스: mnist_softmax.py in verion 1.4](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_softmax.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A very simple MNIST classifier.\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/beginners in version 1.4\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "np.random.seed(219)\n",
    "tf.set_random_seed(219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_data = train_data.reshape(-1, 784)\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_data = test_data.reshape(-1, 784)\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADetJREFUeJzt3X+MVfWZx/HP45T6B/AHSAZwSpcuorExxi4T0gTSuDY2sxsSbKRaYuJsxE7/KHEbV60aTU02jbDZVvjDNJlGLJiWH0ZU0jT9EaNra1biMKnFwrZFMttSxpkCJlijQfDZP+awGXHO91zuPfeeM/O8Xwm5P557znlywmfOufd77v2auwtAPJdU3QCAahB+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBfaKTGzMzLicE2szdrZHXtXTkN7M+M/u9mR0xs/tbWReAzrJmr+03sy5Jf5B0o6Rjkl6TtN7dDyWW4cgPtFknjvwrJR1x96PufkbSLklrW1gfgA5qJfw9kv486fGx7LmPMLMBMxsys6EWtgWgZK184DfVqcXHTuvdfVDSoMRpP1AnrRz5j0laMunxpyQdb60dAJ3SSvhfk7TczD5jZp+U9FVJ+8ppC0C7NX3a7+5nzWyjpJ9L6pK0zd1/V1pnANqq6aG+pjbGe36g7TpykQ+A6YvwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoJqeoluSzGxE0juSzkk66+69ZTSF+rjiiiuS9bvuuitZ37hxY27NLD2Z7NmzZ5P1O++8M1nfuXNnbu3MmTPJZSNoKfyZf3T3EyWsB0AHcdoPBNVq+F3SL8zsgJkNlNEQgM5o9bR/lbsfN7NuSb80s/9x95cnvyD7o8AfBqBmWjryu/vx7HZc0rOSVk7xmkF37+XDQKBemg6/mc02s7nn70v6kqQ3ymoMQHu1ctq/UNKz2XDNJyT92N1/VkpXANrO3L1zGzPr3MYgSerq6krWb7/99mR98+bNyfqCBQsuuqfzxsfHk/Xu7u6m1y1Jy5cvz629+eabLa27ztw9fQFFhqE+ICjCDwRF+IGgCD8QFOEHgiL8QFAM9c0A69evz62tWLEiuezdd9/d0rafe+65ZP3xxx/PrRUNt+3atStZX7nyYxeUfsRLL72UW7vhhhuSy05nDPUBSCL8QFCEHwiK8ANBEX4gKMIPBEX4gaAY558GUj9/LUlbt27NrRX9PPbJkyeT9b6+vmR9eHg4WW/l/9ecOXOS9dOnTze97VWrViWXffXVV5P1OmOcH0AS4QeCIvxAUIQfCIrwA0ERfiAowg8EVcYsvWhR0Xh20Th/aiz/3XffTS67Zs2aZP3AgQPJejsVTaN9+PDhZP3qq68us50ZhyM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVOM5vZtskrZE07u7XZM/Nl7Rb0lJJI5Jucfe329fmzDZ37txk/corr2x63Vu2bEnW9+/f3/S6261onP/gwYPJOuP8aY0c+X8o6cJfdLhf0gvuvlzSC9ljANNIYfjd/WVJpy54eq2k7dn97ZJuKrkvAG3W7Hv+he4+KknZbXd5LQHohLZf229mA5IG2r0dABen2SP/mJktlqTsdjzvhe4+6O697t7b5LYAtEGz4d8nqT+73y/p+XLaAdApheE3s52S/lvSVWZ2zMw2SNok6UYz+6OkG7PHAKaRwvf87p43+fsXS+4lrMsuu6yl5VPf2X/yySdbWjdmLq7wA4Ii/EBQhB8IivADQRF+ICjCDwTFT3fXwLp161pafs+ePbm1o0ePtrRuzFwc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMb5O6DoK7sbNmxoaf1DQ0MtLV9Xl156abK+atWqDnUyM3HkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfvgKuuuipZ7+npaWn9p05dOI/qzNDV1ZWsF+23999/P7f23nvvNdXTTMKRHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCKhznN7NtktZIGnf3a7LnHpH0NUl/zV72oLv/tF1NIm3fvn1Vt1BLR44cya29/vrrHeyknho58v9QUt8Uzz/m7tdl/wg+MM0Uht/dX5Y0My8hAwJr5T3/RjP7rZltM7N5pXUEoCOaDf/3JS2TdJ2kUUnfzXuhmQ2Y2ZCZzcwfmgOmqabC7+5j7n7O3T+U9ANJKxOvHXT3XnfvbbZJAOVrKvxmtnjSwy9LeqOcdgB0SiNDfTslXS9pgZkdk/RtSdeb2XWSXNKIpK+3sUcAbVAYfndfP8XTT7ShF+Aj+vv7W1p+8+bNJXUyM3GFHxAU4QeCIvxAUIQfCIrwA0ERfiAoc/fObcyscxurkVmzZiXrhw4dStaXLVuWrM+ePTu3VuefqF60aFGyPjw83NLyl19+eW7trbfeSi47nbm7NfI6jvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBRTdHfABx98kKyfO3euQ53Uy+rVq5P1onH8ov3WyWtYpiOO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8M0BPT09uLTVNdSd0d3fn1h566KHkskXj+Bs2bEjWx8bGkvXoOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCF4/xmtkTSDkmLJH0oadDdt5rZfEm7JS2VNCLpFnd/u32tzly7d+9O1h9++OFkfd26dbm1TZs2NdVTo7q6upL1++67L7d27bXXJpcdHR1N1nfs2JGsI62RI/9ZSf/m7ldL+rykb5jZZyXdL+kFd18u6YXsMYBpojD87j7q7sPZ/XckHZbUI2mtpO3Zy7ZLuqldTQIo30W95zezpZI+J2m/pIXuPipN/IGQlH8dJ4DaafjafjObI+kZSd9099NmDU0HJjMbkDTQXHsA2qWhI7+ZzdJE8H/k7nuzp8fMbHFWXyxpfKpl3X3Q3XvdvbeMhgGUozD8NnGIf0LSYXf/3qTSPkn92f1+Sc+X3x6AdimcotvMVkv6laSDmhjqk6QHNfG+f4+kT0v6k6SvuPupgnXxW8pTuPnmm5P1p59+OlkfGRnJra1YsSK57NtvtzY6e9tttyXrTz31VG7t1Knkfxf19fUl60NDQ8l6VI1O0V34nt/dfy0pb2VfvJimANQHV/gBQRF+ICjCDwRF+IGgCD8QFOEHguKnu2vgxRdfTNZPnjyZrC9dujS3du+99yaXfeyxx5L1O+64I1lPfWW3yJYtW5J1xvHbiyM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRV+H3+UjfG9/mb0tub/hGkV155Jbc2a9as5LInTpxI1ufPn5+sX3JJ+vixd+/e3Nqtt96aXLZoim5MrdHv83PkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOefAe65557c2gMPPJBcdt68eS1t+9FHH03WU78XUHSNAZrDOD+AJMIPBEX4gaAIPxAU4QeCIvxAUIQfCKpwnN/MlkjaIWmRpA8lDbr7VjN7RNLXJP01e+mD7v7TgnUxzg+0WaPj/I2Ef7Gkxe4+bGZzJR2QdJOkWyT9zd3/s9GmCD/Qfo2Gv3DGHncflTSa3X/HzA5L6mmtPQBVu6j3/Ga2VNLnJO3PntpoZr81s21mNuV1omY2YGZDZsbcS0CNNHxtv5nNkfRfkr7j7nvNbKGkE5Jc0r9r4q1BcmI3TvuB9ivtPb8kmdksST+R9HN3/94U9aWSfuLu1xSsh/ADbVbaF3vMzCQ9Ienw5OBnHwSe92VJb1xskwCq08in/asl/UrSQU0M9UnSg5LWS7pOE6f9I5K+nn04mFoXR36gzUo97S8L4Qfaj+/zA0gi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFX4A54lOyHpfyc9XpA9V0d17a2ufUn01qwye/u7Rl/Y0e/zf2zjZkPu3ltZAwl17a2ufUn01qyqeuO0HwiK8ANBVR3+wYq3n1LX3ural0Rvzaqkt0rf8wOoTtVHfgAVqST8ZtZnZr83syNmdn8VPeQxsxEzO2hmv6l6irFsGrRxM3tj0nPzzeyXZvbH7HbKadIq6u0RM/tLtu9+Y2b/XFFvS8zsRTM7bGa/M7N/zZ6vdN8l+qpkv3X8tN/MuiT9QdKNko5Jek3Senc/1NFGcpjZiKRed698TNjMviDpb5J2nJ8Nycz+Q9Ipd9+U/eGc5+7fqklvj+giZ25uU295M0v/iyrcd2XOeF2GKo78KyUdcfej7n5G0i5Jayvoo/bc/WVJpy54eq2k7dn97Zr4z9NxOb3VgruPuvtwdv8dSednlq503yX6qkQV4e+R9OdJj4+pXlN+u6RfmNkBMxuoupkpLDw/M1J2211xPxcqnLm5ky6YWbo2+66ZGa/LVkX4p5pNpE5DDqvc/R8k/ZOkb2Snt2jM9yUt08Q0bqOSvltlM9nM0s9I+qa7n66yl8mm6KuS/VZF+I9JWjLp8ackHa+gjym5+/HsdlzSs5p4m1InY+cnSc1uxyvu5/+5+5i7n3P3DyX9QBXuu2xm6Wck/cjd92ZPV77vpuqrqv1WRfhfk7TczD5jZp+U9FVJ+yro42PMbHb2QYzMbLakL6l+sw/vk9Sf3e+X9HyFvXxEXWZuzptZWhXvu7rNeF3JRT7ZUMYWSV2Strn7dzrexBTM7O81cbSXJr7x+OMqezOznZKu18S3vsYkfVvSc5L2SPq0pD9J+oq7d/yDt5zertdFztzcpt7yZpberwr3XZkzXpfSD1f4ATFxhR8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaD+D7KyKFZQ0RolAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = 1000\n",
    "print(\"label = {}\".format(train_labels[index]))\n",
    "plt.imshow(train_data[index].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n",
    "\n",
    "### Create placeholders for inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "# x, y는 배치 데이터를 받을 수 있는 placeholder\n",
    "# x: inputs\n",
    "x = tf.placeholder(tf.float32, name='x', shape=[None, 784])\n",
    "# y: labels\n",
    "y = tf.placeholder(tf.int32, name='y', shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "# create Variables using `get_variable`\n",
    "W = tf.get_variable(name='weight', shape=[784, 10],\n",
    "                    initializer=tf.random_normal_initializer())\n",
    "b = tf.get_variable(name='bias', shape=[10],\n",
    "                    initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model: $y = Wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "y_pred = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function\n",
    "\n",
    "* [`tf.nn.softmax_cross_entropy_with_logits_v2`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2)\n",
    "* [`tf.losses.softmax_cross_entropy`](https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처: TensorFlow 공식 문서\n",
    "# The raw formulation of cross-entropy,\n",
    "#\n",
    "#   tf.reduce_mean(-tf.reduce_sum(y * tf.log(tf.nn.softmax(y_pred)),\n",
    "#                                 reduction_indices=[1]))\n",
    "#\n",
    "# can be numerically unstable.\n",
    "#\n",
    "# So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "# outputs of 'y', and then average across the batch.\n",
    "y_one_hot = tf.one_hot(y, depth=10)\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_one_hot,\n",
    "                                                logits=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session()` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches_per_epoch 1875\n",
      "step: 0, loss: 20.411300659179688\n",
      "step: 100, loss: 0.8186461925506592\n",
      "step: 200, loss: 0.5676659941673279\n",
      "step: 300, loss: 0.6220453381538391\n",
      "step: 400, loss: 0.7941129803657532\n",
      "step: 500, loss: 0.5649630427360535\n",
      "step: 600, loss: 0.6956238746643066\n",
      "step: 700, loss: 0.5315375328063965\n",
      "step: 800, loss: 0.6670381426811218\n",
      "step: 900, loss: 0.1328665167093277\n",
      "step: 1000, loss: 0.8226040601730347\n",
      "step: 1100, loss: 0.6190036535263062\n",
      "step: 1200, loss: 0.46451497077941895\n",
      "step: 1300, loss: 0.746595561504364\n",
      "step: 1400, loss: 0.40039700269699097\n",
      "step: 1500, loss: 0.7575790882110596\n",
      "step: 1600, loss: 0.6816705465316772\n",
      "step: 1700, loss: 0.15428262948989868\n",
      "step: 1800, loss: 0.3819967210292816\n",
      "training done!\n",
      "Elapsed time: 2.1149678230285645\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_batches_per_epoch = int(len(train_data) / batch_size)\n",
    "print(\"num_batches_per_epoch {}\".format(num_batches_per_epoch))\n",
    "\n",
    "max_epochs = 1 # Train for only ten epochs\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  batch_index = np.random.permutation(len(train_data))\n",
    "  for step in range(num_batches_per_epoch):\n",
    "    # 여기를 직접 채워 넣으시면 됩니다.\n",
    "    # train_data, train_labels의 인덱스를 잘 설정해보세요.\n",
    "    batch_xs = train_data[step*batch_size:(step+1)*batch_size]\n",
    "    batch_ys = train_labels[step*batch_size:(step+1)*batch_size]\n",
    "    _, loss = sess.run([train_op, cross_entropy],\n",
    "                       feed_dict={x: batch_xs, y: batch_ys})\n",
    "    losses.append(loss)\n",
    "  \n",
    "    if step % 100 == 0:\n",
    "      print(\"step: {}, loss: {}\".format(step, loss))\n",
    "    \n",
    "print('training done!')\n",
    "print(\"Elapsed time: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model\n",
    "\n",
    "* test accuracy: 0.8893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y, tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: test_data,\n",
    "                                    y: test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "batch_xs = test_data[batch_index]\n",
    "y_pred_ = sess.run(y_pred, feed_dict={x: batch_xs})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred_)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  p.set_title(\"y_pred: {}\".format(np.argmax(py)))\n",
    "  p.imshow(px.reshape(28, 28), cmap='gray')\n",
    "  p.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
